-- phpMyAdmin SQL Dump
-- version 4.8.0
-- https://www.phpmyadmin.net/
--
-- Anamakine: 127.0.0.1
-- Üretim Zamanı: 13 May 2018, 17:09:09
-- Sunucu sürümü: 10.1.31-MariaDB
-- PHP Sürümü: 7.2.4

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
SET AUTOCOMMIT = 0;
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Veritabanı: `fblog`
--

-- --------------------------------------------------------

--
-- Tablo için tablo yapısı `articles`
--

CREATE TABLE `articles` (
  `id` int(11) NOT NULL,
  `title` text NOT NULL,
  `author` text NOT NULL,
  `content` text NOT NULL,
  `created_date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Tablo döküm verisi `articles`
--

INSERT INTO `articles` (`id`, `title`, `author`, `content`, `created_date`) VALUES
(5, 'Lorem Ipsum Nedir?', 'baris', '<h2>Lorem Ipsum Nedir?</h2>\r\n\r\n<p><strong>Lorem Ipsum</strong>, dizgi ve baskı end&uuml;strisinde kullanılan mıgır metinlerdir. Lorem Ipsum, adı bilinmeyen bir matbaacının bir hurufat numune kitabı oluşturmak &uuml;zere bir yazı galerisini alarak karıştırdığı 1500&#39;lerden beri end&uuml;stri standardı sahte metinler olarak kullanılmıştır. Beşy&uuml;z yıl boyunca varlığını s&uuml;rd&uuml;rmekle kalmamış, aynı zamanda pek değişmeden elektronik dizgiye de sı&ccedil;ramıştır. 1960&#39;larda Lorem Ipsum pasajları da i&ccedil;eren Letraset yapraklarının yayınlanması ile ve yakın zamanda Aldus PageMaker gibi Lorem Ipsum s&uuml;r&uuml;mleri i&ccedil;eren masa&uuml;st&uuml; yayıncılık yazılımları ile pop&uuml;ler olmuştur.</p>\r\n', '2018-05-11 16:28:56'),
(6, 'Lorem Ipsum Nedir?2', 'baris', '<h2>Lorem Ipsum Nedir?</h2>\r\n\r\n<p><strong>Lorem Ipsum</strong>, dizgi ve baskı end&uuml;strisinde kullanılan mıgır metinlerdir. Lorem Ipsum, adı bilinmeyen bir matbaacının bir hurufat numune kitabı oluşturmak &uuml;zere bir yazı galerisini alarak karıştırdığı 1500&#39;lerden beri end&uuml;stri standardı sahte metinler olarak kullanılmıştır. Beşy&uuml;z yıl boyunca varlığını s&uuml;rd&uuml;rmekle kalmamış, aynı zamanda pek değişmeden elektronik dizgiye de sı&ccedil;ramıştır. 1960&#39;larda Lorem Ipsum pasajları da i&ccedil;eren Letraset yapraklarının yayınlanması ile ve yakın zamanda Aldus PageMaker gibi Lorem Ipsum s&uuml;r&uuml;mleri i&ccedil;eren masa&uuml;st&uuml; yayıncılık yazılımları ile pop&uuml;ler olmuştur.</p>\r\n', '2018-05-11 16:29:06'),
(7, 'Lorem Ipsum Nedir?3', 'baris', '<h2>Lorem Ipsum Nedir?</h2>\r\n\r\n<p><strong>Lorem Ipsum</strong>, dizgi ve baskı end&uuml;strisinde kullanılan mıgır metinlerdir. Lorem Ipsum, adı bilinmeyen bir matbaacının bir hurufat numune kitabı oluşturmak &uuml;zere bir yazı galerisini alarak karıştırdığı 1500&#39;lerden beri end&uuml;stri standardı sahte metinler olarak kullanılmıştır. Beşy&uuml;z yıl boyunca varlığını s&uuml;rd&uuml;rmekle kalmamış, aynı zamanda pek değişmeden elektronik dizgiye de sı&ccedil;ramıştır. 1960&#39;larda Lorem Ipsum pasajları da i&ccedil;eren Letraset yapraklarının yayınlanması ile ve yakın zamanda Aldus PageMaker gibi Lorem Ipsum s&uuml;r&uuml;mleri i&ccedil;eren masa&uuml;st&uuml; yayıncılık yazılımları ile pop&uuml;ler olmuştur.</p>\r\n', '2018-05-11 16:29:13'),
(8, 'Lorem Ipsum Nedir?3', 'baris', '<h2>Lorem Ipsum Nedir?</h2>\r\n\r\n<p><strong>Lorem Ipsum</strong>, dizgi ve baskı end&uuml;strisinde kullanılan mıgır metinlerdir. Lorem Ipsum, adı bilinmeyen bir matbaacının bir hurufat numune kitabı oluşturmak &uuml;zere bir yazı galerisini alarak karıştırdığı 1500&#39;lerden beri end&uuml;stri standardı sahte metinler olarak kullanılmıştır. Beşy&uuml;z yıl boyunca varlığını s&uuml;rd&uuml;rmekle kalmamış, aynı zamanda pek değişmeden elektronik dizgiye de sı&ccedil;ramıştır. 1960&#39;larda Lorem Ipsum pasajları da i&ccedil;eren Letraset yapraklarının yayınlanması ile ve yakın zamanda Aldus PageMaker gibi Lorem Ipsum s&uuml;r&uuml;mleri i&ccedil;eren masa&uuml;st&uuml; yayıncılık yazılımları ile pop&uuml;ler olmuştur.</p>\r\n', '2018-05-11 16:29:23'),
(9, ' Nereden Gelir?', 'baris', '<p>Yaygın inancın tersine, Lorem Ipsum rastgele s&ouml;zc&uuml;klerden oluşmaz. K&ouml;kleri M.&Ouml;. 45 tarihinden bu yana klasik Latin edebiyatına kadar uzanan 2000 yıllık bir ge&ccedil;mişi vardır. Virginia&#39;daki Hampden-Sydney College&#39;dan Latince profes&ouml;r&uuml; Richard McClintock, bir Lorem Ipsum pasajında ge&ccedil;en ve anlaşılması en g&uuml;&ccedil; s&ouml;zc&uuml;klerden biri olan &#39;consectetur&#39; s&ouml;zc&uuml;ğ&uuml;n&uuml;n klasik edebiyattaki &ouml;rneklerini incelediğinde kesin bir kaynağa ulaşmıştır. Lorm Ipsum, &Ccedil;i&ccedil;ero tarafından M.&Ouml;. 45 tarihinde kaleme alınan &quot;de Finibus Bonorum et Malorum&quot; (İyi ve K&ouml;t&uuml;n&uuml;n U&ccedil; Sınırları) eserinin 1.10.32 ve 1.10.33 sayılı b&ouml;l&uuml;mlerinden gelmektedir. Bu kitap, ahlak kuramı &uuml;zerine bir tezdir ve R&ouml;nesans d&ouml;neminde &ccedil;ok pop&uuml;ler olmuştur. Lorem Ipsum pasajının ilk satırı olan &quot;Lorem ipsum dolor sit amet&quot; 1.10.32 sayılı b&ouml;l&uuml;mdeki bir satırdan gelmektedir.</p>\r\n', '2018-05-11 16:45:27'),
(10, 'Nereden Bulabilirim?', 'baris', '<p>Lorem Ipsum pasajlarının bir&ccedil;ok &ccedil;eşitlemesi vardır. Ancak bunların b&uuml;y&uuml;k bir &ccedil;oğunluğu mizah katılarak veya rastgele s&ouml;zc&uuml;kler eklenerek değiştirilmişlerdir. Eğer bir Lorem Ipsum pasajı kullanacaksanız, metin aralarına utandırıcı s&ouml;zc&uuml;kler gizlenmediğinden emin olmanız gerekir. İnternet&#39;teki t&uuml;m Lorem Ipsum &uuml;rete&ccedil;leri &ouml;nceden belirlenmiş metin bloklarını yineler. Bu da, bu &uuml;reteci İnternet &uuml;zerindeki ger&ccedil;ek Lorem Ipsum &uuml;reteci yapar. Bu &uuml;rete&ccedil;, 200&#39;den fazla Latince s&ouml;zc&uuml;k ve onlara ait c&uuml;mle yapılarını i&ccedil;eren bir s&ouml;zl&uuml;k kullanır. Bu nedenle, &uuml;retilen Lorem Ipsum metinleri yinelemelerden, mizahtan ve karakteristik olmayan s&ouml;zc&uuml;klerden uzaktır.</p>\r\n', '2018-05-11 16:46:12');

-- --------------------------------------------------------

--
-- Tablo için tablo yapısı `pinfo`
--

CREATE TABLE `pinfo` (
  `login` text NOT NULL,
  `avatar` text NOT NULL,
  `followers` int(11) NOT NULL,
  `following` int(11) NOT NULL,
  `blog` text NOT NULL,
  `public_repos` int(11) NOT NULL,
  `repos_url` text NOT NULL,
  `created_at` int(11) NOT NULL,
  `rName` text NOT NULL,
  `readme` text NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Tablo döküm verisi `pinfo`
--

INSERT INTO `pinfo` (`login`, `avatar`, `followers`, `following`, `blog`, `public_repos`, `repos_url`, `created_at`, `rName`, `readme`) VALUES
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/asteroids', '\nAsteroids for Atom\nRemember that awesome hack from a few years ago that allowed you to spawn an\nAsteroids shooter on any web page and then blast away at all the HTML DOM\nelements? Well, since Atom is built with web technologies, now you can do the\nsame right in your editor!\nAll credit for the Asteroids game code go to Rootof Creations HB <rootof.com>.\nYou can find their original code at https://github.com/erkie/erkie.github.com.\nTo play, select Packages->Asteroids->Play from the menu bar. Use arrow keys\nto move around and spacebar to fire.\nTo return your editor window to normal, select Window: Reload from the\ncommand pallette.\n\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/bert', '\nBERT\nA BERT (Binary ERlang Term) serialization library for Ruby. It can\nencode Ruby objects into BERT format and decode BERT binaries into Ruby\nobjects.\nSee the BERT specification at bert-rpc.org.\nInstances of the following Ruby classes will be automatically converted to the\nproper simple BERT type:\n\nFixnum\nFloat\nSymbol\nArray\nString\n\nInstances of the following Ruby classes will be automatically converted to the\nproper complex BERT type:\n\nNilClass\nTrueClass\nFalseClass\nHash\nTime\nRegexp\n\nTo designate tuples, simply prefix an Array literal with a t or use the\nBERT::Tuple class:\nt[:foo, [1, 2, 3]]\nBERT::Tuple[:foo, [1, 2, 3]]\n\nBoth of these will be converted to (in Erlang syntax):\n{foo, [1, 2, 3]}\n\nInstallation\ngem install bert -s http://gemcutter.org\n\nUsage\nrequire \'bert\'\n\nbert = BERT.encode(t[:user, {:name => \'TPW\', :nick => \'mojombo\'}])\n# => \"\\203h\\002d\\000\\004userh\\003d\\000\\004bertd\\000\\004dictl\\000\\000\\\n      000\\002h\\002d\\000\\004namem\\000\\000\\000\\003TPWh\\002d\\000\\004nickm\\\n      000\\000\\000\\amojomboj\"\n\nBERT.decode(bert)\n# => t[:user, {:name=>\"TPW\", :nick=>\"mojombo\"}]\n\nNote on Patches/Pull Requests\n\nFork the project.\nMake your feature addition or bug fix.\nAdd tests for it. This is important so I don\'t break it in a\nfuture version unintentionally.\nCommit, do not mess with rakefile, version, or history.\n(if you want to have your own version, that is fine but\nbump version in a commit by itself I can ignore when I pull)\nSend me a pull request. Bonus points for topic branches.\n\nCopyright\nCopyright (c) 2009 Tom Preston-Werner. See LICENSE for details.\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/bert.erl', '\nErlang BERT encoder/decoder. See http://bert-rpc.org for full spec.\nWatch and contribute to this module at http://github.com/mojombo/bert.erl.\nThis module is Semantic Versioning (http://semver.org) compliant.\nThe following types can be automatically encoded and decoded.\nSee http://www.erlang.org/eeps/eep-0008.html for type definitions.\ninteger() -> BERT integer\nfloat()   -> BERT float\natom()    -> BERT atom\ntuple()   -> BERT tuple\nlist()    -> BERT list or BERT bytelist\nstring()  -> BERT list or BERT bytelist (you probably want binary)\nbinary()  -> BERT binary\n[]        -> BERT nil (complex)\nbool()    -> BERT boolean (complex)\ndict()    -> BERT dict (complex)\nBecause times and regular expressions types cannot be automatically\ndetected, you must encode and decode those types manually.\nTo encode Erlang terms to BERT binaries, use:\nencode(term()) -> binary().\n\nTo decode BERT binaries to Erlang terms, use:\ndecode(binary()) -> term().\n\nExamples\n% Encode a variety of literal Erlang terms:\nbert:encode([42, 3.14, banana, {xy, 5, 10}, <<\"robot\">>, true, false]).\n% -> <<131,108,0,0,0,7,97,42,99,51,46,49,52,48,48,48,48,48,48,...>>\n\n% Encode an Erlang dict() record:\nD0 = dict:new().\nD1 = dict:store(apple, red, D0).\nbert:encode(D1).\n% -> <<131,104,9,100,0,4,100,105,99,116,97,0,97,16,97,16,97,8,...>>\n\n% Decode a BERT binary:\nbert:decode(<<131,108,0,0,0,7,97,42,99,51,46,49,52,...>>).\n% -> [42, 3.14, banana, {xy, 5, 10}, <<\"robot\">>, true, false]\n\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/bertrpc', '\nBERTRPC\nBy Tom Preston-Werner (tom@mojombo.com)\nBERT-RPC client library for Ruby. Makes it ridiculously simple to interface\nwith BERT-RPC servers.\nSee the full BERT-RPC specification at bert-rpc.org.\nThis library currently only supports the following BERT-RPC features:\n\ncall requests\ncast requests\n\nBERTRPC was developed for GitHub and is currently in production use performing\nmillions of RPC requests every day. The stability and performance have been\nexemplary.\nInstallation\n$ gem install bertrpc\n\nExamples\nRequire the library and create a service:\nrequire \'bertrpc\'\nsvc = BERTRPC::Service.new(\'localhost\', 9999)\n\nMake a call:\nsvc.call.calc.add(1, 2)\n# => 3\n\nThe underlying BERT-RPC transaction of the above call is:\n-> {call, calc, add, [1, 2]}\n<- {reply, 3}\n\nMake a cast:\nsvc.cast.stats.incr\n# => nil\n\nThe underlying BERT-RPC transaction of the above cast is:\n-> {cast, stats, incr, []}\n<- {noreply}\n\nDocumentation\nCreating a service:\n# No timeout\nsvc = BERTRPC::Service.new(\'localhost\', 9999)\n\n# 10s socket read timeout, raises BERTRPC::ReadTimeoutError\nsvc = BERTRPC::Service.new(\'localhost\', 9999, 10)\n\nCopyright\nCopyright (c) 2009 Tom Preston-Werner. See LICENSE for details.\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/bower', '\nBOWER \nIntroduction\nBower is a package manager for the web. Bower lets you easily install assets such as images, CSS and JavaScript, and manages dependencies for you.\nFor example, to install a package, run:\nbower install jquery\n\nThis will download jQuery to ./components/jquery. That\'s it. The idea is that Bower does package management and package management only.\nInstalling Bower\nBower is installed using Node and npm (oh my, how meta).\nnpm install bower -g\n\nUsage\nYour best friend at this stage is probably bower --help.\nTo install a package:\nbower install jquery\nbower install git://github.com/maccman/package-jquery.git\nbower install maccman/package-jquery (same as above)\nbower install http://code.jquery.com/jquery-1.7.2.js\nbower install ./repos/jquery\n\nAs you can see, packages can be installed by name, Git endpoint, Github shorthand, URL or local path.\nIf you install and URL that is a zip or tar file, bower will automatically extract the contents of it.\nView all packages available through Bower\'s registry.\nDuring install you can have Bower add an entry to your component.json as well:\nbower install --save jquery\n\nTo update a package, reference it by name:\nbower update jquery-ui\n\nTo list installed packages:\nbower list\n\nTo search for packages:\nbower search [name]\n\nTo list all the available packages, just call bower search without specifying a name.\nTo clean the cache:\nbower cache-clean [name]\n\nSeveral packages can be cleaned at the same time.\nTo clean the entire cache, just call bower cache-clean without any names.\nAlso, both the install and update commands have a --force flag that tells bower to bypass the cache and always fetch remote sources.\nYou can disable colors by using the --no-color flag.\nBower Configuration\nBower can be configured by creating a .bowerrc file in your home folder (usually ~/bowerrc) with one or all of the following configuration parameters. You can also configure Bower on a per-project basis by creating a .bowerrc file in the project directory, Bower will merge this configuration with the configuration found in your home directory. This allows you to version your project specific Bower configuration with the rest of your code base.\n{\n  \"directory\" : \"components\",\n  \"json\"      : \"component.json\",\n  \"endpoint\"  : \"https://bower.herokuapp.com\"\n}\nTo run your own Bower Endpoint for custom components/packages that are behind a firewall you can use a simple implementation of bower server at https://github.com/twitter/bower-server.\nDefining a package\nYou can create a component.json file in your project\'s root, specifying all of its dependencies. This is similar to Node\'s package.json, or Ruby\'s Gemfile, and is useful for locking down a project\'s dependencies.\n{\n  \"name\": \"myProject\",\n  \"version\": \"1.0.0\",\n  \"main\": \"./path/to/main.css\",\n  \"dependencies\": {\n    \"jquery\": \"~1.7.2\"\n  }\n}\nPut this under your project\'s root, listing all of your dependencies. When you run bower install, Bower will read this component.json file, resolve all the relevant dependencies and install them.\nFor now, name, version, main, and dependencies are the only properties that are used by Bower. If you have several files you\'re distributing as part of your package, pass an array to main like this:\n{\n  \"name\": \"myProject\",\n  \"version\": \"1.0.0\",\n  \"main\": [\"./path/to/app.css\", \"./path/to/app.js\", \"./path/to/sprite.img\"],\n  \"dependencies\": {\n    \"jquery\": \"~1.7.2\"\n  }\n}\nThere should only be at most one file per file type in the main list. So only one .js or .css.\nInstalling dependencies\nDependencies are installed locally via the bower install command. First they’re resolved to find conflicts. Then they’re downloaded and unpacked in a local subdirectory called ./components, for example:\n/component.json\n/components/jquery/index.js\n/components/jquery/component.json\n\nYou can also install packages one at a time bower install git://my/git/thing\nThere are no system wide dependencies, no dependencies are shared between different apps, and the dependency tree is flat.\nDeploying\nThe easiest approach is to use Bower statically, just reference the packages manually from a script tag:\n<script src=\"components/jquery/index.js\"></script>\n\nFor more complex projects, you\'ll probably want to concatenate your scripts. Bower is just a package manager, but there are lots of awesome libraries out there to help you do this, such as Sprockets and RequireJS.\nFor example, to use Sprockets:\nenvironment = Sprockets::Environment.new\nenvironment.append_path \'components\'\nenvironment.append_path \'public\'\nrun environment\nPackage Consumption\nBower also makes available a source mapping – this can be used by build tools to easily consume Bower components.\nIf you pass the option --map to bower\'s list command, it will generate a JSON with dependency objects. Alternatively, you can pass the --paths flag to the list command to get a simple path to name mapping:\n{\n  \"backbone\": \"components/backbone/index.js\",\n  \"jquery\": \"components/jquery/index.js\",\n  \"underscore\": \"components/underscore/index.js\"\n}\nAuthoring packages\nTo register a new package, it\'s as simple as specifying a component.json, pushing the package to a Git endpoint, say GitHub, and running:\nbower register myawesomepackagename git://github.com/maccmans/face\n\nThere\'s no authentication or user management. It\'s on a first come, first served basis. Think of it like a URL shortener. Now anyone can run bower install myawesomepackagename, and get your library installed.\nThere is no direct way to unregister a package yet. Meanwhile you can request it here.\nPhilosophy\nCurrently, people are managing dependencies, such as JavaScript libraries, manually. This sucks, and we want to change it.\nIn a nutshell, Bower is a generic tool which will resolve dependencies and lock packages down to a version. It runs over Git, and is package-agnostic. A package may contain JavaScript, CSS, images, etc., and doesn\'t rely on any particular transport (AMD, CommonJS, etc.).\nBower then makes available a simple programmatic API which exposes the package dependency model, so that existing build tools (like Sprockets, LoadBuilder, curls.js, Ender, etc.) can consume it and build files accordingly.\nProgrammatic API\nBower provides a pretty powerful programmatic api. All commands can be accessed through the bower.commands object.\nvar bower = require(\'bower\');\n\nbower.commands\n  .install(paths, options)\n  .on(\'end\', function (data) {\n    data && console.log(data);\n  });\nAll commands emit three types of events: data, end, and error.\nFor a better of idea how this works, you may want to check out our bin file.\nFAQ\nWhat distinguishes Bower from Jam, Volo, Component, or Ender? What does it do better?\nBower is a lower level component than Jam, Volo, Component, or Ender. These managers could consume Bower as a dependency.\nBower\'s aim is simply to install Git paths, resolve dependencies from a component.json, check versions, and then provide an API which reports on these things. Nothing more. This is a major diversion from past attempts at browser package management.\nBower is working under the assumption that there is a single, common problem in frontend application development: dependency resolution. Past attempts (Jam, Volo, Ender, Component) try to tackle this problem in such a way that they actually end up alienating and further segregating the JavaScript community around transports (Sprockets, CommonJS, RequireJS, regular script tags).\nBower offers a generic, unopinionated solution to the problem of package management, while exposing an API that can be consumed by a more opinionated build stack.\nVolo is an arguably more established project and works with the GitHub search API. Will it take long for Bower to contain a decent number of packages?\nBower (being a Git powered package manager) should, in theory, be capable of consuming every package that Volo does, with the additional benefit of supporting internal networks and other Git repositories not hosted on GitHub.\nWe recently saw what happened when the main NPM registry went down. Is a single point of failure possible for Bower and if so, do you have redundancy planned?\nThere\'s no redundancy planned at the moment, as Bower just installs Git URLs. It\'s up to the URL provider to establish redundancy.\nIsn\'t having a package.json file going to conflict with my npm\'s package.json? Will this be a problem?\nDon\'t use a package.json – use a component.json.\nBower is an open-source Twitter project. How well can we expect it to be maintained in the future?\nTwitter is in the process of migrating its frontend architecture onto Bower, so it\'s fairly safe to say it will be maintained and invested in going forward.\nContact\nHave a question? Ask on our mailing list!\ntwitter-bower@googlegroups.com\nhttp://groups.google.com/group/twitter-bower\nAuthors\n\n@fat\n@maccman\n@satazor\n\nThanks for assistance and contributions:\n\n@addyosmani\n@angus-c\n@borismus\n@chriseppstein\n@danwrong\n@desandro\n@isaacs\n@josh\n@jrburke\n@mklabs\n@paulirish\n@rvagg\n@sindresorhus\n@SlexAxton\n@sstephenson\n@tomdale\n@visionmedia\n@wagenet\n@wycats\n\nLicense\nCopyright 2012 Twitter, Inc.\nLicensed under the MIT License\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/chronic', '\nChronic\nChronic is a natural language date/time parser written in pure Ruby. See below\nfor the wide variety of formats Chronic will parse.\nInstallation\n$ gem install chronic\n\nUsage\nrequire \'chronic\'\n\nTime.now   #=> Sun Aug 27 23:18:25 PDT 2006\n\nChronic.parse(\'tomorrow\')\n  #=> Mon Aug 28 12:00:00 PDT 2006\n\nChronic.parse(\'monday\', :context => :past)\n  #=> Mon Aug 21 12:00:00 PDT 2006\n\nChronic.parse(\'this tuesday 5:00\')\n  #=> Tue Aug 29 17:00:00 PDT 2006\n\nChronic.parse(\'this tuesday 5:00\', :ambiguous_time_range => :none)\n  #=> Tue Aug 29 05:00:00 PDT 2006\n\nChronic.parse(\'may 27th\', :now => Time.local(2000, 1, 1))\n  #=> Sat May 27 12:00:00 PDT 2000\n\nChronic.parse(\'may 27th\', :guess => false)\n  #=> Sun May 27 00:00:00 PDT 2007..Mon May 28 00:00:00 PDT 2007\n\nChronic.parse(\'6/4/2012\', :endian_precedence => :little)\n  #=> Fri Apr 06 00:00:00 PDT 2012\n\nChronic.parse(\'INVALID DATE\')\n  #=> nil\nIf the parser can find a date or time, either a Time or Chronic::Span\nwill be returned (depending on the value of :guess). If no\ndate or time can be found, nil will be returned.\nSee Chronic.parse for detailed usage instructions.\nExamples\nChronic can parse a huge variety of date and time formats. Following is a\nsmall sample of strings that will be properly parsed. Parsing is case\ninsensitive and will handle common abbreviations and misspellings.\nSimple\n\nthursday\nnovember\nsummer\nfriday 13:00\nmon 2:35\n4pm\n10 to 8\n10 past 2\nhalf past 2\n6 in the morning\nfriday 1pm\nsat 7 in the evening\nyesterday\ntoday\ntomorrow\nlast week\nnext week\nthis tuesday\nnext month\nlast winter\nthis morning\nlast night\nthis second\nyesterday at 4:00\nlast friday at 20:00\nlast week tuesday\ntomorrow at 6:45pm\nafternoon yesterday\nthursday last week\n\nComplex\n\n3 years ago\na year ago\n5 months before now\n7 hours ago\n7 days from now\n1 week hence\nin 3 hours\n1 year ago tomorrow\n3 months ago saturday at 5:00 pm\n7 hours before tomorrow at noon\n3rd wednesday in november\n3rd month next year\n3rd thursday this september\n4th day last week\nfourteenth of june 2010 at eleven o\'clock in the evening\nmay seventh \'97 at three in the morning\n\nSpecific Dates\n\nJanuary 5\n22nd of june\n5th may 2017\nFebruary twenty first\ndec 25\nmay 27th\nOctober 2006\noct 06\njan 3 2010\nfebruary 14, 2004\nfebruary 14th, 2004\n3 jan 2000\n17 april 85\n5/27/1979\n27/5/1979\n05/06\n1979-05-27\nFriday\n5\n4:00\n17:00\n0800\n\nSpecific Times (many of the above with an added time)\n\nJanuary 5 at 7pm\n22nd of june at 8am\n1979-05-27 05:00:00\n03/01/2012 07:25:09.234567\n2013-08-01T19:30:00.345-07:00\n2013-08-01T19:30:00.34-07:00\netc\n\nTime Zones\nChronic allows you to set which Time class to use when constructing times. By\ndefault, the built in Ruby time class creates times in your system\'s local\ntime zone. You can set this to something like ActiveSupport\'s\nTimeZone\nclass to get full time zone support.\n>> Time.zone = \"UTC\"\n>> Chronic.time_class = Time.zone\n>> Chronic.parse(\"June 15 2006 at 5:45 AM\")\n=> Thu, 15 Jun 2006 05:45:00 UTC +00:00\n\nLimitations\nChronic uses Ruby\'s built in Time class for all time storage and computation.\nBecause of this, only times that the Time class can handle will be properly\nparsed. Parsing for times outside of this range will simply return nil.\nSupport for a wider range of times is planned for a future release.\nContribute\nIf you\'d like to hack on Chronic, start by forking the repo on GitHub:\nhttps://github.com/mojombo/chronic\nThe best way to get your changes merged back into core is as follows:\n\nClone down your fork\nCreate a thoughtfully named topic branch to contain your change\nInstall the development dependencies by running bundle install\nHack away\nAdd tests and make sure everything still passes by running bundle exec rake\nEnsure your tests pass in multiple timezones. ie TZ=utc bundle exec rake TZ=BST bundle exec rake\nIf you are adding new functionality, document it in the README\nDo not change the version number, we will do that on our end\nIf necessary, rebase your commits into logical chunks, without errors\nPush the branch up to GitHub\nSend a pull request for your branch\n\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/clippy', '\nClippy - Helping you copy text to your clipboard\nClippy is a very simple Flash widget that makes it possible to place arbitrary\ntext onto the client\'s clipboard. Here is what Clippy looks like on GitHub:\n\nHere is a sample Rails (Ruby) helper that can be used to place Clippy on a\npage:\ndef clippy(text, bgcolor=\'#FFFFFF\')\n  html = <<-EOF\n    <object classid=\"clsid:d27cdb6e-ae6d-11cf-96b8-444553540000\"\n            width=\"110\"\n            height=\"14\"\n            id=\"clippy\" >\n    <param name=\"movie\" value=\"/flash/clippy.swf\"/>\n    <param name=\"allowScriptAccess\" value=\"always\" />\n    <param name=\"quality\" value=\"high\" />\n    <param name=\"scale\" value=\"noscale\" />\n    <param NAME=\"FlashVars\" value=\"text=#{text}\">\n    <param name=\"bgcolor\" value=\"#{bgcolor}\">\n    <embed src=\"/flash/clippy.swf\"\n           width=\"110\"\n           height=\"14\"\n           name=\"clippy\"\n           quality=\"high\"\n           allowScriptAccess=\"always\"\n           type=\"application/x-shockwave-flash\"\n           pluginspage=\"http://www.macromedia.com/go/getflashplayer\"\n           FlashVars=\"text=#{text}\"\n           bgcolor=\"#{bgcolor}\"\n    />\n    </object>\n  EOF\nend\n\nInstallation (Pre-Built SWF)\nIf you want to use Clippy unmodified, just copy build/clippy.swf to your\npublic directory or wherever your static assets can be found.\nInstallation (Compiling)\nIn order to compile Clippy from source, you need to install the following:\n\nhaXe\nswfmill\n\nThe haXe code is in clippy.hx, the button images are in assets, and the\ncompiler config is in compile.hxml. Make sure you look at all of these to\nsee where and what you\'ll need to modify. To compile everything into a final\nSWF, run the following from Clippy\'s root directory:\nswfmill simple library.xml library.swf && haxe compile.hxml\n\nIf that is successful, copy build/clippy.swf to your\npublic directory or wherever your static assets can be found.\nContribute\nIf you\'d like to hack on Clippy, start by forking my repo on GitHub:\nhttp://github.com/mojombo/clippy\nThe best way to get your changes merged back into core is as follows:\n\nClone down your fork\nCreate a topic branch to contain your change\nHack away\nIf you are adding new functionality, document it in README.md\nIf necessary, rebase your commits into logical chunks, without errors\nPush the branch up to GitHub\nSend me (mojombo) a pull request for your branch\n\nLicense\nMIT License (see LICENSE file)\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/erlectricity', '\nErlectricity\nhttp://github.com/mojombo/erlectricity\nBy Scott Fleckenstein, Tom Preston-Werner\nDevelopment Status: Production/Stable\nDescription\nErlectricity allows a Ruby program to receive and respond to Erlang messages\nsent over the Erlang binary protocol.\nInstall\n$ gem install erlectricity\n-or-\n$ gem install mojombo-erlectricity -s http://gems.github.com\nThe Simplest Example\nRuby side (echo.rb)\nrequire \'rubygems\'\nrequire \'erlectricity\'\n\nreceive do |f|\n  f.when([:echo, String]) do |text|\n    f.send!([:result, \"You said: #{text}\"])\n    f.receive_loop\n  end\nend\n\nErlang side (echo.erl)\n-module(echo).\n-export([test/0]).\n\ntest() ->\n  Cmd = \"ruby echo.rb\",\n  Port = open_port({spawn, Cmd}, [{packet, 4}, nouse_stdio, exit_status, binary]),\n  Payload = term_to_binary({echo, <<\"hello world!\">>}),\n  port_command(Port, Payload),\n  receive\n    {Port, {data, Data}} ->\n      {result, Text} = binary_to_term(Data),\n      io:format(\"~p~n\", [Text])\n  end.\n\nData Type Conversions and Matching\n% Port is the port opened via open_port({spawn, Cmd}, [{packet, 4}, ...])\n% Message is the Erlang term to encode and send to the port\nsend(Port, Message) ->\n  port_command(Port, term_to_binary(Message)).\n\n# Each triplet below represents:\n# (line 1) the Erlang call\n# (line 2) the Ruby matcher\n# (line 3) the Ruby output\n\nsend(Port, test).\nf.when(:test) { p :ok }\n# :ok\n\nsend(Port, {atom, symbol}).\nf.when([:atom, Symbol]) { |sym| p sym }\n# :symbol\n\nsend(Port, {number, 1}).\nf.when([:number, Fixnum]) { |num| p num }\n# 1\n\nsend(Port, {string, <<\"foo\">>}).\nf.when([:string, String]) { |str| p str }\n# \"foo\"\n\nsend(Port, {array, [1,2,3]}).\nf.when([:array, Array]) { |arr| p arr }\n# [1, 2, 3]\n\nsend(Port, {array, [<<\"abc\">>, <<\"def\">>]}).\nf.when([:array, Array]) { |arr| p arr }\n# [\"abc\", \"def\"]\n\nsend(Port, {hash, [{key,val}]}).\nf.when([:hash, Erl.hash]) { |hash| p hash }\n# {:key=>:val}\n\nsend(Port, {object, {1,{2},3,<<\"four\">>}}).\nf.when([:object, Any]) { |any| p any }\n# [1, [2], 3, \"four\"]\n\nContribute\nIf you\'d like to hack on Erlectricity, start by forking my repo on GitHub:\nhttp://github.com/mojombo/erlectricity\nTo get all of the dependencies, install the gem first. The best way to get\nyour changes merged back into core is as follows:\n\nClone down your fork\nCreate a topic branch to contain your change\nHack away\nAdd tests and make sure everything still passes by running rake\nIf you are adding new functionality, document it in the README.md\nDo not change the version number, I will do that on my end\nIf necessary, rebase your commits into logical chunks, without errors\nPush the branch up to GitHub\nSend me (mojombo) a pull request for your branch\n\nCopyright\nCopyright (c) 2009 Scott Fleckenstein and Tom Preston-Werner. See LICENSE for details.\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/ernie', '\nErnie\nBy Tom Preston-Werner (tom@mojombo.com)\nErnie is a BERT-RPC server implementation that uses an Erlang server to accept\nincoming connections, and then delegates the request to custom modules that\nyou can write in any language (currently only Ruby and Erlang support is\nincluded).\nModules that are written in Ruby or any non-Erlang language are known as\n\"external\" modules and you must specify how many workers of each module should\nbe spawned. Requests against these modules are balanced between the workers.\nModules that are written in Erlang are known as \"native\" modules and run\nwithin the Erlang server\'s runtime. Since these are spawned as lightweight\nprocesses, there is no balancing necessary and much less communication\noverhead when compared to external modules.\nErnie supports multiple heterogenous modules. For instance, you can have an\nexternal Ruby module running 10 workers and a native Erlang module running\nsimultaneously. Ernie keeps track of sending requests to the proper module.\nUsing a technique called \"shadowing,\" you can selectively optimize certain\nexternal module functions with native code and Ernie will handle selecting the\ncorrect function.\nSee the full BERT-RPC specification at bert-rpc.org.\nErnie currently supports the following BERT-RPC features:\n\ncall requests\ncast requests\n\nErnie was developed for GitHub and is currently in production use serving\nmillions of RPC requests every day. The stability and performance have been\nexemplary.\nErnie follows Semantic Versioning for release\nversioning.\nInstallation\nStep 1: Install Erlang (R13B or higher).\nhttp://www.erlang.org/download.html\n\nStep 2: Install Ernie:\n$ [sudo] gem install ernie\n\nRunning\nUsage: ernie [command] [options]\n    -c, --config CONFIG              Config file.\n    -p, --port PORT                  Port.\n    -l, --log-level                  Log level (0-4).\n    -a, --access-log LOGFILE         Access log file.\n    -d, --detached                   Run as a daemon.\n    -P, --pidfile PIDFILE            Location to write pid file.\n    --name NAME                      Erlang process name.\n    --sname SNAME                    Erlang short process name.\n    -E, --erlang ERLANG_OPTIONS      Options passed to Erlang VM.\n\nCommands:\n  <none>                Start an Ernie server.\n  reload-handlers       Gracefully reload all of the external handlers\n                        and use the new code for all subsequent requests.\n  stats                 Print a list of connection and handler statistics.\n\nExamples:\n  ernie -d -p 9999 -c example.cfg\n    Start the ernie server in the background on port 9999 using the\n    example.cfg configuration file.\n\n  ernie reload-handlers -p 9999\n    Reload the handlers for the ernie server currently running on\n    port 9999.\n\n  ernie -c example.cfg -E \'-run mymodule\'\n    Start the ernie server with an additional erlang module called\n    \'mymodule\'\n\nConfiguration File\nErnie configuration files are written as a series of dotted Erlang terms. Each\nterm is a list of 2-tuples that specify options for a module.\nNative Modules\nThe form for native modules is:\n[{module, Module},\n {type, native},\n {codepaths, CodePaths}].\n\nWhere Module is an atom corresponding to the module name and CodePaths is a\nlist of strings representing the file paths that should be added to the\nruntime\'s code path. These paths will be prepended to the code path and must\ninclude the native module\'s directory and the directories of any dependencies.\nExternal Modules\nThe form for external modules is:\n[{module, Module},\n {type, external},\n {command, Command},\n {count, Count}].\n\nWhere Module is an atom corresponding to the module name, Command is a string\nspecifying the command to be executed in order to start a worker, and Count is\nthe number of workers to spawn.\nShadowing\nIf you specify a native module and an external module of the same name (and in\nthat order), Ernie will inspect the native module to see if it has the\nrequested function exported and use that if it does. If it does not, then it\nwill fall back on the external module. This can be used to selectively\noptimize certain functions in a module without any modifications to your\nclient code.\nPredicate Shadowing\nIn some circumstances it can be nice to conditionally shadow a function in an\nexternal module based on the nature of the arguments. For example, you might\nwant requests for math:fib(X) to be routed to the external module when X is\nless than 10, but to be handled by the native module when X is 10 or greater.\nThis can be accomplished by implementing a function math:fib_pred(X) in the\nnative module. Notice the _pred appended to the normal function name (pred\nis short for predicate). If a function like this is present, Ernie will call\nit with the requested arguments and if the return value is true the native\nmodule will be used. If the return value is false the external module will\nbe used.\nExample Configuration File\nThe following example config file informs Ernie of two modules. The first term\nidentifies a native module \'nat\' that resides in the nat.beam file under the\n\'/path/to/app/ebin\' directory. The second term specifies an external module\n\'ext\' that will have 2 workers started with the command \'ruby\n/path/to/app/ernie/ext.rb\'.\n[{module, nat},\n {type, native},\n {codepaths, [\"/path/to/app/ebin\"]}].\n\n[{module, ext},\n {type, external},\n {command, \"ruby /path/to/app/ernie/ext.rb\"},\n {count, 2}].\n\nAccess Log\nIf you have requested that an access log be written (using the -a or\n--access-log option) then all requests will be logged to that file. Each\nrequest is printed on a single line. The elements of the log line are as\nfollows (with comments on the right side):\nACC                                  type of message [ ACC | ERR ]\n[2010-02-20T11:42:25.259750]         time the connection was accepted\n0.000053                             seconds from connection to processing start\n0.000237                             seconds from processing start to finish\n-                                    delimiter\n0                                    size of high queue at connect time\n0                                    size of low queue at connect time\nnat                                  type of handler [ nat | ext ]\nhigh                                 priority [ high | low ]\n-                                    delimiter\n{call,nat,add,[1,2]}                 message\n\nLog lines are written when the request completes so they may appear out of\norder with respect to connection time. To facilitate log rotation, Ernie will\ncreate a new access log file if the current log file is moved or deleted.\nNative (Erlang) Handler\nNative handlers are written as normal Erlang modules. The exported functions\nwill become available to BERT-RPC clients.\nExample\n-module(nat).\n-export([add/2]).\n\nadd(A, B) ->\n  A + B.\n\nBERT-RPC Sequence Example\n-> {call, nat, add, [1, 2]}\n<- {reply, 3}\n\nExternal (Ruby) Handler\nIncluded in this gem is a library called ernie that makes it easy to write\nErnie handlers in Ruby. All you have to do is write a standard Ruby module and\nexpose it to Ernie and the functions of that module will become available to\nBERT-RPC clients.\nExample\nUsing a Ruby module and Ernie.expose:\nrequire \'rubygems\'\nrequire \'ernie\'\n\nmodule Ext\n  def add(a, b)\n    a + b\n  end\nend\n\nErnie.expose(:ext, Ext)\n\nBERT-RPC Sequence Example\n-> {call, nat, add, [1, 2]}\n<- {reply, 3}\n\nLogging\nYou can have logging sent to a file by adding these lines to your handler:\nlogfile(\'/var/log/ernie.log\')\nloglevel(Logger::INFO)\n\nThis will log startup info, requests, and error messages to the log. Choosing\nLogger::DEBUG will include the response (be careful, doing this can generate\nvery large log files).\nAutostart\nNormally Ruby Ernie handlers will become active after the file has been loaded\nin. you can disable this behavior by setting:\nErnie.auto_start = false\n\nSelecting Queue Priority\nErnie maintains High and Low priority queues for incoming connections. If\nthere are any connections in the High priority queue, they will always be\nprocessed first. If the High priority queue is empty, connections will be\nprocessed from the Low priority queue. By default, connections go into the\nHigh priority queue. To select a queue, an info BERP of the following form\nmust be sent preceding the call.\n-- {info, priority, Priority}\n\nWhere Priority is either the high or low atom. An example sequence where\nthe low priority queue is being selected would look like the following.\n-> {info, priority, low}\n-> {call, nat, add, [1, 2]}\n<- {reply, 3}\n\nUsing the BERTRPC gem to make calls to Ernie\nYou can make BERT-RPC calls from Ruby with the BERTRPC gem:\nrequire \'bertrpc\'\n\nsvc = BERTRPC::Service.new(\'localhost\', 8000)\nsvc.call.ext.add(1, 2)\n# => 3\n\nContribute\nIf you\'d like to hack on Ernie, start by forking my repo on GitHub:\nhttp://github.com/mojombo/ernie\n\nTo get all of the dependencies, install the gem first. To run ernie from\nsource, you must first build the Erlang code:\nrake ebuild\n\nThe best way to get your changes merged back into core is as follows:\n\nClone down your fork\nCreate a topic branch to contain your change\nHack away\nAdd tests and make sure everything still passes by running rake\nIf you are adding new functionality, document it in the README.md\nDo not change the version number, I will do that on my end\nIf necessary, rebase your commits into logical chunks, without errors\nPush the branch up to GitHub\nSend me (mojombo) a pull request for your branch\n\nCopyright\nCopyright (c) 2009 Tom Preston-Werner. See LICENSE for details.\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/god', '\nGod: The Ruby Framework for Process Management\n\nAuthors: Tom Preston-Werner, Kevin Clark, Eric Lindvall\nWebsite: http://godrb.com\n\nDescription\nGod is an easy to configure, easy to extend monitoring framework written in\nRuby.\nKeeping your server processes and tasks running should be a simple part of\nyour deployment process. God aims to be the simplest, most powerful monitoring\napplication available.\nDocumentation\nSee in-repo documentation at REPO_ROOT/doc.\nSee online documentation at http://godrb.com.\nCommunity\nSign up for the god mailing list at http://groups.google.com/group/god-rb\nLicense\nSee LICENSE file.\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/grit', '\nGrit\nGrit is no longer maintained. Check out rugged.\nGrit gives you object oriented read/write access to Git repositories via Ruby.\nThe main goals are stability and performance. To this end, some of the\ninteractions with Git repositories are done by shelling out to the system\'s\ngit command, and other interactions are done with pure Ruby\nreimplementations of core Git functionality. This choice, however, is\ntransparent to end users, and you need not know which method is being used.\nThis software was developed to power GitHub, and should be considered\nproduction ready. An extensive test suite is provided to verify its\ncorrectness.\nGrit is maintained by Tom Preston-Werner, Scott Chacon, Chris Wanstrath, and\nPJ Hyett.\nThis documentation is accurate as of Grit 2.3.\nRequirements\n\ngit (http://git-scm.com) tested with 1.7.2.1\n\nInstall\nEasiest install is via RubyGems:\n$ gem install grit\n\nSource\nGrit\'s Git repo is available on GitHub, which can be browsed at:\nhttp://github.com/mojombo/grit\n\nand cloned with:\ngit clone git://github.com/mojombo/grit.git\n\nDevelopment\nYou will need these gems to get tests to pass:\n\nmocha\n\nContributing\nIf you\'d like to hack on Grit, follow these instructions. To get all of the dependencies, install the gem first.\n\nFork the project to your own account\nClone down your fork\nCreate a thoughtfully named topic branch to contain your change\nHack away\nAdd tests and make sure everything still passes by running rake\nIf you are adding new functionality, document it in README.md\nDo not change the version number, I will do that on my end\nIf necessary, rebase your commits into logical chunks, without errors\nPush the branch up to GitHub\nSend a pull request for your branch\n\nUsage\nGrit gives you object model access to your Git repositories. Once you have\ncreated a Repo object, you can traverse it to find parent commits,\ntrees, blobs, etc.\nInitialize a Repo object\nThe first step is to create a Grit::Repo object to represent your repo. In\nthis documentation I include the Grit module to reduce typing.\nrequire \'grit\'\nrepo = Grit::Repo.new(\"/Users/tom/dev/grit\")\n\nIn the above example, the directory /Users/tom/dev/grit is my working\ndirectory and contains the .git directory. You can also initialize Grit with\na bare repo.\nrepo = Repo.new(\"/var/git/grit.git\")\n\nGetting a list of commits\nFrom the Repo object, you can get a list of commits as an array of Commit\nobjects.\nrepo.commits\n# => [#<Grit::Commit \"e80bbd2ce67651aa18e57fb0b43618ad4baf7750\">,\n      #<Grit::Commit \"91169e1f5fa4de2eaea3f176461f5dc784796769\">,\n      #<Grit::Commit \"038af8c329ef7c1bae4568b98bd5c58510465493\">,\n      #<Grit::Commit \"40d3057d09a7a4d61059bca9dca5ae698de58cbe\">,\n      #<Grit::Commit \"4ea50f4754937bf19461af58ce3b3d24c77311d9\">]\n\nCalled without arguments, Repo#commits returns a list of up to ten commits\nreachable by the master branch (starting at the latest commit). You can\nask for commits beginning at a different branch, commit, tag, etc.\nrepo.commits(\'mybranch\')\nrepo.commits(\'40d3057d09a7a4d61059bca9dca5ae698de58cbe\')\nrepo.commits(\'v0.1\')\n\nYou can specify the maximum number of commits to return.\nrepo.commits(\'master\', 100)\n\nIf you need paging, you can specify a number of commits to skip.\nrepo.commits(\'master\', 10, 20)\n\nThe above will return commits 21-30 from the commit list.\nThe Commit object\nCommit objects contain information about that commit.\nhead = repo.commits.first\n\nhead.id\n# => \"e80bbd2ce67651aa18e57fb0b43618ad4baf7750\"\n\nhead.parents\n# => [#<Grit::Commit \"91169e1f5fa4de2eaea3f176461f5dc784796769\">]\n\nhead.tree\n# => #<Grit::Tree \"3536eb9abac69c3e4db583ad38f3d30f8db4771f\">\n\nhead.author\n# => #<Grit::Actor \"Tom Preston-Werner <tom@mojombo.com>\">\n\nhead.authored_date\n# => Wed Oct 24 22:02:31 -0700 2007\n\nhead.committer\n# => #<Grit::Actor \"Tom Preston-Werner <tom@mojombo.com>\">\n\nhead.committed_date\n# => Wed Oct 24 22:02:31 -0700 2007\n\nhead.message\n# => \"add Actor inspect\"\n\nYou can traverse a commit\'s ancestry by chaining calls to #parents.\nrepo.commits.first.parents[0].parents[0].parents[0]\n\nThe above corresponds to master^^^ or master~3 in Git parlance.\nThe Tree object\nA tree records pointers to the contents of a directory. Let\'s say you want\nthe root tree of the latest commit on the master branch.\ntree = repo.commits.first.tree\n# => #<Grit::Tree \"3536eb9abac69c3e4db583ad38f3d30f8db4771f\">\n\ntree.id\n# => \"3536eb9abac69c3e4db583ad38f3d30f8db4771f\"\n\nOnce you have a tree, you can get the contents.\ncontents = tree.contents\n# => [#<Grit::Blob \"4ebc8aea50e0a67e000ba29a30809d0a7b9b2666\">,\n      #<Grit::Blob \"81d2c27608b352814cbe979a6acd678d30219678\">,\n      #<Grit::Tree \"c3d07b0083f01a6e1ac969a0f32b8d06f20c62e5\">,\n      #<Grit::Tree \"4d00fe177a8407dbbc64a24dbfc564762c0922d8\">]\n\nThis tree contains two Blob objects and two Tree objects. The trees are\nsubdirectories and the blobs are files. Trees below the root have additional\nattributes.\ncontents.last.name\n# => \"lib\"\n\ncontents.last.mode\n# => \"040000\"\n\nThere is a convenience method that allows you to get a named sub-object\nfrom a tree.\ntree / \"lib\"\n# => #<Grit::Tree \"e74893a3d8a25cbb1367cf241cc741bfd503c4b2\">\n\nYou can also get a tree directly from the repo if you know its name.\nrepo.tree\n# => #<Grit::Tree \"master\">\n\nrepo.tree(\"91169e1f5fa4de2eaea3f176461f5dc784796769\")\n# => #<Grit::Tree \"91169e1f5fa4de2eaea3f176461f5dc784796769\">\n\nThe Blob object\nA blob represents a file. Trees often contain blobs.\nblob = tree.contents.first\n# => #<Grit::Blob \"4ebc8aea50e0a67e000ba29a30809d0a7b9b2666\">\n\nA blob has certain attributes.\nblob.id\n# => \"4ebc8aea50e0a67e000ba29a30809d0a7b9b2666\"\n\nblob.name\n# => \"README.txt\"\n\nblob.mode\n# => \"100644\"\n\nblob.size\n# => 7726\n\nYou can get the data of a blob as a string.\nblob.data\n# => \"Grit is a library to ...\"\n\nYou can also get a blob directly from the repo if you know its name.\nrepo.blob(\"4ebc8aea50e0a67e000ba29a30809d0a7b9b2666\")\n# => #<Grit::Blob \"4ebc8aea50e0a67e000ba29a30809d0a7b9b2666\">\n\nOther\nThere are many more API methods available that are not documented here. Please\nreference the code for more functionality.\nCopyright\nCopyright (c) 2010 Tom Preston-Werner. See LICENSE for details.\n\n'),
('mojombo', 'https://avatars0.githubusercontent.com/u/1?v=4', 20945, 11, 'http://tom.preston-werner.com', 60, 'https://api.github.com/users/mojombo/repos', 2007, 'mojombo/homebrew', '\nHomebrew - The Missing Package Manager for OS X\nFeatures and usage are summarized on the homepage.\nQuick Install to /usr/local\nThis script will prompt for confirmation before it does anything:\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.github.com/gist/323731)\"\n\nAfterwards, install Xcode.\nUmm… I thought I could install it anywhere?\nIndeed, you can. Refer to our complete installation instructions.\nDude! Just give me a one-liner!\nOkay then, but please note this installs Homebrew as root and\nwe recommend against that.\ncurl -LsSf https://github.com/mxcl/homebrew/tarball/master | sudo /usr/bin/tar xvz -C/usr/local --strip 1\n\nBut what packages are available?\nBefore installing you can\nbrowse the Formula folder on GitHub.\nAfter installing, you can use brew search to find packages or brew server\nto browse packages off of a local web server.\nMore Documentation\nThe wiki is your friend.\nWho Are You?\nI\'m Max Howell and I\'m a splendid chap.\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/agendas', '\nagendas\nTC39 meeting agendas\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/allocation_counter', '\nAllocationCounter\nTODO: Write a gem description\nInstallation\nAdd this line to your application\'s Gemfile:\ngem \'allocation_counter\'\n\nAnd then execute:\n$ bundle\n\nOr install it yourself as:\n$ gem install allocation_counter\n\nUsage\nTODO: Write usage instructions here\nContributing\n\nFork it\nCreate your feature branch (git checkout -b my-new-feature)\nCommit your changes (git commit -am \'Add some feature\')\nPush to the branch (git push origin my-new-feature)\nCreate new Pull Request\n\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/atom-pain-split', '\nPain Split\nThis package is no longer being actively developed. I don\'t use Atom any more. If you want to be the new maintainer, by all means, just ask.\nIn the mean time, I\'ll at least try and fix bugs for a while longer, but I don\'t think I\'ll do feature requests.\n\nAtom\'s pane:split-* commands suck if you want to do anything else but re-open\nthe current document in your shiny new pane. Pain Split is here to help.\nDefault key bindings go something like this. Sorry for polluting the pane:*\nnamespace.\n\'.platform-darwin\':\n  # Open an empty pane and bring it to focus to do as you will.\n  # If there had to be only one way to split panes, this should have\n  # been it.\n  \'cmd-k left\':  \'pane:split-left-creating-empty-pane\'\n  \'cmd-k right\': \'pane:split-right-creating-empty-pane\'\n  \'cmd-k up\':    \'pane:split-up-creating-empty-pane\'\n  \'cmd-k down\':  \'pane:split-down-creating-empty-pane\'\n  \n  # Open a new pane and move the current editor tab to it.\n  \'cmd-k m left\':  \'pane:split-left-moving-current-tab\'\n  \'cmd-k m right\': \'pane:split-right-moving-current-tab\'\n  \'cmd-k m up\':    \'pane:split-up-moving-current-tab\'\n  \'cmd-k m down\':  \'pane:split-down-moving-current-tab\'\n  \n  # Atom\'s default behavior. \"D\" for duplicate the current tab.\n  # It\'s still there if you want it, I guess.\n  \'cmd-k d left\':  \'pane:split-left\'\n  \'cmd-k d right\': \'pane:split-right\'\n  \'cmd-k d up\':    \'pane:split-up\'\n  \'cmd-k d down\':  \'pane:split-down\'\n  \n  # Merge all tabs into one(first) pane.\n  \'cmd-k m\':  \'pane:merge-all-panes\'\n\nFor windows / linux:\n\'.platform-linux, .platform-win32\':\n  # Open an empty pane and bring it to focus to do as you will.\n  # If there had to be only one way to split panes, this should have\n  # been it.\n  \'ctrl-k left\':  \'pane:split-left-creating-empty-pane\'\n  \'ctrl-k right\': \'pane:split-right-creating-empty-pane\'\n  \'ctrl-k up\':    \'pane:split-up-creating-empty-pane\'\n  \'ctrl-k down\':  \'pane:split-down-creating-empty-pane\'\n\n  # Open a new pane and move the current editor tab to it.\n  \'ctrl-k m left\':  \'pane:split-left-moving-current-tab\'\n  \'ctrl-k m right\': \'pane:split-right-moving-current-tab\'\n  \'ctrl-k m up\':    \'pane:split-up-moving-current-tab\'\n  \'ctrl-k m down\':  \'pane:split-down-moving-current-tab\'\n\n  # Atom\'s default behavior. \"D\" for duplicate the current tab.\n  # It\'s still there if you want it, I guess.\n  \'ctrl-k d left\':  \'pane:split-left\'\n  \'ctrl-k d right\': \'pane:split-right\'\n  \'ctrl-k d up\':    \'pane:split-up\'\n  \'ctrl-k d down\':  \'pane:split-down\'\n\n  # Merge all tabs into one(first) pane.\n  \'ctrl-k m\':  \'pane:merge-all-panes\'\n\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/bench-backburner', '\nbench-backburner\nInstallation\n# Install Node Modules:\nnpm install\n\n# Install the Science Tap:\nbrew tap homebrew/science\n\n# Install R:\nbrew install r\n\n# Install ggplot2:\nR\ninstall.packages(\"ggplot2\")\nUsage\n\nWithin your chrome web browser, navigate to your application (http://www.myapp.com)\nOpen the Chrome Dev Tools : Network Tab\nRefresh\nRight-click the file for the document Type \"www.myapp.com\" and select \"Save as HAR with Content\"\nSave the .har (ideally save the file within the the \"bench-backburner/..\" dir.\nWithin \"bench-backburner/server.ts\" update const HAR_FILE = with the location of the saved .har file and save.\n\n# Launch the server\nnpm run-script serve\n\n# In a new terminal tab/window launch the benchmark\nnpm run-script bench\n\n# Once the benchmark runner runs 50x (default) and completes launch the report\n./report.R\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/bootstrap-nitrous', '\nbootstrap-nitrous\n\n');
INSERT INTO `pinfo` (`login`, `avatar`, `followers`, `following`, `blog`, `public_repos`, `repos_url`, `created_at`, `rName`, `readme`) VALUES
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/broccoli-concat', '\nBroccoli concatenator that generates & propagates sourcemaps\n\n\nThis filter is designed to be fast & good enough. It can generates\nsource maps substantially faster than you\'ll get via\nmozilla/source-map, because it\'s special-cased for straight\nline-to-line contenation.\nIt discovers input sourcemaps in relative URLs, including data URIs.\nUsage\nvar node = concat(node);\nAdvanced Usage\nvar node = concat(node, {\n  outputFile: \'/output.js\',\n  header: \";(function() {\",\n  headerFiles: [\'loader.js\'],\n  inputFiles: [\'**/*\']\n  footerFiles: [\'auto-start.js\'],\n  footer: \"}());\",\n  sourceMapConfig: { enabled: true },\n  allowNone: false | true // defaults to false, and will error if trying to concat but no files are found.\n});\nThe structure of output.js will be as follows:\n// - header\n// - ordered content of the files in headerFiles\n// - un-ordered content of files matched by inputFiles, but not in headerFiles or footerFiles\n// - ordered content of the files in footerFiles\n// - footer\n\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/broccoli-typescript-compiler', '\nbroccoli-typescript-compiler\n\n\nA Broccoli plugin which\ncompiles TypeScript files.\nHow to install?\n$ npm install broccoli-typescript-compiler --save-dev\nHow to use?\nvar typescript = require(\'broccoli-typescript-compiler\').typescript;\nvar cjsTree = typescript(inputTree, {\n  tsconfig: {\n    compilerOptions: {\n      module: \"commonjs\",\n      target: \"es5\",\n      moduleResolution: \"node\",\n      newLine: \"LF\",\n      rootDir: \"src\",\n      outDir: \"dist\",\n      sourceMap: true,\n      declaration: true\n    },\n    files: [\n      \"src/index.ts\",\n      \"src/tests/**\"\n    ]\n  },\n  annotation: \"compile program\"\n});\nConfig Options:\ntsconfig:\n\ndefault (when ommited): will find the nearest tsconfig relative to where the BroccoliTypeScriptCompiler is invoked.\nas string: a absolute path to a config tsconfig file\nas config object: See: https://www.typescriptlang.org/docs/handbook/tsconfig-json.html\n\nannotation:\nAn optional string, which when provide should be a descriptive annotation. Useful for debugging, to tell multiple instances of the same plugin apart.\nWays to use:\nvia the broccoli plugin subclass\nvar TypeScriptPlugin = require(\'broccoli-typescript-compiler\').TypeScriptPlugin;\nvia a function:\nvar filterTypeScript = require(\'broccoli-typescript-compiler\').filterTypeScript;\nvar scriptTree = filterTypeScript(inputTree);\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/cafe', '\nWelcome to the café\nWe feature the most exquisite compilers to brew your JavaScript just the way you like it.\nThere is much work to be done but a few recipes are ready for you to try and help improve.\nNo refunds if you grow a third arm!\nInstall\nClone the repository with git.\njs.js\nHere\'s an example of callbacks from the examples directory:\nvar \n  sys = require(\"sys\"),\n  cafe = require(\"../lib/cafe\"),\n  compiler = new cafe.js.Compiler();\n\ncompiler.on(\"parse:FunctionDecl\", function () {\n  sys.puts(sys.inspect(this));\n\n  // rewrite the function name\n  this.children[0].name = \'zomg\';\n});\n\nvar source = \"function foo (bar) { return this; }\";\n\nvar ast = compiler.parse(source);\n\nsys.puts(ast.toJS(0));\n\nRun with node:\nnode ./examples/function-decl.js\n\nThe script is parsed in AST form, which you can modify, and then converted back to JavaScript.\nI\'m not sure what kind of API cafe will have, really, but this seemed like something cool and easy to show. There are more tricks possible.\nobjj.js\nThere\'s also an Objective-J to JavaScript compiler. It doesn\'t contain the Objective-J runtime or anything, but could help catch syntactic errors. It probably needs an official once over from 280North to make sure it\'s correct, but when it is it should be better at catching syntax errors than the official Objective-J parser.\nharmony.js (soon)\nI plan to have a Simple Modules to JavaScript and CommonJS module compilation and other syntactic goodies.\n(your js here).js\nWhatever, let\'s go wild and add stuff.\nTo be continued...\n-Zach\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/cargo', '\nCargo downloads your Rust project’s dependencies and compiles your project.\nLearn more at http://crates.io/.\nCompiling cargo\nYou\'ll want to clone cargo using --recursive on git, to clone in it\'s submodule\ndependencies.\n$ git clone --recursive https://github.com/rust-lang/cargo\n\nor\n$ git submodule init\n$ git submodule update\n\nThen it\'s as simple as make followed by make install and you\'re\nready to go.\nContributing to the Docs\nTo contribute to the docs, please submit pull requests to wycats/cargo-website.\nAll you need to do is change the markdown files in the source directory.\nLicense\nCargo is primarily distributed under the terms of both the MIT license\nand the Apache License (Version 2.0).\nSee LICENSE-APACHE and LICENSE-MIT for details.\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/color-rs', '\ncolor-rs\nA library that provides types and conversions for working with various color formats.\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/crates.io', '\ncrates.io\nSource code for the default registry for Cargo users. Can be found online at\ncrates.io\nInstallation\n\ngit clone this repository\nnpm install\nbower install\n\nMaking UI tweaks or changes\nThis website is built using Ember.js for the frontend,\nwhich enables tweaking the UI of the site without actually having the server\nrunning locally. To get up and running, just run:\nember server --proxy https://staging-crates-io.herokuapp.com\n\nThis will give you a local server to browse while using the staging backend\n(hosten on heroku). You can also specify the proxy as https://crates.io/, but\nbeware that any modifications made are permanent!\nWorking on the backend\nIf you\'d like to change the API server (the Rust backend), then the setup is a\nlittle more complicated.\n\n\nDefine some environment variables:\n# Credentials for uploading packages to S3, these can be blank if you\'re not\n# publishing locally.\nexport S3_BUCKET=...\nexport S3_ACCESS_KEY=...\nexport S3_SECRET_KEY=...\nexport S3_REGION=...      # not needed if the S3 bucket is in US standard\n\n# Credentials for talking to github, can be blank if you\'re not logging in.\n#\n# When registering a new application, be sure to set the callback url to the\n# address `http://localhost:4200/authorize/github`.\nexport GH_CLIENT_ID=...\nexport GH_CLIENT_SECRET=...\n\n# Key to sign and encrypt cookies with\nexport SESSION_KEY=...\n\n# Location of the *postgres* database\n#\n# e.g. postgres://postgres:@localhost/cargo_registry\nexport DATABASE_URL=...\n\n# Remote and local locations of the registry index\nexport GIT_REPO_URL=file://`pwd`/tmp/index-bare\nexport GIT_REPO_CHECKOUT=`pwd`/tmp/index-co\n\n\n\nSet up the git index\n./script/init-local-index.sh\n\n\n\nBuild the server\ncargo build\n\n\n\nRun the migrations\n./target/migrate\n\n\n\nRun the servers\n# In one window, run the api server\n./target/server\n\n# In another window run the ember-cli server\nember server --proxy http://localhost:8888/\n\n\n\nRunning Tests\n\n\nConfigure the location of the test database. Note that this should just be a\nblank database, the test harness will ensure that migrations are run.\nexport TEST_DATABASE_URL=...\n\n\n\nRun the API server tests\ncargo test\n\n\n\nRun frontend tests\nember test\nember test --server\n\n\n\nTools\nFor more information on using ember-cli, visit\nhttp://iamstef.net/ember-cli/.\nFor more information on using cargo, visit\ndoc.crates.io.\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/dash-rust', '\nDash docset for Rust\nInstallation\nDownload the latest release of the docset from the releases page. If you want to generate the docset yourself, follow the steps in \"Usage\" below.\nUsage\n$ bundle install\n$ rake\n$ open Rust.docset\nTo download the documentation for the latest Rust nightly and build a\nfresh docset from that:\n$ bundle install\n$ rake nightly\n$ open Rust.docset\nLicense\nMIT License, copyright 2014 by André Arko\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/dbmonster', '\nDbmonster\nThis README outlines the details of collaborating on this Ember application.\nA short introduction of this app could easily go here.\nPrerequisites\nYou will need the following things properly installed on your computer.\n\nGit\nNode.js (with NPM)\nBower\nEmber CLI\nPhantomJS\n\nInstallation\n\ngit clone <repository-url> this repository\nchange into the new directory\nnpm install\nbower install\n\nRunning / Development\n\nember server\nVisit your app at http://localhost:4200.\n\nCode Generators\nMake use of the many generators for code, try ember help generate for more details\nRunning Tests\n\nember test\nember test --server\n\nBuilding\n\nember build (development)\nember build --environment production (production)\n\nDeploying\nSpecify what it takes to deploy your app.\nFurther Reading / Useful Links\n\nember.js\nember-cli\nDevelopment Browser Extensions\n\nember inspector for chrome\nember inspector for firefox\n\n\n\n\n'),
('wycats', 'https://avatars0.githubusercontent.com/u/4?v=4', 8861, 4, 'http://yehudakatz.com', 203, 'https://api.github.com/users/wycats/repos', 2008, 'wycats/decaf', '\n\nReflect.js is a JavaScript (ES3 compatible) implementation of Mozilla\'s Parser API. It does not currently support some of Mozilla\'s extensions, such as generators, list comprehensions, for each, E4X, etc. but may eventually support ones that are, or become Harmony proposals.\nBuilders are also supported.\nParsing really large files can be slow, for reasons articulated by Andy Chu.\nDownload\nYou can download a minified-standalone version of reflect.js to embed in web pages here.\nInstall\nReflect.js is available as a CommonJS module for Node.js. Simply install it with npm:\nnpm install reflect\n\nUse\nvar Reflect = require(\'reflect\');\n\nvar ast = Reflect.parse(\"var a = 4 + 7\");\n\nconsole.log(Reflect.stringify(ast, \"  \"));\n\nRefer to Mozilla\'s docs for details on the AST interface.\nBuilders\nThe optional builder parameter to Reflect.parse() makes it possible to construct user-specified data from the parser, rather than the default Node objects.\nThe reflect.js module exports the default builder so you can redefine only the node constructors you care about and leave the rest default.\nvar Reflect = require(\'reflect\');\nvar builder = Reflect.builder;\n\n// redefine callback for variable declarations\nbuilder[\"variableDeclaration\"] = function (kind, declarators, loc) { ... };\n\nvar ast = Reflect.parse(\"var a = 4 + 7\", {builder: builder});\n\nLicense\nMIT X Licensed.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/aports', '\nAlpine Linux aports repository\nThis repository contains the APKBUILD files for each and every\nAlpine Linux package, along with the required patches and scripts,\nif any.\nIt also contains some extra files and directories related to testing\n(and therefore, building) those packages on GitHub (via Travis).\nIf you want to contribute, please read the\ncontributor guide\nand feel free to either submit a git patch on the Alpine aports\nmailing list (alpine-aports@lists.alpinelinux.org), or to submit a\npull request on GitHub.\nGit Hooks\nYou can find some useful git hooks in the .githooks directory.\nTo use them, run the following command after cloning this repository:\ngit config --local core.hooksPath .githooks\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/atom-rubinius-terminal', '\n\nRubinius Terminal\nOpens a terminal tab or pane within Atom that is configured to run Rubinius.\nThe main objective is to provide the simplest way to try Rubinius by providing direct access to an isolated install of Rubinius that does not interfere with a system Ruby or another Ruby switcher.\nAnother objective is creating better integration between the terminal and the editor without re-implementing terminal features in the editor or editor features in the terminal.\nNote: this project is alpha-stage. It is being developed on OS X first, but will eventually support Linux and Windows as well. For outstanding work, see the issues.\nThe Rubinius Terminal installs a binary build of Rubinius. When a terminal tab or pane is opened, the shell instance is configured so that Rubinius is the active Ruby.\nCode of Conduct\nParticipation in this project is governed by the Rubinius Code of Conduct.\nLicense\nRubinius Terminal is licensed under Mozilla Public License, 2.0. See the LICENSE file.\nThanks\nRubinius Terminal is heavily inspired by term2, term, and terminal. Thanks to the authors of those packages. The term.js library is used. The copyright notice for term.js is included in lib/vendor/term.js.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/atomy', '\nAtomy\nA DSL-oriented programming language targeting the Rubinius VM.\nIRC: #atomo on freenode\nAtomy provides the foundation for a language that grows with its users.\nMany languages end up being snapshots of their initial design goals. The\ndesigners bake features right into the language\'s core, rather than\nimplementing them as libraries. Eventually, this language may start to feel\nstale, and either mediocre or backwards-incompatible changes are introduced to\nbreathe new life into it. This often occurs at the syntax level.\nNot only does this lead to long transition periods, but it fragments the\nlanguage and your projects. The move towards a backwards-incompatible version\ncan be painfully slow. Hell, Ubuntu 12.04 still ships with Ruby 1.8, years\nafter 1.9 came around.\nAtomy avoids this by saying very little about language semantics at its core,\ninstead providing a system that you can use to build the language you want.\nThe core components are detailed below.\nSimple Grammar\nReaching Atomy\'s goals requires a stable, deceptively simple grammar. It says\nnothing about language semantics, instead defining primitive \"building blocks\"\nwhich, when composed together, form the notation of the language.\nThe various forms are as follows:\n\nWord: a, foo-bar\nConstant: Foo, FooBar123\nPrimitive: 1, 200\nLiteral: 2.0, \"foo\"\nList: [1, 2, 3]\nBlock: { a, b }, : a, b. The second notation is whitespace-aware.\nInfix: 1 + 1\nPostfix: no!\nPrefix: @foo\nCall: foo(bar, baz)\nCompose: 1 inspect\nQuote: \'foo\nQuasiQuote: `foo\nUnquote: ~foo\n\nNote that these say nothing about variables, messages, methods, or functions.\nBlocks also don\'t have arguments; the notation for this is actually built up\nby composing simpler forms together:\n[a, b]: a + b\n\nThis parses as a List composed with a Block. The AST looks like this:\nCompose\n  List\n    Word (a)\n    Word (b)\n  Block\n    Infix\n      Word (a)\n      Word (b)\n\nOn its own, this means nothing. If you try to compile this, you\'ll get an\nerror - things like Compose and Call have no meaning on their own. This is\nwhere macros come in.\nMacros!\nIf Atomy\'s AST is a bunch of Lego bricks pieced together, the macro system is\nthe imagination that gives it meaning.\nMacros are defined using patterns that match arbitrary expressions. This is in\ncontrast to most macro systems, which either use named macros (i.e. Lisps) or\nraw text substitution (C). Atomy\'s macros are nameless, and match on the AST\nitself, rather than source code.\nFor example, when a List is composed with a Block, we get a Block with\narguments:\nmacro([~*args]: ~*body):\n  Block new(node line, body, args)\n\nHere we\'re using splice unquotes (~*foo) to match the contents of the list\nand block, and creating a new block with the original\'s contents and the given\narguments. This macro is defined in the \"core\" Atomy library.\nNote that we\'re creating the Block manually; macros can return any object as\nlong as it knows how to compile itself. After expansion, nodes are sent\nbytecode(g, mod), where g is the code-generator and mod is the module\nbeing compiled. Users are free to define arbitrary nodes that do whatever they\nneed to at the bytecode level. This is how things like if-then-else are\nimplemented without being a primitive:\nmy-if-then-else = class:\n  def(initialize(@if, @then, @else)) {}\n\n  def(bytecode(gen, mod)):\n    else = gen new-label\n    done = gen new-label\n\n    mod compile(gen, @if)\n    gen goto-if-false(else)\n\n    mod compile(gen, @then)\n    gen goto(done)\n\n    else set!\n    mod compile(gen, @else)\n\n    done set!\n\nmacro(my-if(~x) then: ~*y; else: ~*z):\n  my-if-then-else new(x, `(do: ~*y), `(do: ~*z))\n\nmy-if(true)\n  then: puts(\"1\")\n  else: puts(\"0\")\n\nFor more information on the Rubinius VM bytecode, see the instruction\nset.\nNormally though you probably won\'t be digging into bytecode to write macros.\nFor most cases you\'ll probably just use Lisp-style quasiquotation:\nmacro(~x for(~*args) in(~c) when(~t)):\n  names [tmp]:\n    `(do:\n        ~tmp = []\n        ~c collect [~*args]:\n          when(~t):\n            ~tmp << ~x\n\n        ~tmp)\n\n(v * 3) for(v) in(0 .. 10) when(v even?)\n\nHere we\'re implementing Python-style list comprehensions. We use names to\ngenerate temporary variable names to avoid collision.\nClosures Everywhere\nEverything in Atomy is a closure. This differs from Ruby, where methods and\nclass/module bodies do not capture local variables.\nThere are many places this comes in useful, but in terms of other Atomy\nfeatures, it\'s good for assigning modules to variables at the top of a file,\nand defining helper functions for use in your exposed methods (see the\nfollowing section for more info there).\nCode Isolation\nFiles are Modules\nVaguely similar to CommonJS-style modules, require will result in a module\nobject, rather than evaluating the file in some global scope.\nMethods defined at the toplevel are defined on the file\'s module, and can be\ncalled by anyone requireing the file.\nMacros defined in a file are local to its module. For another module to use\nthem, they must call use rather than require. use will also bring the\nmodule\'s methods into the user.\nFor example, if we have a file a.ay:\nuse(\"atomy\")\nmacro(bar): 42\ndef(foo(a)): a + bar\n\nWe can do this to invoke the foo method:\nrequire(\"a\") foo(2) -- => 44\n\nOr we can use it to bring in its macros and methods:\nuse(\"a\")\nbar    -- => 42\nfoo(2) -- => 44\n\nMethods and Functions\nMethods are always public, and should be used only for things you want exposed\nas your API.\nFunctions replace private/helper methods. They are simply locals bound to\na block that gets called with self as the self of its caller. They can do\neverything a method can, except they are not bound to a class or a module. To\ndefine a function, use fn(x): ... instead of def(x): ....\nThis may seem a little odd coming from Ruby, but in practice it simplifies\nyour decision process when writing code. When defining something, it comes\ndown to one question: should this be part of my public API? If not, define it\nas a function. It doesn\'t matter where you put it, as long as it\'s in scope,\nand there\'s no concern of others using it in production.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/bundler-canary', '\nBundler Canary\nThis repository contains a Gemfile to assist troubleshooting issues with\nBundler and RubySL. The Gemfile includes:\nplatforms :rbx do\n  gem \'rubysl\'\nend\nContributing\n\nFork it\nCreate your feature branch (git checkout -b my-new-feature)\nCommit your changes (git commit -am \'Added some feature\')\nPush to the branch (git push origin my-new-feature)\nCreate new Pull Request\n\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/chaos', '\nChaos\nChaos is a multi-threaded, multi-process, highly parallel application exhibiting heavy object allocation, IO, and CPU-bound workloads. Chaos is an instrument to aid in testing and evaluating highly parallel system architectures.\nThe following components are essential elements of the chaos application:\n\nParallelism\nProcesses\nLogging\nCommunication\nEntropy\nAsymmetry\nAllocation\nFinalization\nCPU-bound workloads\nIO-bound workloads\nFFI\nC-API\n\nChaos is presently written in Ruby, but no guarantees are made that it will continue to be written in Ruby.\nInvoking Chaos\n$ git clone https://github.com/rubinius/chaos\n$ cd chaos\n$ # Select desired Rubinius version\n$ rbx chaos\n\nCode of Conduct\nParticipation in this project is governed by the Rubinius Code of Conduct.\nLicense\nThis project uses MPL-2.0. See the LICENSE file for details.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/chrb', '\nA Tool to Activate a Ruby Engine\nThe chrb utility activates an installed Ruby engine.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/chruby', '\nchruby\n\nChanges the current Ruby.\nFeatures\n\nUpdates $PATH.\n\nAlso adds RubyGems bin/ directories to $PATH.\n\n\nCorrectly sets $GEM_HOME and $GEM_PATH.\n\nUsers: gems are installed into ~/.gem/$ruby/$version.\nRoot: gems are installed directly into /path/to/$ruby/$gemdir.\n\n\nAdditionally sets $RUBY_ROOT, $RUBY_ENGINE, $RUBY_VERSION and\n$GEM_ROOT.\nOptionally sets $RUBYOPT if second argument is given.\nCalls hash -r to clear the command-lookup hash-table.\nFuzzy matching of Rubies by name.\nDefaults to the system Ruby.\nOptionally supports auto-switching and the .ruby-version file.\nSupports bash and zsh.\nSmall (~90 LOC).\nHas tests.\n\nAnti-Features\n\nDoes not hook cd.\nDoes not install executable shims.\nDoes not require Rubies be installed into your home directory.\nDoes not automatically switch Rubies by default.\nDoes not require write-access to the Ruby directory in order to install gems.\n\nRequirements\n\nbash >= 3 or zsh\n\nInstall\nwget -O chruby-0.3.9.tar.gz https://github.com/postmodern/chruby/archive/v0.3.9.tar.gz\ntar -xzvf chruby-0.3.9.tar.gz\ncd chruby-0.3.9/\nsudo make install\n\nPGP\nAll releases are PGP signed for security. Instructions on how to import my\nPGP key can be found on my blog. To verify that a release was not tampered\nwith:\nwget https://raw.github.com/postmodern/chruby/master/pkg/chruby-0.3.9.tar.gz.asc\ngpg --verify chruby-0.3.9.tar.gz.asc chruby-0.3.9.tar.gz\n\nsetup.sh\nchruby also includes a setup.sh script, which installs chruby. Simply run the\nscript as root or via sudo:\nsudo ./scripts/setup.sh\n\nHomebrew\nchruby can also be installed with homebrew:\nbrew install chruby\n\nOr the absolute latest chruby can be installed from source:\nbrew install chruby --HEAD\n\nArch Linux\nchruby is already included in the AUR:\nyaourt -S chruby\n\nFreeBSD\nchruby is included in the official FreeBSD ports collection:\ncd /usr/ports/devel/chruby/ && make install clean\n\nRubies\nManually\nChruby provides detailed instructions for installing additional Rubies:\n\nRuby\nJRuby\nRubinius\nMagLev\n\nruby-install\nYou can also use ruby-install to install additional Rubies:\nInstalling to /opt/rubies or ~/.rubies:\nruby-install ruby\nruby-install jruby\nruby-install rubinius\nruby-install maglev\n\nruby-build\nYou can also use ruby-build to install additional Rubies:\nInstalling to /opt/rubies:\nruby-build 1.9.3-p392 /opt/rubies/ruby-1.9.3-p392\nruby-build jruby-1.7.3 /opt/rubies/jruby-1.7.3\nruby-build rbx-2.0.0-rc1 /opt/rubies/rubinius-2.0.0-rc1\nruby-build maglev-1.0.0 /opt/rubies/maglev-1.0.0\n\nConfiguration\nAdd the following to the ~/.bashrc or ~/.zshrc file:\nsource /usr/local/share/chruby/chruby.sh\nSystem Wide\nIf you wish to enable chruby system-wide, add the following to\n/etc/profile.d/chruby.sh:\nif [ -n \"$BASH_VERSION\" ] || [ -n \"$ZSH_VERSION\" ]; then\n  source /usr/local/share/chruby/chruby.sh\n  ...\nfi\nThis will prevent chruby from accidentally being loaded by /bin/sh, which\nis not always the same as /bin/bash.\nRubies\nWhen chruby is first loaded by the shell, it will auto-detect Rubies installed\nin /opt/rubies/ and ~/.rubies/. After installing new Rubies, you must\nrestart the shell before chruby can recognize them.\nFor Rubies installed in non-standard locations, simply append their paths to\nthe RUBIES variable:\nsource /usr/local/share/chruby/chruby.sh\n\nRUBIES=(\n  /opt/jruby-1.7.0\n  \"$HOME/src/rubinius\"\n)\nMigrating\nIf you are migrating from another Ruby manager, set RUBIES accordingly:\nRVM\nRUBIES+=(~/.rvm/rubies/*)\nrbenv\nRUBIES+=(~/.rbenv/versions/*)\nrbfu\nRUBIES+=(~/.rbfu/rubies/*)\nAuto-Switching\nIf you want chruby to auto-switch the current version of Ruby when you cd\nbetween your different projects, simply load auto.sh in ~/.bashrc or\n~/.zshrc:\nsource /usr/local/share/chruby/chruby.sh\nsource /usr/local/share/chruby/auto.sh\nchruby will check the current and parent directories for a .ruby-version\nfile. Other Ruby switchers also understand this file:\nhttps://gist.github.com/1912050\nIf you want to automatically run the version of a gem executable specified in\nyour project\'s Gemfile, try\nrubygems-bundler.\nDefault Ruby\nIf you wish to set a default Ruby, simply call chruby in ~/.bash_profile or\n~/.zprofile:\nchruby ruby-1.9\n\nIf you have enabled auto-switching, simply create a .ruby-version file:\necho \"ruby-1.9\" > ~/.ruby-version\n\nRubyGems\nGems installed as a non-root user via gem install will be installed into\n~/.gem/$ruby/X.Y.Z.  By default, RubyGems will use the absolute path to the\ncurrently selected ruby for the shebang of any binstubs it generates.  In some\ncases, this path may contain extra version information (e.g.\nruby-2.0.0-p451).  To mitigate potential problems when removing rubies, you\ncan force RubyGems to generate binstubs with shebangs that will search for\nruby in your $PATH by using gem install --env-shebang (or the equivalent\nshort option -E).  This parameter can also be added to your gemrc file.\nIntegration\nFor instructions on using chruby with other tools, please see the wiki:\n\nCapistrano\nChef\nCron\nEmacs\nPow\nPuppet\nSudo\nVim\nFish\n\nExamples\nList available Rubies:\n$ chruby\n   ruby-1.9.3-p392\n   jruby-1.7.0\n   rubinius-2.0.0-rc1\n\nSelect a Ruby:\n$ chruby 1.9.3\n$ chruby\n * ruby-1.9.3-p392\n   jruby-1.7.0\n   rubinius-2.0.0-rc1\n$ echo $PATH\n/home/hal/.gem/ruby/1.9.3/bin:/opt/rubies/ruby-1.9.3-p392/lib/ruby/gems/1.9.1/bin:/opt/rubies/ruby-1.9.3-p392/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/home/hal/bin:/home/hal/bin\n$ gem env\nRubyGems Environment:\n  - RUBYGEMS VERSION: 1.8.23\n  - RUBY VERSION: 1.9.3 (2013-02-22 patchlevel 392) [x86_64-linux]\n  - INSTALLATION DIRECTORY: /home/hal/.gem/ruby/1.9.3\n  - RUBY EXECUTABLE: /opt/rubies/ruby-1.9.3-p392/bin/ruby\n  - EXECUTABLE DIRECTORY: /home/hal/.gem/ruby/1.9.3/bin\n  - RUBYGEMS PLATFORMS:\n    - ruby\n    - x86_64-linux\n  - GEM PATHS:\n     - /home/hal/.gem/ruby/1.9.3\n     - /opt/rubies/ruby-1.9.3-p392/lib/ruby/gems/1.9.1\n  - GEM CONFIGURATION:\n     - :update_sources => true\n     - :verbose => true\n     - :benchmark => false\n     - :backtrace => false\n     - :bulk_threshold => 1000\n     - \"gem\" => \"--no-rdoc\"\n  - REMOTE SOURCES:\n     - http://rubygems.org/\n\nSwitch to JRuby in 1.9 mode:\n$ chruby jruby --1.9\n$ ruby -v\njruby 1.7.0 (1.9.3p203) 2012-10-22 ff1ebbe on OpenJDK 64-Bit Server VM 1.7.0_09-icedtea-mockbuild_2012_10_17_15_53-b00 [linux-amd64]\n\nSwitch back to system Ruby:\n$ chruby system\n$ echo $PATH\n/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hal/bin\n\nRun a command under a Ruby with chruby-exec:\n$ chruby-exec jruby -- gem update\n\nSwitch to an arbitrary Ruby on the fly:\n$ chruby_use /path/to/ruby\n\nUninstall\nAfter removing the chruby configuration:\n$ sudo make uninstall\n\nAlternatives\n\nRVM\nrbenv\nrbfu*\nry\nruby-version*\n\n* Deprecated in favor of chruby.\nEndorsements\n\nyeah chruby is nice, does the limited thing of switching really good,\nthe only hope it never grows\n\n-- Michal Papis of RVM\n\nI just looooove chruby For the first time I\'m in total control of\nall aspects of my Ruby installation.\n\n-- Marius Mathiesen\n\nWritten by Postmodern, it\'s basically the simplest possible thing that can\nwork.\n\n-- Steve Klabnik\n\nSo far, I\'m a huge fan. The tool does what it advertises exactly and simply.\nThe small feature-set is also exactly and only the features I need.\n\n-- Patrick Brisbin\n\nI wrote ruby-version; however, chruby is already what ruby-version wanted to\nbe. I\'ve deprecated ruby-version in favor of chruby.\n\n-- Wil Moore III\nCredits\n\nmpapis for reviewing the code.\nhavenwood for handling the homebrew formula.\nzendeavor for style fixes.\n#bash, #zsh, #machomebrew for answering all my questions.\n\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/collateral', '\nRubinius Collateral\n\nLogos\nStickers\nShirts\nEtc\n\nLicense\n(cc) Some Rights Reserved 2011 Shane Becker\nAttribution-NoDerivatives 4.0 International — CC BY-ND 4.0\nYou are free:\nShare — copy and redistribute the material in any medium or format for any purpose, even commercially.\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\nUnder the following conditions:\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\nNoDerivatives — If you remix, transform, or build upon the material, you may not distribute the modified material.\nNo additional restrictions — You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.\nNotices:\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\nThis is a human-readable summary of (and not a substitute for) the license.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/daedalus-core', '\nDaedalus\nDaedalus |ˈdedl-əs| Greek Mythology\na craftsman, considered the inventor of carpentry, who is said to have\nbuilt the labyrinth for Minos, king of Crete. Minos imprisoned him and\nhis son Icarus, but they escaped using wings that Daedalus made and\nfastened with wax. Icarus, however, flew too near the sun and was killed.\nIn other words, he built things for a living.\nDaedalus is an extraction from the Rubinius build system. Very few of the\nfeatures have been implemented at this point. It is very very alpha.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/docker', '\n\nDocker Images for the Rubinius Ecosystem\nThis repository contains Dockerfiles for Rubinius itelf, as well as other parts of the ecosystem, such as the Rubinius Metrics image that combines InfluxDB and Grafana for a simple destination for Rubinius Metrics.\nCode of Conduct\nParticipation in this project is governed by the Rubinius Code of Conduct.\nLicense\nAttribution-NonCommercial-ShareAlike 4.0 International\n\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/ffi2', '\nFfi2\nTODO: Write a gem description\nInstallation\nAdd this line to your application\'s Gemfile:\ngem \'ffi2\'\n\nAnd then execute:\n$ bundle\n\nOr install it yourself as:\n$ gem install ffi2\n\nUsage\nTODO: Write usage instructions here\nContributing\n\nFork it\nCreate your feature branch (git checkout -b my-new-feature)\nCommit your changes (git commit -am \'Added some feature\')\nPush to the branch (git push origin my-new-feature)\nCreate new Pull Request\n\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/ffi2-generators', '\nFfi2::Generators\nTODO: Write a gem description\nInstallation\nAdd this line to your application\'s Gemfile:\ngem \'ffi2-generators\'\n\nAnd then execute:\n$ bundle\n\nOr install it yourself as:\n$ gem install ffi2-generators\n\nUsage\nTODO: Write usage instructions here\nContributing\n\nFork it\nCreate your feature branch (git checkout -b my-new-feature)\nCommit your changes (git commit -am \'Add some feature\')\nPush to the branch (git push origin my-new-feature)\nCreate new Pull Request\n\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/heroku-rbx-puma-rails-app', '\nHow to Use Rubinius and Puma on Heroku\nIn Three Steps\n\n\nInstall the Puma gem in your Gemfile\ngem \"puma\", \"~> 2.2.2\"\n\n\nTell Heroku to use Puma as your webserver in your Procfile (at the root of your app)\nweb: bundle exec puma -p $PORT\n\n\nSpecify Rubinius as your ruby engine in your Gemfile\nruby \"1.9.3\", :engine => \"rbx\", :engine_version => \"2.0.0.rc1\"\n\n\nConfirmation\nWhen you commit and push these changes to Heroku, if all goes well, you should see something this output:\n-----> Using Ruby version: ruby-1.9.3-rbx-2.0.0.rc1\n-----> Installing dependencies using Bundler version 1.3.0.pre.5\n       Ruby version change detected. Clearing bundler cache.\n       Old: ruby 1.9.2p290 (2011-07-09 revision 32553) [x86_64-linux]\n       New: rubinius 2.0.0.rc1 (1.9.3 release yyyy-mm-dd JI) [x86_64-unknown-linux-gnu]\n\nTo confirm that your app is really running on Rubinius 2.0, you can use the console.\nheroku run console\n\nFirst ask what the RUBY_ENGINE is\nRUBY_ENGINE\n\nYou should get back rbx\n\"rbx\"\n\nNext, ask what version of Rubinius is running\nRubinius.version\n\nIt should be something that starts with rubinius 2.0.0.rc1\n\"rubinius 2.0.0.rc1 (1.9.3 release yyyy-mm-dd JI) [x86_64-unknown-linux-gnu]\"\n\nThat\'s it! You\'re running Rubinius 2.0 on Heroku!\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/homebrew-apps', '\nRubinius Homebrew\nThese are Homebrew formula that provide useful\nRubinius-related packages (including Rubinius) that aren\'t acceptable to\nHomebrew. They have\nreasons.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/influxdb-grafana', '\ninfluxdb-grafana\nDocker container for using InfluxDB + Grafana to consume Rubinius Metrics StatsD output.\nUsage\n\n\nInstall docker.\n\n\nRun an instance of the container.  If this is the first time you\'re running it, docker will automatically download the stack of images that make up the container from the automated build of this repository.  The following command will run the container as a daemon (-d), mapping port 8125 of localhost to the container\'s 8125 statsd port, port 8086 of localhost to the container\'s 8086 influxdb query port, and port 80 of localhost to the container\'s 80 port serving the grafana interface.  Depending on how docker is installed, you may need to run this command with sudo.\ndocker run -d \\\n  -p 8125:8125/udp \\\n  -p 8086:8086 \\\n  -p 80:80 \\\n  rubinius/influxdb-grafana\n\n\nRun your application with Rubinius configured to output metrics to statsd at localhost:8125.\nRBXOPT=\"-Xsystem.metrics.target=statsd \\\n        -Xsystem.metrics.statsd.server=localhost:8125\" \\\n  rbx # (your app here)\n\n\nOpen the dashboard in your browser at http://localhost:80.  The grafana dashboard will run in your browser, sending queries for data to influxdb at http://localhost:8086.\n\n\nUsage on Remote Hosts\nThe usage instructions above assume that the docker container is being run and exposing its ports on the same host as the browser.  This is not likely to be true if you are running the container as a service to be connected to from other machines.  In that case, you must be sure to replace localhost in the steps above with the host name or IP address running the container with the correct exposed or forwarded port.\nAs mentioned in step 4 above, grafana runs in your browser, sending queries for data to influxdb at http://localhost:8086.  This also needs to be replaced with the correct remotely accessible address by passing an environment variable into the docker container.  For example, if the remote address in your setup is is rbx.myhost.com, the docker container should be run on that host with the command:\ndocker run -d \\\n  -p 8125:8125/udp \\\n  -p 8086:8086 \\\n  -p 80:80 \\\n  -e INFLUXDB_SERVER=http://rbx.myhost.com:8086 \\\n  rubinius/influxdb-grafana\nUsage on Mac OS X\nBecause docker relies on features of the Linux kernel, it does not run containers natively in Mac OS X - it hosts containers inside of a Linux VM called boot2docker.  One consequence of this is that the ports mapped to the docker host from containers are not mapped to localhost of OS X, but to the boot2docker host. Therefore, it must be treated like a \'remote host\' using the steps outlined above, substituting localhost with the IP address given the command boot2docker ip.  For example, if the boot2docker ip address in your setup is is 192.168.59.103, the docker container could be run with the command:\ndocker run -d \\\n  -p 8125:8125/udp \\\n  -p 8086:8086 \\\n  -p 80:80 \\\n  -e INFLUXDB_SERVER=http://192.168.59.103:8086 \\\n  rubinius/influxdb-grafana\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/license', '\nRubinus License\nAll the Rubinius project licenses in one convenient place.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/literate-ruby', '\nLiterate Ruby Specification\nThis is an experiment in \"literate specification\". Literate specification takes inspiration from \"literate programming\", an idea created by Donald Knuth:\n\nI believe that the time is ripe for significantly better documentation of programs, and that we can best achieve this by considering programs to be works of literature. Hence, my title: \"Literate Programming.\"\n\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.\n\n\nThe practitioner of literate programming can be regarded as an essayist, whose main concern is with exposition and excellence of style. Such an author, with thesaurus in hand, chooses the names of variables carefully and explains what each variable means. He or she strives for a program that is comprehensible because its concepts have been introduced in an order that is best for human understanding, using a mixture of formal and informal methods that reinforce each other.\n\n— Donald Knuth. \"Literate Programming (1984)\" in Literate Programming. CSLI, 1992, pg. 99. See http://www.literateprogramming.com.\nThe problem with documentation and code\nLiterate programming is a powerful idea. It refocuses the purpose of writing programs on the humans that are writing, reading, and maintaining the programs. It explicitly makes the narrative of the program primary, rather than the structure and organization of the program source code.\nUnfortunately, by combining these two very different things into one physical shape, one must necessarily be subordinate to the other. It is impossible to be otherwise. We much more commonly see the \"documentation\", in the form of comments embedded in source code, as the subordinate one. The flow and organization of the documentation is then subordinate to organization of the source code.\nWhile literate programming inverts this more commonly observed relationship between documentation and source code, it does not produce a notably more useful solution to the inherent tension. Documentation and source code have independent and separate proper forms. The former influenced by human cognition and the latter influenced by the particular mechanics of the programming language.\nWhile code and documentation are forever destined to be oil and water, there are two things that mix together very well: documentation and tests. If mixed just so, we have a good chance of creating a useful specification:\n\nspecification noun: an act of identifying or describing something precisely or of stating a precise requirement\n\n— Apple Dictionary\nA literate specification system\nAs defined here, a \"literate specification\" is comprised of five parts:\n\nThe human readable text of the specification, which is suitable for documentation.\nA special-purpose specification language with a defined logic that enables reasoning about the specification.\nExample source code that illustrates the specification. Execution of the example code results in a state that shows compliance or noncompliance with the specification the source code illustrates.\nA system that implements the specification and is architected such that all aspects of the specification are observable and controllable.\nA system that processes the literate specification and causes the example source code to be run on the system that implements the specification.\n\nCode of Conduct\nParticipation in this project is governed by the Rubinius Code of Conduct.\nOnline Discussion\n\nLicense\n© 2016 Brian Shirai\nFor any part of this work for which the license is applicable, this work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International license. See LICENSE.CC-BY-NC-ND-4.0.\n\nAny part of this work for which the CC-BY-NC-ND-4.0 license is not applicable is licensed under the Mozilla Public License 2.0. See LICENSE.MPL-2.0.\nAny part of this work that is known to be derived from an existing work is licensed under the license of that existing work. Where such license is known, the license text is included in the LICENSE.ext file, where \"ext\" indicates the license.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/melbourne', '\nRubinius Melbourne parser.\nThe repo is deprecated and will be replaced with melbourne18, melbourne19, and melbourne20 very soon.\n\n');
INSERT INTO `pinfo` (`login`, `avatar`, `followers`, `following`, `blog`, `public_repos`, `repos_url`, `created_at`, `rName`, `readme`) VALUES
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/mkrb', '\n\nComplain about the way other people make software by making software. — Andre Torrez\n\nmkrb: A Tool to Build or Install a Ruby Engine\nThe mkrb utility installs a pre-built binary of a Ruby engine for your\nplatform, or alternatively, builds the Ruby engine from source code.\nCode of Conduct\nParticipation in this project is governed by the Rubinius Code of Conduct.\nDependencies\nBash\nInstallation\nA tool to install things should not be hard to install. You can install mkrb in any of the following ways:\n\nwith RubyGems\nwith NPM\nwith Homebrew\nwith curl | bash\nfrom source code\n\nInstall with RubyGems\nIf you have any Ruby engine installed, installing mkrb is simple:\n$ gem install mkrb\n\nYou now have the mkrb command installed wherever RubyGems installs bin wrappers. That directory should be in your PATH, so you should be able to run mkrb on the command line.\nInstall with NPM\nIf you have NPM installed, installing mkrb is simple:\n$ npm install -g mkrb\n\nIf you don\'t want mkrb installed globally, remove the -g from the above command.\nInstall with Homebrew\nWe\'d really love to give you the ability to install with Homebrew but they have some odd rules about projects they accept. We\'ll work on this, but if you can help, that would be appreciated.\nInstall with curl | bash\nComing soon.\nInstall from source\nCopy the bin/mkrb script to wherever you want it.\nHelp\n$ mkrb -h\nUsage: mkrb engine [version] [options]\n\n  where:\n    engine                    The Ruby engine to install, in the following set:\n                              { rbx|rubinius, ruby, jruby }.\n    version                   The engine version to install. If no version is provided,\n                              installs the lastest stable release.\n\n    -b, --binary              Install a binary if it exists or fail. Without this option\n                              and without -m, installing a binary is the default, with\n                              automatic fallback to installing from source. With this\n                              option, only a binary is installed.\n    -m, --make                Build from source, do not install a binary\n    -s, --switcher SWITCHER   The Ruby engine switcher to install for, in the\n                              following set: { chrb, chruby, rbenv, rvm }.\n    -v, --version             Print the mkrb version.\n    -h, --help                Print this help.\n\n  examples:\n    $ mkrb rbx              # Installs the latest version of Rubinius, or prints that it\'s\n                            # already installed.\n\n    $ mkrb rbx -s chruby    # Same as above. The location for the install is the default for\n                            # the chruby Ruby switcher.\n\n    $ mkrb rbx 3.2          # Install version 3.2 of Rubinius.\n\n    $ mkrb rbx -b           # Install the latest Rubinius binary for this platform, or fail\n                            # if the binary doesn\'t exist.\n\n\nThe Problem\nEvery implementation of Ruby has a build system. Every. Single. One.\nThis simple fact is apparently of no particular interest to most package managers.\nThis fact suggests a relatively simple goal for a tool to install one or more implementations of Ruby: Put the thing where the user requested it, and don\'t do anything else.\nOr, in algorithmic pseudo code:\n\nPut\nthe thing\nwhere the user requested it\nPeriod\n\nWhy is this so hard?\nWhat The Heck, World?\nThere\'s the joke about the two hard problems in Computer Science. Naming\nthings, cache invalidation, and off-by-one errors are undeniably hard\nproblems. However, they have nothing on package management.\nThere is one hard problem in system maintenance: Installing a piece of\nsoftware.\nIt is such a stupid and hard problem that every single platform has multiple ways of doing it.\nThey hardly ever work well together.\nAnd most of them don\'t even work correctly all by themselves. They can\'t manage dependencies correctly. They can\'t upgrade and downgrade correctly. They can\'t interoperate with other package managers.\nThey can\'t interoperate with the OS defaults.\nThey can\'t interoperate with the file system\'s standards or defaults.\nThey can\'t interoperate with software installed from source.\nThey have terrible user experience.\nThis list is not exhaustive.\nThe Manager Hierarchy\nThere are actually hierarchies of package managers.\nThere\'s the uber-package manager for the whole system. These uber-package managers think they are God. They think their purpose in the world is to tell everyone else what to do. They all have really good reasons for this. Just ask any mainainer of one.\nThen there are specialty package managers for a single programming language, like RubyGems. They\'re built by people trying to escape the pain of many Gods. It shows.\nThere are package managers for implementations of a language, like ruby-build. They also think their tiny sliver of the world is the most important. It\'s not.\nThe uber-package managers hate the specialty package managers because they never respect the all-important uber-package managers.\nUsers of the specialty package managers hate the uber-package managers for making stupid decisions for a language they know nothing about.\nThe software authors hate everyone because no package manager correctly uses the software\'s own build system, which required tons of effort to make work across platforms.\nIt is such a stupid and easy problem that everyone thinks they know the best way to solve it. And they are certain their way is more correct compared to all the others. In fact, they usually just suck less in one or a few limited areas, and they offset this suck-less-ness with terrible decisions in other places.\nThe solution: There is no solution. It\'s like trying to solve religion.\nThere is only one dismal hope: Make a thing that works as well as it can in a crazy world for the few people who will use it.\nA Solution\nJust because hope is dim does not require us to abandon the world or suffer with existing solutions when we can clearly see what rather simple thing would make our day a bit brighter.\nAssumptions and Principles\nTo inform our decisions, a dash of assumptions and a few principles will go a long way.\nAssumptions:\n\nthe user knows what she wants\nthe software knows how to build itself\nall software has dependencies\n\nPrinciples:\n\ndo the fewest number of things\nuse the simplest defaults\ndo only what the user explicitly requested\nmodularize\nisolate\nask, don\'t tell\n\nApplication of these principles and assumptions should ensure, for instance, that changing unrelated code doesn\'t repeatedly break other code. Or that we never assume when the user requests that we install Ruby that they actually wanted us to install a new package manager for them. Seriously, why? Or that we somehow know better than the software authors how to build their software.\nHow mkrb Works\nThe mkrb tool prefers to install a binary.\n\nIf the requested Ruby engine + engine version is not installed, it will start by looking in a local cache directory.\nIf a matching binary is found, it will be installed.\nIf no matching binary is found, then it will look at distinguished places on the internet.\nIf a binary is found, it will be installed.\nIf no binary is found, it will attempt to locate a source tarball matching the request.\nIf that is found, mkrb will attempt to resolve dependencies and build.\n\nIf the user requests that only a binary or only a source build are installed, mkrb will only install the requested source.\nmkrb uses the Internet\nThe mkrb tool will never include, directly in its own sourcecode, the SHAs, digests, or any other specific version information. It uses patterns to locate resources. It follows redirects. If it caches information, it will update the cache transparently when the user asks to install.\nThis is not a hard problem. It\'s a solved problem.\nLicense\nThis project uses MPL-2.0. See the LICENSE file for details.\nCredits\nThanks to all the others for making it so painfully obvious over the years about what to not ever, ever do.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/openssl-canary', '\nTest whether Rubinis binaries for Travis can load OpenSSL.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/rubinius', '\n \nThe Rubinius Language Platform\nRubinius is a modern language platform that supports a number of programming languages.\nRubinius includes a bytecode virtual machine, generational garbage collector, and just-in-time (JIT) native machine code compiler. Rubinius provides concurrency support via native OS threads with no global interpreter lock.\nRubinius runs on Mac OS X and many Unix/Linux operating systems. Microsoft Windows is not yet supported.\nCode of Conduct\nParticipation in the Rubinius project is governed by the Rubinius Code of Conduct.\nThe Ruby Programming Language\nMany popular Ruby applications, like Rails, run on Rubinius, which aims to be compatible with Ruby version 2.3.1.\nRubinius includes a Ruby parser, Ruby bytecode compiler, Ruby core library, and C-API compatibility for native C extensions. The Ruby core library is written almost entirely in Ruby. The Ruby bytecode compiler and other tools, such as the debugger, are also written in Ruby.  Rubinius provides the standard Ruby libraries, with the following exceptions:\n\nContinuation\nRipper\nTracePoint\nTracer\n\nThe following Ruby features are not supported on Rubinius:\n\nRefinements\n$SAFE levels\n\nLicense\nAll source code in this repository is subject to the terms of the Mozilla Public License, version 2.0 unless stated otherwise. A copy of this license can be found the file \"LICENSE\" or at https://www.mozilla.org/MPL/2.0/.\nContributions made prior to January 3rd, 2016 are licensed under the old BSD 3-clause license. A copy of this license can be found in the file \"BSD_LICENSE\".\nIn the event a directory contains a \"LICENSE\", \"LICENSE.txt\" or \"COPYING\" file the license specified in said file applies to the contents of said directory and all sub directories, overwriting the licenses specified above.\nInstalling Rubinius from Source\nPlease see https://book.rubinius.com/manuscript/getting_rubinius.html\nfor a more complete guide to installing Rubinius from source.\nTo install Rubinius, use the following steps:\n\nEnsure that MRI 2.0+, rubygems, rake, git and LLVM are installed\ngit clone git://github.com/rubinius/rubinius.git\ncd rubinius\n./build.sh --prefix=/path/to/install/dir\n\nWhen the install process finishes, follow the directions printed to the terminal to add the Rubinius executable (bin) directory to your PATH.\nUsing RubyGems\nRubinius comes with RubyGems built-in. To install a gem, run the following:\n$ rbx -S gem install <gem_name>\n\nDocumentation\nThe Rubinius documentation is the Rubinius book.\nIssues & Support\nPlease file tickets for bugs or problems.\nFor additional help, visit the Rubinius Gitter chat room.\nContributing\nThe Rubinius team welcomes contributions. For more information, read the contributing file.\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/rubinius-benchmark', '\nBenchmark\nThis is a dump of the benchmark/ directory that was in the Rubinius\nrepository.\nTODO\n\nThrow out the crap.\nCreate a proper benchmarking framework.\nDevelop a useful range of benchmarks.\n\n\n'),
('rubinius', 'https://avatars2.githubusercontent.com/u/317747?v=4', 0, 0, 'https://rubinius.com', 51, 'https://api.github.com/users/rubinius/repos', 2010, 'rubinius/rubinius-book', '\nThe Rubinius Book\nThis is a book about building a community and language platform.\nThe book is about building a community because all technology is embedded in a human context. We must deliberately build and nurture the community if we want it to have desirable characteristics.\nThe book is about building a language platform because languages are the most powerful tools we have created to process and share information. Technology is essential to advancing our understanding of the world and solving problems. Human health and well-being is intimately intertwined with our developing technology.\nIn the span of human history, computers are very young. The Internet is even younger, but has unprecedented power to connect people across vast physical distances, cultures, languages, political systems, and belief systems. Those of us enjoying the tremendous privileges of modern life are obligated to help other people use these systems for improving, not just changing, the world.\nThis is a book about one small attempt to do that.\nCode of Conduct\nParticipation in the Rubinius book project is governed by the Rubinius Code of Conduct.\nOnline Discussion\n\nLicense\nCopyright © 2016 Rubinius, Inc.\nAttribution-NonCommercial-ShareAlike 4.0 International \n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/analytics-ruby', '\nanalytics-ruby\n\nanalytics-ruby is a ruby client for Segment.io\nDocumentation\nDocumentation is available at segment.io/libraries/ruby\nLicense\nWWWWWW||WWWWWW\n W W W||W W W\n      ||\n    ( OO )__________\n     /  |           \\\n    /o o|    MIT     \\\n    \\___/||_||__||_|| *\n         || ||  || ||\n        _||_|| _||_||\n       (__|__|(__|__|\n\n(The MIT License)\nCopyright (c) 2013 Segment.io Inc. friends@segment.io\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \'Software\'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \'AS IS\', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/api-blueprint', '\n\nAPI Blueprint\nAPI Design for Humans\nAPI Blueprint is a documentation-oriented API description language. A couple of semantical assumptions over the plain Markdown.\nAPI Blueprint is perfect for designing your Web API and its comprehensive documentation but also for quick prototyping and collaboration. It is easy to learn and even easier to read; after all it is just a form Markdown.\nTL;DR\n\nWeb API description language\nPure Markdown\nDesigned for humans\nUnderstandable by machines\n\nGetting started with API Blueprint\nAll it really takes to describe an endpoint of your API is write something like this:\n# GET /message\n+ Response 200 (text/plain)\n    \n        Hello World!\nin your favorite Markdown editor. Now you can share and discuss this API in your API repository and let GitHub to render the API documentation so others can see it.\nJump directly to the API Blueprint Tutorial or browse the interactive examples to learn more about the API Blueprint syntax.\nDescribing your API is only the start. The API Blueprint can be used by variety of tools from interactive documentation and code generators to API testing tools thanks to its machine-friendly face:\n{\n  \"_version\": \"1.0\",\n  \"metadata\": {},\n  \"name\": \"\",\n\n    ...\n[full listing]\nIt is the task of the native API Blueprint parser or one of its bindings to \"translate\" the API Blueprint Markdown representation into a machine friendly format – AST.\nVisit the tooling section of the API Blueprint website to find more about the actual tools or check the Developing tools for API Blueprint article if you are interested in using API Blueprint in your tool chain.\nLearn more\n\nAPI Blueprint Tutorial\nAPI Blueprint Examples\nAPI Blueprint Glossary of Terms\nAPI Blueprint Language Specification\nTools working with API Blueprint\n\nDevelopers\n\nAPI Blueprint reference parser – Snow Crash\nSnow Crash Bindings to other languages\nAPI Blueprint AST Serialization Media Types\nDeveloping tools for API Blueprint\n\nFuture of API Blueprint\nFind about the future of API Blueprint in its Milestones.\nContribute\nFork & pull request.\nHave a question?\nAsk at Stack Overflow, make sure to use the apiblueprint tag. Alternatively, mention @apiblueprint on Twitter.\nCheck out the API Blueprint Issues Page for planned features, API Blueprint and issues discussion.\nLicense\nMIT License. See the LICENSE file.\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/apollo-client', '\nApollo client  \nApollo Client is a fully-featured caching GraphQL client with integrations for React, Angular, and more. It allows you to easily build UI components that fetch data via GraphQL. To get the most value out of apollo-client, you should use it with one of its view layer integrations.\nTo get started with the React integration, go to our React Apollo documentation website.\nApollo Client also has view layer integrations for all the popular frontend frameworks. For the best experience, make sure to use the view integration layer for your frontend framework of choice.\nApollo Client can be used in any JavaScript frontend where you want to use data from a GraphQL server. It\'s:\n\nIncrementally adoptable, so that you can drop it into an existing JavaScript app and start using GraphQL for just part of your UI.\nUniversally compatible, so that Apollo works with any build setup, any GraphQL server, and any GraphQL schema.\nSimple to get started with, so you can start loading data right away and learn about advanced features later.\nInspectable and understandable, so that you can have great developer tools to understand exactly what is happening in your app.\nBuilt for interactive apps, so your users can make changes and see them reflected in the UI immediately.\nSmall and flexible, so you don\'t get stuff you don\'t need. The core is under 25kb compressed.\nCommunity driven, because Apollo is driven by the community and serves a variety of use cases. Everything is planned and developed in the open.\n\nGet started on the home page, which has great examples for a variety of frameworks.\nInstallation\n# installing the preset package\nnpm install apollo-client-preset graphql-tag graphql --save\n# installing each piece independently\nnpm install apollo-client apollo-cache-inmemory apollo-link-http graphql-tag graphql ---save\nTo use this client in a web browser or mobile app, you\'ll need a build system capable of loading NPM packages on the client. Some common choices include Browserify, Webpack, and Meteor 1.3+.\nInstall the Apollo Client Developer tools for Chrome for a great GraphQL developer experience!\nUsage\nYou get started by constructing an instance of the core class ApolloClient. If you load ApolloClient from the apollo-client-preset package, it will be configured with a few reasonable defaults such as our standard in-memory cache and a link to a GraphQL API at /graphql.\nimport ApolloClient from \'apollo-client-preset\';\n\nconst client = new ApolloClient();\nTo point ApolloClient at a different URL, just create your own HttpLink instance, like so, replacing https://graphql.example.com with your GraphQL API\'s URL:\nimport { ApolloClient } from \'apollo-client\';\nimport { HttpLink } from \'apollo-link-http\';\nimport { InMemoryCache } from \'apollo-cache-inmemory\';\n\nconst client = new ApolloClient({\n  link: new HttpLink({ uri: \'https://graphql.example.com\' }),\n  cache: new InMemoryCache()\n});\nMost of the time you\'ll hook up your client to a frontend integration. But if you\'d like to directly execute a query with your client, you may now call the client.query method like this:\nimport gql from \'graphql-tag\';\n\nclient.query({\n  query: gql`\n    query TodoApp {\n      todos {\n        id\n        text\n        completed\n      }\n    }\n  `,\n})\n  .then(data => console.log(data))\n  .catch(error => console.error(error));\nNow your client will be primed with some data in its cache. You can continue to make queries, or you can get your client instance to perform all sorts of advanced tasks on your GraphQL data. Such as reactively watching queries with watchQuery, changing data on your server with mutate, or reading a fragment from your local cache with readFragment.\nTo learn more about all of the features available to you through the apollo-client package, be sure to read through the apollo-client API reference.\nLearn how to use Apollo Client with your favorite framework\n\nReact\nAngular\nVue\nEmber\nPolymer\nMeteor\nBlaze\nVanilla JS\nNext.js\n\n\nContributing\n\n\n\nRead the Apollo Contributor Guidelines.\nRunning tests locally:\n# nvm use node\nnpm install\nnpm test\n\nThis project uses TypeScript for static typing and TSLint for linting. You can get both of these built into your editor with no configuration by opening this project in Visual Studio Code, an open source IDE which is available for free on all platforms.\nImportant discussions\nIf you\'re getting booted up as a contributor, here are some discussions you should take a look at:\n\nStatic typing and why we went with TypeScript also covered in the Medium post\nIdea for pagination handling\nDiscussion about interaction with Redux and domain vs. client state\nLong conversation about different client options, before this repo existed\n\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/blueprint', '\n\nBlueprint \nBlueprint is a React UI toolkit for the web.\nIt is optimized for building complex, data-dense web interfaces for desktop applications.\nIf you rely heavily on mobile interactions and are looking for a mobile-first UI toolkit, this may not be for you.\nView the full documentation ▸\nRead our FAQ on the wiki ▸\nPackages\nThis repository contains multiple projects in the packages/ directory that are distributed as separate packages on NPM:\n – Core styles & components.\n – Components for interacting with dates and times.\n – Scalable interactive table component.\nThe other packages (docs and landing) are not published to NPM as they are used to build the documentation site.\nDevelopment\nWe use Lerna to manage inter-package dependencies in this monorepo.\nBuilds are orchestrated via Gulp tasks.\nPrerequisite: Node.js v6 or v7\n\ngit clone this repository (or fork if you lack permissions)\nnpm install to install build dependencies\nnpm run bootstrap to install and link each package using Lerna\nnpm run gulp to compile and start the server and watcher\nOpen your browser to localhost:9000/packages/docs/dist/\n\nContributing\nLooking for places to contribute to the codebase? Check out the\nStatus: accepting PRs label.\nRead about our contribution guidelines and\ndevelopment practices to give your PR\nits best chance at getting merged.\nLicense\nThis project is made available under the BSD License.\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/CardMaven', '\nCardMaven\nAn iOS card game app that allows for multiple variants of games, multi-player over bluetooth, and much more...\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/cube', '\nCube\n               __\n              /\\ \\\n  ___   __  __\\ \\ \\____    ____\n / ___\\/\\ \\/\\ \\\\ \\  __ \\  / __ \\\n/\\ \\__/\\ \\ \\_\\ \\\\ \\ \\_\\ \\/\\  __/\n\\ \\____\\\\ \\____/ \\ \\____/\\ \\____\\\n \\/____/ \\/___/   \\/___/  \\/____/\n\nSee http://square.github.com/cube for an introduction.\nSee http://github.com/square/cube/wiki for documentation.\nThank You\nCube is built with the following open-source systems and libraries:\n\nD3.js\nMongoDB\nnode-mongodb-native\nNode.js\nPEG.js\nVows\nwebsocket-client\nwebsocket-server\n\nContributing\nWe\'d love for you to participate in the development of Cube. Before we can accept your pull request, please sign our Individual Contributor License Agreement. It\'s a short form that covers our bases and makes sure you\'re eligible to contribute. Thank you!\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/deepo', '\n\n\n\nDeepo is a Docker image with a full reproducible deep learning research environment. It contains most popular deep learning frameworks:\ntheano,\ntensorflow,\nsonnet,\npytorch,\nkeras,\nlasagne,\nmxnet,\ncntk,\nchainer,\ncaffe,\ntorch.\n\nQuick Start\n\nInstallation\nUsage\n\n\nComparison to Alternatives\nLicensing\n\n\n\nQuick Start\n\nInstallation\nStep 1. Install Docker and nvidia-docker.\nStep 2. Obtain the Deepo image\nYou can either directly download the image from Docker Hub, or build the image yourself.\nOption 1: Get the image from Docker Hub (recommended)\ndocker pull ufoym/deepo\nOption 2: Build the Docker image locally\ngit clone https://github.com/ufoym/deepo.git\ncd deepo && docker build -t ufoym/deepo .\nNote that this may take several hours as it compiles a few libraries from scratch.\n\nUsage\nNow you can try this command:\nnvidia-docker run --rm ufoym/deepo nvidia-smi\nThis should work and enables Deepo to use the GPU from inside a docker container.\nIf this does not work, search the issues section on the nvidia-docker GitHub -- many solutions are already documented. To get an interactive shell to a container that will not be automatically deleted after you exit do\nnvidia-docker run -it ufoym/deepo bash\nIf you want to share your data and configurations between the host (your machine or VM) and the container in which you are using Deepo, use the -v option, e.g.\nnvidia-docker run -it -v /host/data:/data -v /host/config:/config ufoym/deepo bash\nThis will make /host/data from the host visible as /data in the container, and /host/config as /config. Such isolation reduces the chances of your containerized experiments overwriting or using wrong data.\nYou are now ready to begin your journey.\ntensorflow\n$ python\n>>> import tensorflow\n>>> print(tensorflow.__name__, tensorflow.__version__)\ntensorflow 1.3.0\nsonnet\n$ python\n>>> import sonnet\n>>> print(sonnet.__name__, sonnet.__path__)\nsonnet [\'/usr/local/lib/python3.5/dist-packages/sonnet\']\npytorch\n$ python\n>>> import torch\n>>> print(torch.__name__, torch.__version__)\ntorch 0.2.0_3\nkeras\n$ python\n>>> import keras\n>>> print(keras.__name__, keras.__version__)\nkeras 2.0.8\nmxnet\n$ python\n>>> import mxnet\n>>> print(mxnet.__name__, mxnet.__version__)\nmxnet 0.11.0\ncntk\n$ python\n>>> import cntk\n>>> print(cntk.__name__, cntk.__version__)\ncntk 2.2\nchainer\n$ python\n>>> import chainer\n>>> print(chainer.__name__, chainer.__version__)\nchainer 3.0.0\ntheano\n$ python\n>>> import theano\n>>> print(theano.__name__, theano.__version__)\ntheano 0.10.0beta4+14.gb6e3768\nlasagne\n$ python\n>>> import lasagne\n>>> print(lasagne.__name__, lasagne.__version__)\nlasagne 0.2.dev1\ncaffe\n$ python\n>>> import caffe\n>>> print(caffe.__name__, caffe.__version__)\ncaffe 1.0.0\n$ caffe --version\ncaffe version 1.0.0\n\ntorch\n$ th\n │  ______             __   |  Torch7\n │ /_  __/__  ________/ /   |  Scientific computing for Lua.\n │  / / / _ \\/ __/ __/ _ \\  |  Type ? for help\n │ /_/  \\___/_/  \\__/_//_/  |  https://github.com/torch\n │                          |  http://torch.ch\n │\n │th>\n\n\nComparison to alternatives\n\n\n\n.\nmodern-deep-learning\ndl-docker\njupyter-deeplearning\nDeepo\n\n\n\n\nubuntu\n16.04\n14.04\n14.04\n16.04\n\n\ncuda\n❌\n8.0\n6.5-8.0\n8.0\n\n\ncudnn\n❌\nv5\nv2-5\nv6\n\n\ntheano\n❌\n✔️\n✔️\n✔️\n\n\ntensorflow\n✔️\n✔️\n✔️\n✔️\n\n\nsonnet\n❌\n❌\n❌\n✔️\n\n\npytorch\n❌\n❌\n❌\n✔️\n\n\nkeras\n✔️\n✔️\n✔️\n✔️\n\n\nlasagne\n❌\n✔️\n✔️\n✔️\n\n\nmxnet\n❌\n❌\n❌\n✔️\n\n\ncntk\n❌\n❌\n❌\n✔️\n\n\nchainer\n❌\n❌\n❌\n✔️\n\n\ncaffe\n✔️\n✔️\n✔️\n✔️\n\n\ntorch\n❌\n✔️\n✔️\n✔️\n\n\nLicensing\nDeepo is MIT licensed.\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/Faster-RCNN_CloudML', '\nFaster-RCNN for Cloud ML\nThis is a fork of @smallcorgi\'s experimental Tensorflow implementation of Faster RCNN made to run on Google Cloud Machine Learning.  For details about R-CNN please refer to the paper Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks by Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun.\nInstallation\nCloud\nCreate a Google Cloud Project and enable Cloud ML as described here\nLocal\nThis code should support running without a GPU / CUDA, but if compilation blows up I would not be suprised.  Ideally you\'ll test your code locally on a machine with CUDA.  In which case, be sure to set the CUDAHOME env variable, it\'s usually /usr/local/cuda.  It\'s also probably a good idea to use virtualenv.  Once you\'ve activated your virtualenv, it should be as easy as:\ngit clone https://github.com/vanpelt/Faster-RCNN_CloudML\nexport CUDAHOME=/usr/local/cuda\npip install tensorflow\ncd lib\npython setup.py build_ext --inplace\nHardware Requirements\nFor training the end-to-end version of Faster R-CNN with VGG16, 3G of GPU memory is sufficient (using CUDNN)\nDemo\nWe modified this library to support data generated by the CrowdFlower annotation tool.  We asked the crowd to draw boxes around monkey\'s.  There is a crowdflower module in the datasets directory which converts the data to imdb format.  To use you\'ll need to copy our dataset to your bucket as described below.\nWe\'ll use the cloud-ml.sh script to submit jobs.  Before we can submit jobs we have to setup our google cloud storage bucket.  All jobs are run within a namespace, we\'ll call ours demo.  The following commands upload data to the google cloud storage bucket created for our machine learning project described above.\nexport MY_PROJECT_NAME=funky_monkey\nexport NAME=demo\ngsutils cp config/cfg.yml.sample gs://$MY_PROJECT_NAME-ml/$NAME/cfg.yml\ngsutils cp config/class_names.json.sample gs://$MY_PROJECT_NAME-ml/$NAME/class_names.json\ngsutils -m cp gs://crowdflower-machine-vision/monkey/* gs://$MY_PROJECT_NAME-ml/$NAME/\ngsutils -m cp gs://crowdflower-machine-vision/VGG_imagenet.npy gs://$MY_PROJECT_NAME-ml/$NAME/\nGenerally it\'s a good idea to test things locally because CloudML takes 5-10 minutes to startup.  To test training locally run:\n./cloud-ml -l demo\nIf that looks good, let\'s send it to the ☁️.\n./cloud-ml demo\nIf you\'re feeling ambitious, you can run other datasets defined in the datasets directory.  For instance, here\'s how to run pascal_voc\nexport NAME=demo_voc\ngsutils cp config/voc_cfg.yml.sample gs://$MY_PROJECT_NAME-ml/$NAME/cfg.yml\ngsutils -m cp gs://crowdflower-machine-vision/VOC2007 gs://$MY_PROJECT_NAME-ml/$NAME/\ngsutils -m cp gs://crowdflower-machine-vision/VGG_imagenet.npy gs://$MY_PROJECT_NAME-ml/$NAME/\n./cloud-ml -v demo_voc\nReferences\nFaster-RCNN_TF\nFaster R-CNN caffe version\nA tensorflow implementation of SubCNN (working progress)\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/guestlist', '\nConfirm your spot in the Nightmix party\nThe Nightmix party is free for engineers, but how do you prove you are an engineer?\nSimple, just add yourself to the engineering guest list, which happens to be this github repository.\nInstructions\n\n\nCreate a file in the guests/ directory with the name First_Last, where First is your first name and Last is your last name.\n\n\nThe file should be 2 lines long, with the following format:\nName: Your Name\nCompany: Your employer\n\n\nSubmit a pull request to github with your commit.  Once your pull request is approved, your spot is confirmed.\n\n\nRemember: you must also register with Eventbrite with the same github username which actually performed the commit.\n\n\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/js-data-cloud-datastore', '\n\njs-data-cloud-datastore\n\n\n\n\n\nGoogle Cloud Datastore adapter for js-data.\nNote: This adapter is in beta, and uses the 3.0 beta version of js-data.\nTo get started, visit http://js-data.io.\nLinks\n\nQuick start - Get started in 5 minutes\nGuides and Tutorials - Learn how to use JSData\nCloudDatastoreAdapter Guide - Learn how to use CloudDatastoreAdapter\nAPI Reference Docs - Explore components, methods, options, etc.\nCommunity & Support - Find solutions and chat with the community\nGeneral Contributing Guide - Give back and move the project forward\n\nContributing to js-data-cloud-datastore\n\n\n\nLicense\nApache Version 2.0\nCopyright (c) 2016 js-data-cloud-datastore project authors\n\nLICENSE\nAUTHORS\nCONTRIBUTORS\n\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/js-segment-annotator', '\nJS Segment Annotator\nJavascript image annotation tool based on image segmentation.\n\nLabel image regions with mouse.\nWritten in vanilla Javascript, with require.js dependency (packaged).\nPure client-side implementation of image segmentation.\n\nA browser must support HTML canvas to use this tool.\nThere is an online demo.\nImporting data\nPrepare a JSON file that looks like the following. The required fields are\nlabels and imageURLs. The annotationURLs are for existing data and can\nbe omitted. Place the JSON file inside the data/ directory.\n{\n  \"labels\": [\n    \"background\",\n    \"skin\",\n    \"hair\",\n    \"dress\",\n    \"glasses\",\n    \"jacket\",\n    \"skirt\"\n  ],\n  \"imageURLs\": [\n    \"data/images/1.jpg\",\n    \"data/images/2.jpg\"\n  ],\n  \"annotationURLs\": [\n    \"data/annotations/1.png\",\n    \"data/annotations/2.png\"\n  ]\n}\n\nThen edit main.js to point to this JSON file. Open a Web browser and visit\nindex.html.\nKnow issues\nBrowser incompatibility\nA segmentation result can greatly differ due to the difference in Javascript\nimplementation across Web browsers. The difference stems from numerical\nprecision of floating point numbers, and there is no easy way to produce the\nexact same result across browsers.\nMatlab tips\nAnnotation PNG\nThe annotation PNG file contains label map encoded in RGB value. Do the\nfollowing to encode an index map.\nEncode:\nX = cat(3, bitand(annotation, 255), ...\n           bitand(bitshift(annotation, -8), 255), ...\n           bitand(bitshift(annotation, -16)), 255));\nimwrite(uint8(X), \'data/annotations/0.png\');\n\nDecode:\nX = imread(\'data/annotations/0.png\');\nannotation = X(:, :, 1);\nannotation = bitor(annotation, bitshift(X(:, :, 2), 8));\nannotation = bitor(annotation, bitshift(X(:, :, 3), 16));\n\nJSON\nUse the matlab-json package.\n\nhttps://github.com/kyamagu/matlab-json\n\nUsing dataURL\nGet the byte encoding tools.\n\nhttps://www.mathworks.com/matlabcentral/fileexchange/39526-byte-encoding-utilities\n\nDo the following to convert between dataURL and Matlab format.\nEncode:\npng_data = imencode(annotation, \'png\');\ndataURL = [\'data:image/png;base64,’, base64encode(png_data)];\n\nDecode:\ndataURL = \'data:image/png;base64,...\';\npng_data = base64decode(strrep(dataURL, \'data:image/png;base64,’, ‘’));\nannotation = imdecode(png_data, ‘png’);\n\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/KittiSeg', '\nKittiSeg\nKittiSeg performs segmentation of roads by utilizing an FCN based model. The model achieved first place on the Kitti Road Detection Benchmark at submission time. Check out our paper for a detailed model description.\n  \n  \nThe model is designed to perform well on small datasets. The training is done using just 250 densely labelled images. Despite this a state-of-the art MaxF1 score of over 96% is achieved. The model is usable for real-time application. Inference can be performed at the impressive speed of 95ms per image.\nThe repository contains code for training, evaluating and visualizing semantic segmentation in TensorFlow. It is build to be compatible with the TensorVision back end which allows to organize experiments in a very clean way. Also check out KittiBox a similar projects to perform state-of-the art detection. And finally the MultiNet repository contains code to jointly train segmentation, classification and detection. KittiSeg and KittiBox are utilized as submodules in MultiNet.\nRequirements\nThe code requires Tensorflow 1.0, python 2.7 as well as the following python libraries:\n\nmatplotlib\nnumpy\nPillow\nscipy\ncommentjson\n\nThose modules can be installed using: pip install numpy scipy pillow matplotlib commentjson or pip install -r requirements.txt.\nSetup\n\nClone this repository: git clone https://github.com/MarvinTeichmann/KittiSeg.git\nInitialize all submodules: git submodule update --init --recursive\n[Optional] Download Kitti Road Data:\n\nRetrieve kitti data url here: http://www.cvlibs.net/download.php?file=data_road.zip\nCall python download_data.py --kitti_url URL_YOU_RETRIEVED\n\n\n\nRunning the model using demo.py does not require you to download kitti data (step 3). Step 3 is only required if you want to train your own model using train.py or bench a model agains the official evaluation score evaluate.py. Also note, that I recommend using download_data.py instead of downloading the data yourself. The script will also extract and prepare the data. See Section Manage data storage if you like to control where the data is stored.\nTo update an existing installation do:\n\nPull all patches: git pull\nUpdate all submodules: git submodule update --init --recursive\n\nIf you forget the second step you might end up with an inconstant repository state. You will already have the new code for KittiSeg but run it old submodule versions code. This can work, but I do not run any tests to verify this.\nTutorial\nGetting started\nRun: python demo.py --input_image data/demo/demo.png to obtain a prediction using demo.png as input.\nRun: python evaluate.py to evaluate a trained model.\nRun: python train.py --hypes hypes/KittiSeg.json to train a model using Kitti Data.\nIf you like to understand the code, I would recommend looking at demo.py first. I have documented each step as  	thoroughly as possible in this file.\nManage Data Storage\nKittiSeg allows to separate data storage from code. This is very useful in many server environments. By default, the data is stored in the folder KittiSeg/DATA and the output of runs in KittiSeg/RUNS. This behaviour can be changed by setting the bash environment variables: $TV_DIR_DATA and $TV_DIR_RUNS.\nInclude  export TV_DIR_DATA=\"/MY/LARGE/HDD/DATA\" in your .profile and the all data will be downloaded to /MY/LARGE/HDD/DATA/data_road. Include export TV_DIR_RUNS=\"/MY/LARGE/HDD/RUNS\" in your .profile and all runs will be saved to /MY/LARGE/HDD/RUNS/KittiSeg\nRUNDIR and Experiment Organization\nKittiSeg helps you to organize large number of experiments. To do so the output of each run is stored in its own rundir. Each rundir contains:\n\noutput.log a copy of the training output which was printed to your screen\ntensorflow events tensorboard can be run in rundir\ntensorflow checkpoints the trained model can be loaded from rundir\n[dir] images a folder containing example output images. image_iter controls how often the whole validation set is dumped\n[dir] model_files A copy of all source code need to build the model. This can be very useful of you have many versions of the model.\n\nTo keep track of all the experiments, you can give each rundir a unique name with the --name flag. The --project flag will store the run in a separate subfolder allowing to run different series of experiments. As an example, python train.py --project batch_size_bench --name size_5 will use the following dir as rundir:  $TV_DIR_RUNS/KittiSeg/batch_size_bench/size_5_KittiSeg_2017_02_08_13.12.\nThe flag --nosave is very useful to not spam your rundir.\nModifying Model & Train on your own data\nThe model is controlled by the file hypes/KittiSeg.json. Modifying this file should be enough to train the model on your own data and adjust the architecture according to your needs. A description of the expected input format can be found here.\nFor advanced modifications, the code is controlled by 5 different modules, which are specified in hypes/KittiSeg.json.\n\"model\": {\n   \"input_file\": \"../inputs/kitti_seg_input.py\",\n   \"architecture_file\" : \"../encoder/fcn8_vgg.py\",\n   \"objective_file\" : \"../decoder/kitti_multiloss.py\",\n   \"optimizer_file\" : \"../optimizer/generic_optimizer.py\",\n   \"evaluator_file\" : \"../evals/kitti_eval.py\"\n},\n\nThose modules operate independently. This allows easy experiments with different datasets (input_file), encoder networks (architecture_file), etc. Also see TensorVision for a specification of each of those files.\nUtilize TensorVision backend\nKittiSeg is build on top of the TensorVision TensorVision backend. TensorVision modularizes computer vision training and helps organizing experiments.\nTo utilize the entire TensorVision functionality install it using\n$ cd KittiSeg/submodules/TensorVision \n$ python setup.py install\nNow you can use the TensorVision command line tools, which includes:\ntv-train --hypes hypes/KittiSeg.json trains a json model. \ntv-continue --logdir PATH/TO/RUNDIR trains the model in RUNDIR, starting from the last saved checkpoint. Can be used for fine tuning by increasing max_steps in model_files/hypes.json .\ntv-analyze --logdir PATH/TO/RUNDIR evaluates the model in RUNDIR \nUseful Flags & Variabels\nHere are some Flags which will be useful when working with KittiSeg and TensorVision. All flags are available across all scripts.\n--hypes : specify which hype-file to use \n--logdir : specify which logdir to use \n--gpus : specify on which GPUs to run the code \n--name : assign a name to the run \n--project : assign a project to the run \n--nosave : debug run, logdir will be set to debug \nIn addition the following TensorVision environment Variables will be useful:\n$TV_DIR_DATA: specify meta directory for data \n$TV_DIR_RUNS: specify meta directory for output \n$TV_USE_GPUS: specify default GPU behaviour. \nOn a cluster it is useful to set $TV_USE_GPUS=force. This will make the flag --gpus mandatory and ensure, that run will be executed on the right GPU.\nQuestions?\nPlease have a look into the FAQ. Also feel free to open an issue to discuss any questions not covered so far.\nCitation\nIf you benefit from this code, please cite our paper:\n@article{teichmann2016multinet,\n  title={MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving},\n  author={Teichmann, Marvin and Weber, Michael and Zoellner, Marius and Cipolla, Roberto and Urtasun, Raquel},\n  journal={arXiv preprint arXiv:1612.07695},\n  year={2016}\n}\n\n\n'),
('vanpelt', 'https://avatars1.githubusercontent.com/u/17?v=4', 70, 14, 'vandev.com', 44, 'https://api.github.com/users/vanpelt/repos', 2008, 'vanpelt/mali', '\nMali\nA minimalistic gRPC microservice framework.\n\n\nMali is still in development and preview state. It is good for exploration, but may not\nbe suitable for production use yet.\nInstallation\n$ npm install mali\n\nExample\nconst path = require(\'path\')\nconst Mali = require(\'mali\')\n\nconst PROTO_PATH = path.resolve(__dirname, \'../protos/helloworld.proto\')\n\nasync function sayHello (ctx) {\n  ctx.res = { message: \'Hello \'.concat(ctx.req.name) }\n}\n\nfunction main () {\n  const app = new Mali(PROTO_PATH)\n  app.use({ sayHello })\n  app.start(\'0.0.0.0:50051\')\n}\nDocumentation\nFull documentation.\nLicense\nApache-2.0\n\n'),
('defunkt', 'https://avatars0.githubusercontent.com/u/2?v=4', 16838, 208, 'http://chriswanstrath.com/', 107, 'https://api.github.com/users/defunkt/repos', 2007, 'defunkt/cijoe', '\nCI Joe\nJoe is a Continuous\nIntegration\nserver that\'ll run your tests on demand and report their pass/fail status.\nBecause knowing is half the battle.\n\n(Buy the shirt)\nQuickstart\nRubyGems:\n$ gem install cijoe\n$ git clone git://github.com/you/yourrepo.git\n$ cijoe yourrepo\n\nBoom. Navigate to http://localhost:4567 to see Joe in action.\nCheck cijoe -h for other options.\nBasically you need to run cijoe and hand it the path to a git\nrepo. Make sure this isn\'t a shared repo: Joe needs to own it.\nJoe looks for various git config settings in the repo you hand it. For\ninstance, you can tell Joe what command to run by setting\ncijoe.runner:\n$ git config --add cijoe.runner \"rake -s test:units\"\n\nJoe doesn\'t care about Ruby, Python, or whatever. As long as the\nrunner returns a non-zero exit status on fail and a zero on success,\neveryone is happy.\nNeed to do some massaging of your repo before the tests run, like\nmaybe swapping in a new database.yml? No problem - Joe will try to\nrun .git/hooks/after-reset if it exists before each build phase.\nDo it in there. Just make sure it\'s executable.\nWant to notify IRC or email on test pass or failure? Joe will run\n.git/hooks/build-failed or .git/hooks/build-worked if they exist\nand are executable on build pass / fail. They\'re just shell scripts -\nput whatever you want in there.\nTip: your repo\'s HEAD will point to the commit used to run the\nbuild. Pull any metadata you want out of that scro.\n** WARNING ** Do not run this against a git repo that has unpushed\ncommits, as this will do a hard reset against the github remote and\nwipe out unpushed changes.\nOther Branches\nWant joe to run against a branch other than master? No problem:\n$ git config --add cijoe.branch deploy\n\nQueueing\nJoe runs just one build at the time. If you expect concurrent push\'s\nto your repo and want joe to build each in a kind of queue, just set:\n$ git config --add cijoe.buildqueue true\n\nJoe will save requests while another build runs. If more than one push\nhits joe, he just picks the last after finishing the prior.\nCampfire\nCampfire notification is included, because it\'s what we use. Want Joe\nnotify your Campfire? Put this in your repo\'s .git/config:\n[campfire]\n	token = abcd1234\n	subdomain = whatever\n	room = Awesomeness\n	ssl = false\n\nOr do it the old fashion way:\n$ cd yourrepo\n$ git config --add campfire.token abcd1234\n$ git config --add campfire.subdomain github\netc.\n\nCheckin\' Status\nWant to see how your build\'s doing without any of this fancy UI crap?\nPing Joe for the lowdown:\ncurl http://localhost:4567/ping\n\nJoe will return 200 OK if all is quiet on the Western Front. If\nJoe\'s busy building or your last build failed, you\'ll get 412 PRECONDITION FAILED.\nMultiple Projects\nWant CI for multiple projects? Just start multiple instances of Joe!\nHe can run on any port - try cijoe -h for more options.\nIf you\'re using Passenger, see this blog post.\nHTTP Auth\nWorried about people triggering your builds? Setup HTTP auth:\n$ git config --add cijoe.user chris\n$ git config --add cijoe.pass secret\n\nGitHub Integration\nAny POST to Joe will trigger a build. If you are hiding Joe behind\nHTTP auth, that\'s okay - GitHub knows how to authenticate properly.\n\nYou can find the Post-Receive option under the \'Service Hooks\' subtab\nof your project\'s \"Admin\" tab.\nDaemonize\nWant to run Joe as a daemon? Use nohup:\n$ nohup cijoe -p 4444 repo &\n\nOther CI Servers\nNeed more features? More notifiers? Check out one of these bad boys:\n\nJenkins\nCerberus\nIntegrity\nCruiseControl.rb\nBuildBot\nSignal\n\nDoes GitHub use cijoe?\nNo. We use Jenkins.\nScreenshots\n\n\nQuestions? Concerns?\nIssues\n\n');
INSERT INTO `pinfo` (`login`, `avatar`, `followers`, `following`, `blog`, `public_repos`, `repos_url`, `created_at`, `rName`, `readme`) VALUES
('defunkt', 'https://avatars0.githubusercontent.com/u/2?v=4', 16838, 208, 'http://chriswanstrath.com/', 107, 'https://api.github.com/users/defunkt/repos', 2007, 'defunkt/coffee-mode', '\nCoffeeScript Major Mode\n  \nAn Emacs major mode for CoffeeScript and IcedCoffeeScript.\nProvides syntax highlighting, indentation support, imenu support,\na menu bar, and a few cute commands.\n\nRequirement\n\nEmacs 24.3 or higher\nCoffeeScript 1.9.3 or higher\n\nInstallation via package.el\ncoffee-mode is available on MELPA and MELPA-STABLE.\nYou can install coffee-mode with the following command.\nM-x package-install [RET] coffee-mode [RET]\nPlease do not install GNU Emacs Lisp Package Archive version.\nIt\'s too old and many features(Block string, block comment etc) are not implemented.\nWhitespace\ncoffee-mode used to offer automatic deletion of trailing whitespace.\nThis is now left to whitespace-mode. See its documentation for full\ndetails, but as a hint, configure:\n;; automatically clean up bad whitespace\n(setq whitespace-action \'(auto-cleanup))\n;; only show bad whitespace\n(setq whitespace-style \'(trailing space-before-tab indentation empty space-after-tab))\nThen turn on whitespace-mode, or global-whitespace-mode.\nIndentation\nTo set the number of spaces used with each additional indentation, add this to your .emacs or\ninit.el or other initialization file:\n;; This gives you a tab of 2 spaces\n(custom-set-variables \'(coffee-tab-width 2))\ncoffee-tab-width is buffer local variable. You can set indentation size\nper buffer by using File Variables.\n# Local variables:\n# coffee-tab-width: 4\n# End:\nUsing TAB\nSet coffee-indent-tabs-mode t if you want to use TAB instead of spaces.\nMove to corresponding point in JavaScript file after compiling\nYou can archive this with sourcemap and\nfollowing configuration.\nYou can install sourcemap package from MELPA.\n;; generating sourcemap by \'-m\' option. And you must set \'--no-header\' option\n(setq coffee-args-compile \'(\"-c\" \"--no-header\" \"-m\"))\n(add-hook \'coffee-after-compile-hook \'sourcemap-goto-corresponding-point)\n\n;; If you want to remove sourcemap file after jumping corresponding point\n(defun my/coffee-after-compile-hook (props)\n  (sourcemap-goto-corresponding-point props)\n  (delete-file (plist-get props :sourcemap)))\n(add-hook \'coffee-after-compile-hook \'my/coffee-after-compile-hook)\nimenu\nIf you\'re using imenu, coffee-mode should work just fine. This\nmeans users of textmate.el will find that ⇧⌘T\n(textmate-go-to-symbol) mostly works as expected.\nIf you\'re not using imenu check out this page or textmate.el for\na really awesome way to jump quickly to a function\'s definition in a\nfile.\nDefault Key Bindings\n\n\n\nKey\nCommand\n\n\n\n\nC-m, Return\nInsert newline and indent line\n\n\nC-c C-<, backtab\nIndent line or region to left\n\n\nC-c C->\nIndent line or region to right\n\n\nC-M-a\nMove to beginning of defun\n\n\nC-M-e\nMove to end of defun\n\n\nC-M-h\nMark this defun\n\n\nA-r, C-c C-k\nCompile buffer to JavaScript\n\n\nA-R\nCompile content of region to JavaScript\n\n\nA-M-r, C-c C-z\nRun CoffeeScript REPL\n\n\nC-c C-l\nSend this line to REPL buffer\n\n\nC-c C-r\nSend content of region to REPL buffer\n\n\nC-c C-b\nSend content of buffer to REPL buffer\n\n\nC-c C-o C-s\nEnable coffee-cos-mode\n\nC-m and Return key insert newline and indentation. If you don\'t want indentation please overwrite it as below.\n(define-key coffee-mode-map (kbd \"C-m\") \'newline)\nCommands\neasymenu\nIf you have easymenu you can get to any of these commands from the\nmenu bar:\n\ncoffee-repl\nLaunch a CoffeeScript REPL\ncoffee-compile-file\nCompile buffer to JavaScript.\ncoffee-compile-buffer\nCompile region to JavaScript\ncoffee-watch\nRun coffee with the --watch flag on a directory or file.\ncoffee-cos-mode\nMinor mode for compiling to JavaScript at save file.\ncoffee-live-compile-mode\nMinor mode for compiling buffer in real time.\nCustomization\nIndent like python-mode\nWhen coffee-indent-like-python-mode is non-nil, indent command works like python-mode.\nI suppose that Evil\'s o and O commands\nworks as you expect with this option.\n(custom-set-variables\n \'(coffee-indent-like-python-mode t))\nSample Configuration\n;; coffeescript\n(custom-set-variables\n \'(coffee-tab-width 2)\n \'(coffee-args-compile \'(\"-c\" \"--no-header\" \"--bare\")))\n\n(eval-after-load \"coffee-mode\"\n  \'(progn\n     (define-key coffee-mode-map [(meta r)] \'coffee-compile-buffer)\n     (define-key coffee-mode-map (kbd \"C-j\") \'coffee-newline-and-indent)))\nBugs\nPlease file bugs at https://github.com/defunkt/coffee-mode/issues\n\n'),
('defunkt', 'https://avatars0.githubusercontent.com/u/2?v=4', 16838, 208, 'http://chriswanstrath.com/', 107, 'https://api.github.com/users/defunkt/repos', 2007, 'defunkt/d3', '\nData-Driven Documents\n\nD3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG and CSS. D3’s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation.\nWant to learn more? See the wiki.\nFor examples, see the gallery and mbostock’s bl.ocks.\n\n'),
('defunkt', 'https://avatars0.githubusercontent.com/u/2?v=4', 16838, 208, 'http://chriswanstrath.com/', 107, 'https://api.github.com/users/defunkt/repos', 2007, 'defunkt/dotenv', '\ndotenv\nLoads environment variables from .env into ENV, automagically.\nRead more about the motivation for dotenv at opensoul.org.\nInstallation\nRails\nAdd this line to your application\'s Gemfile:\ngem \'dotenv\', :groups => [:development, :test]\n\nAnd then execute:\n$ bundle\n\nSinatra or Plain ol\' Ruby\nInstall the gem:\n$ gem install dotenv\n\nAs early as possible in your application bootstrap process, load .env:\nDotenv.load\n\nTo ensure .env is loaded in rake, load the tasks:\nrequire \'dotenv/tasks\'\n\ntask :mytask => :dotenv do\n  # things that require .env\nend\n\nUsage\nAdd your application configuration to .env.\nS3_BUCKET=dotenv\nSECRET_KEY=sssshhh!\n\nWhenever your application loads, these variables will be available in ENV:\nconfig.fog_directory  = ENV[\'S3_BUCKET\']\n\nContributing\n\nFork it\nCreate your feature branch (git checkout -b my-new-feature)\nCommit your changes (git commit -am \'Added some feature\')\nPush to the branch (git push origin my-new-feature)\nCreate new Pull Request\n\n\n'),
('defunkt', 'https://avatars0.githubusercontent.com/u/2?v=4', 16838, 208, 'http://chriswanstrath.com/', 107, 'https://api.github.com/users/defunkt/repos', 2007, 'defunkt/email_reply_parser', '\nEmail Reply Parser\nEmailReplyParser is a small library to parse plain text email content.\nSee the rocco-documented source code for specifics on how it works.\nThis is what GitHub uses to display comments that were created from\nemail replies.  This code is being open sourced in an effort to\ncrowdsource the quality of our email representation.\nSee more at the Rocco docs.\nProblem?\nIf you have a question about the behavior and formatting of email replies on GitHub, check out support.  If you have a specific issue regarding this library, then hit up the Issues.\nInstallation\nGet it from GitHub or gem install email_reply_parser.  Run rake to run the tests.\nContribute\nIf you\'d like to hack on EmailReplyParser, start by forking the repo on GitHub:\nhttps://github.com/github/email_reply_parser\nThe best way to get your changes merged back into core is as follows:\n\nClone down your fork\nCreate a thoughtfully named topic branch to contain your change\nHack away\nAdd tests and make sure everything still passes by running rake\nIf you are adding new functionality, document it in the README\nDo not change the version number, I will do that on my end\nIf necessary, rebase your commits into logical chunks, without errors\nPush the branch up to GitHub\nSend a pull request to the github/email_reply_parser project.\n\nKnown Issues\nQuoted Headers\nQuoted headers aren\'t picked up if there\'s an extra line break:\nOn <date>, <author> wrote:\n\n> blah\n\nAlso, they\'re not picked up if the email client breaks it up into\nmultiple lines.  GMail breaks up any lines over 80 characters for you.\nOn <date>, <author>\nwrote:\n> blah\n\nNot to mention that we\'re search for \"on\" and \"wrote\".  It won\'t work\nwith other languages.\nPossible solution: Remove \"reply@reply.github.com\" lines...\nWeird Signatures\nLines starting with - or _ sometimes mark the beginning of\nsignatures:\nHello\n\n-- \nRick\n\nNot everyone follows this convention:\nHello\n\nMr Rick Olson\nGalactic President Superstar Mc Awesomeville\nGitHub\n\n**********************DISCLAIMER***********************************\n* Note: blah blah blah                                            *\n**********************DISCLAIMER***********************************\n\nStrange Quoting\nApparently, prefixing lines with > isn\'t universal either:\nHello\n\n--\nRick\n\n________________________________________\nFrom: Bob [reply@reply.github.com]\nSent: Monday, March 14, 2011 6:16 PM\nTo: Rick\n\n\n'),
('defunkt', 'https://avatars0.githubusercontent.com/u/2?v=4', 16838, 208, 'http://chriswanstrath.com/', 107, 'https://api.github.com/users/defunkt/repos', 2007, 'defunkt/facebox', '\nFacebox\nFacebox is a jQuery-based, Facebook-style lightbox which can display images, divs, or entire remote pages.\nSee it in action.\n\nDownload the latest release\nCompatibility\nThis release relies on a lot of advanced CSS techniques (box-shadow, border-radius, RGBA). That being said, it\'s compatible with many browsers.\n\nSafari 4\nWebkit Nightlies (Chromium, Chrome) as of 4/17/10\nFirefox 3.5\nIE8 (degraded experience)\nIE7 (degraded experience)\nIE6 - I just don\'t care\nOpera - I just don\'t care\n\nUsage\nInclude jQuery, src/facebox.js and src/facebox.css. Then tell facebox where you\'ve put src/loading.gif and src/closelabel.png\n$.facebox.settings.closeImage = \'/images/facebox/closelabel.png\'\n$.facebox.settings.loadingImage = \'/images/facebox/loading.gif\'\n\nCalling facebox() on any anchor tag will do the trick, it\'s easier to give your Faceboxy links a rel=\"facebox\"  and hit them all onDomReady.\njQuery(document).ready(function($) {\n  $(\'a[rel*=facebox]\').facebox()\n})\n\nAny anchor links with rel=\"facebox\" with now automatically use facebox:\n<a href=\"#terms\" rel=\"facebox\">Terms</a>\n  Loads the #terms div in the box\n\n<a href=\"terms.html\" rel=\"facebox\">Terms</a>\n  Loads the terms.html page in the box\n\n<a href=\"terms.png\" rel=\"facebox\">Terms</a>\n  Loads the terms.png image in the box\n\nUsing facebox programmatically\njQuery.facebox(\'some html\')\njQuery.facebox(\'some html\', \'my-groovy-style\')\n\nThe above will open a facebox with \"some html\" as the content.\njQuery.facebox(function($) {\n  $.get(\'blah.html\', function(data) { $.facebox(data) })\n})\n\nThe above will show a loading screen before the passed function is called,\nallowing for a better ajaxy experience.\nThe facebox function can also display an ajax page, an image, or the contents of a div:\njQuery.facebox({ ajax: \'remote.html\' })\njQuery.facebox({ ajax: \'remote.html\' }, \'my-groovy-style\')\njQuery.facebox({ image: \'stairs.jpg\' })\njQuery.facebox({ image: \'stairs.jpg\' }, \'my-groovy-style\')\njQuery.facebox({ div: \'#box\' })\njQuery.facebox({ div: \'#box\' }, \'my-groovy-style\')\n\nEvents\nWant to close the facebox?  Trigger the close.facebox document event:\njQuery(document).trigger(\'close.facebox\')\n\nFacebox also has a bunch of other hooks:\n\nloading.facebox\nbeforeReveal.facebox\nreveal.facebox (aliased as afterReveal.facebox)\ninit.facebox\nafterClose.facebox  (callback after closing facebox)\n\nSimply bind a function to any of these hooks:\n$(document).bind(\'reveal.facebox\', function() { ...stuff to do after the facebox and contents are revealed... })\n\nCustomization\nYou can give the facebox container an extra class (to fine-tune the display of the facebox) with the facebox[.class] rel syntax.\n<a href=\"remote.html\" rel=\"facebox[.bolder]\">text</a>\n\nContact & Help\nIf you have questions, feel free to ask on the Google Groups Mailing List. Alternatively if you find a bug, you can open an issue.\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/activesupport_notifications_backport', '\nActiveSupport::Notifications Backport\nThis is a backport of ActiveSupport::Notifications for Rails 2.3.  That\'s all!\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/allofthestars', '\nAllOfTheStars\nI don\'t even know what this is going to be, yet.  Right now, it\'s just a\nRiak Search testbed.  Probably not useful for anyone.\n\nSetup\nSetup the rubies:\ngem install bundler\nbundle install\n\nAnd the Riaks:\nbrew install riak-search # or equivalent\nriaksearch start\n\n# setup the search schema\nsearch-cmd set-schema stars db/stars.erl\nsearch-cmd set-schema clusters db/clusters.erl\n\n# setup the automatic riak kv => riak search hooks\nsearch-cmd install stars\nsearch-cmd install clusters\n\nUSAGE\nAll JSON API for now.  The actual serialization and fields will likely\nchange.\n\ncreate a cluster\n\nPOST /clusters\n\n\ncreate a star\n\nPOST /clusters/:id/stars\n\n\nget a cluster\n\nGET /clusters/:id\n\n\nget a star\n\nGET /stars/:id\n\n\nget stars in a cluster\n\nGET /clusters/:id/stars\n?q=blah - search content\n?t=Campfire - filter by type\n?custom[foo] - filter by custom field\n?start - the starting result of the query (pagination)\n\n\n\nTests\nNo tests.  shrug lol\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/assert', '\nAssert (c) Blake Mizerany and Keith Rarick -- MIT LICENCE\nAssertions for Go tests\nInstall\n$ go get github.com/bmizerany/assert\n\nUse\npoint.go\npackage point\n\ntype Point struct {\n    x, y int\n}\n\npoint_test.go\npackage point\n\nimport (\n    \"testing\"\n    \"github.com/bmizerany/assert\"\n)\n\nfunc TestAsserts(t *testing.T) {\n    p1 := Point{1, 1}\n    p2 := Point{2, 1}\n\n    assert.Equal(t, p1, p2)\n}\n\noutput\n$ go test\n--- FAIL: TestAsserts (0.00 seconds)\nassert.go:15: /Users/flavio.barbosa/dev/stewie/src/point_test.go:12\nassert.go:24: ! X: 1 != 2\nFAIL\nDocs\nhttp://github.com/bmizerany/assert\n\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/beautiful-docs', '\nBeautiful Docs\nI love documentation. If you work with/are writing code intended for usage and consumption by more than one person, you should love it, too. Documentation and other resources will make or break the success of your project. And the more open and collaborative you want development to be, the more crucial docs become.\nWith that in mind, here\'s a list of docs and other developer resources that myself and others find particularly useful, well-written, and otherwise \"beautiful\". May they serve to inspire you when writing and designing yours.\nThis should be updated fairly regularly. As usual, pull requests are encouraged.\nMark\nAnd Now For The Docs (in no particular)\n\nRedis Commands - Most of the Redis docs are exceptional, but this section really epitomizes the combination of good design and usability. And all the individual command pages give you the ability to test things out without leaving the page. Pretty close to perfect.\nBrightBox Developer Docs - Clean design, easy to navigate, very in-depth.\nRiaknostic - Made possible by Bootstrap, this is a great example of a project homepage that doubles as a documentation teaser.\nGitHub Developer Docs - I don\'t always get excited about accordion-based navigation, but when I do...\nDropwizard - Beautiful and concise; another Bootstrap joint.\nRiak Pipe README - The simplicity of READMEs means that you can\'t obscure shitty content with flashy design. This one is written by my Basho colleague Bryan Fink and should be committed to memory as an excellent example of how to write READMEs.\nLearn You Some Erlang (contributed by @lenary)\nDjango Documentation (contributed by @bretthoerner)\nErldocs.com (contributed by @bradfordw)\nClojuredocs.org (contributed by @mrb_bk)\nFreeBSD Handbook - Clean and direct documentation of an OS. (contributed by @mrtazz)\nRuby on Rails Guides (contributed by @seancribbs)\nPow Annotated Source - Built with docco. (contributed by @BonzoESC)\n\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/blog', '\nI am a researcher and computer scientist. I was once in San Francisco, but am now traveling.\nNote: Things may be a bit of a mess here while I de-jekyll all the previous posts. This may all go away if I find that jekyll actually did something valuable for me, but at this point I wouldn\'t worry about that.\nPosts\n\nSorting out graph processing\nWe revisit the conventional wisdom that sorting is expensive, and random access is fast. In particular, if you think you might need to do a bunch of random accesses, maybe you should consider sorting the requests first. We look at some results in a paper from SOSP 2013 and see how speedy sorting algorithms likely change the trade-offs the paper proposes.\nAlso, differential dataflow goes a lot faster as a result of this stuff, so you should read about it.\n\nThe impact of fast networks on graph analytics, part 2.\nMalte and I did a bit deeper into the sources of the performance discrepancies between GraphX and Timely dataflow. We measure many things, and work through some neat time series that look like\n\n\nThe impact of fast networks on graph analytics, part 1.\nMalte Schwarzkopf and I look in to the question of to what degree does improving networking help in graph computation. We do some measurement, comparing a PageRank implementation in both GraphX and in Timely dataflow.\n\nDifferential graph computation\nPrompted by reader questions, we take a light tour through some of the things that differential dataflow can do.\n\n\nAbomonation: terrifying serialization\n\nData-parallelism in timely dataflow\n\nWorst-case optimal joins, in dataflow\n\nDifferential dataflow\n\nBigger data; same laptop\n\nScalability! But at what COST?\n\nTimely dataflow: core concepts\n\nTimely dataflow: reboot\n\nColumnarization in Rust, part 2\n\nColumnarization in Rust\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/brew', '\nHomebrew\nFeatures, usage and installation instructions are summarised on the homepage.\nUpdate Bug\nIf Homebrew was updated on Aug 10-11th 2016 and brew update always says Already up-to-date. you need to run:\ncd $(brew --repo); git fetch; git reset --hard origin/master; brew update\nWhat Packages Are Available?\n\nType brew search for a list.\nOr visit braumeister.org to browse packages online.\nOr use brew search --desc <keyword> to browse packages from the command line.\n\nMore Documentation\nbrew help, man brew or check our documentation.\nTroubleshooting\nFirst, please run brew update and brew doctor.\nSecond, read the Troubleshooting Checklist.\nIf you don\'t read these it will take us far longer to help you with your problem.\nContributing\nWe\'d love you to contribute to Homebrew. First, please read our Contribution Guide and Code of Conduct.\nWe explicitly welcome contributions from people who have never contributed to open-source before: we were all beginners once! We can help build on a partially working pull request with the aim of getting it merged. We are also actively seeking to diversify our contributors and especially welcome contributions from women from all backgrounds and people of colour.\nA good starting point for contributing is running brew audit (or brew audit --strict) with some of the packages you use (e.g. brew audit wget if you use wget) and then read through the warnings, try to fix them until brew audit shows no results and submit a pull request. If no formulae you use have warnings you can run brew audit without arguments to have it run on all packages and pick one. Good luck!\nSecurity\nPlease report security issues to security@brew.sh.\nThis is our PGP key which is valid until May 24, 2017.\n\nKey ID: 0xE33A3D3CCE59E297\nFingerprint: C657 8F76 2E23 441E C879 EC5C E33A 3D3C CE59 E297\nFull key: https://keybase.io/homebrew/key.asc\n\nWho Are You?\nHomebrew\'s current maintainers are Misty De Meo, Andrew Janke, Xu Cheng, Tomasz Pajor, Mike McQuaid, Baptiste Fontaine, Brett Koonce, ilovezfs, Martin Afanasjew, Dominyk Tiller, Tim Smith and Alex Dunn.\nFormer maintainers with significant contributions include Jack Nagel, Adam Vandenberg and Homebrew\'s creator: Max Howell.\nLicense\nCode is under the BSD 2 Clause (NetBSD) license.\nDocumentation is under the Creative Commons Attribution license.\nDonations\nHomebrew is a non-profit project run entirely by unpaid volunteers. We need your funds to pay for software, hardware and hosting around continuous integration and future improvements to the project. Every donation will be spent on making Homebrew better for our users.\nHomebrew is a member of the Software Freedom Conservancy which provides us with an ability to receive tax-deductible, Homebrew earmarked donations (and many other services). Software Freedom Conservancy, Inc. is a 501(c)(3) organization incorporated in New York, and donations made to it are fully tax-deductible to the extent permitted by law.\n\nDonate with PayPal\nDonate by USA $ check from a USA bank:\n\nMake check payable to \"Software Freedom Conservancy, Inc.\" and place \"Directed donation: Homebrew\" in the memo field.  Checks should then be mailed to:\n\nSoftware Freedom Conservancy, Inc.\n137 Montague ST  STE 380\nBROOKLYN, NY 11201             USA\n\n\n\n\nDonate by wire transfer: contact accounting@sfconservancy.org for wire transfer details.\nDonate with Flattr or PayPal Giving Fund: coming soon.\n\nSponsors\nOur CI infrastructure was paid for by our Kickstarter supporters.\nOur CI infrastructure is hosted by The Positive Internet Company.\nOur bottles (binary packages) are hosted by Bintray.\n\nSecure password storage and syncing provided by 1Password for Teams by AgileBits\n\nHomebrew is a member of the Software Freedom Conservancy\n\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/call-for-proposals', '\nEuRuKo 2012 Call For Proposals\nSo, you are interested in giving a talk at the most awesome Ruby\nconference in Europe?\nAnd what better way to submit a talk proposal than via GitHub!\nHere is what do to\n\nFork this repository\nCopy the sample folder example/ to your_name-talk_name\nEdit the README.md file and type a good description about your talk\nand about yourself\nReplace the profile_picture.jpg with a picture of yourself that we\ncan use to list you on the speakers page when you are accepted.\nCreate a pull request of your fork\n\nYou can add any type of assets to support your proposal inside your\nfolder.\nWhen a talk gets accepted we will press the lovely green merge button.\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/camo.go', '\nCamo.go\nPort of atmos/camo in Go, using Falcore as the HTTP server.\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/celluloid', '\nCelluloid\n\n\n\"I thought of objects being like biological cells and/or individual\ncomputers on a network, only able to communicate with messages\"\n--Alan Kay, creator of Smalltalk, on the meaning of \"object oriented programming\"\n\nCelluloid provides a simple and natural way to build fault-tolerant concurrent\nprograms in Ruby. With Celluloid, you can build systems out of concurrent\nobjects just as easily as you build sequential programs out of regular objects.\nRecommended for any developer, including novices, Celluloid should help ease\nyour worries about building multithreaded Ruby programs.\nUnder the hood, Celluloid wraps regular objects in threads that talk to each\nother using messages. These concurrent objects are called \"actors\". When a\ncaller wants another actor to execute a method, it literally sends it a\nmessage object telling it what method to execute. The receiver listens on its\nmailbox, gets the request, runs the method, and sends the caller the result.\nThe receiver processes messages in its inbox one-at-a-time, which means that\nyou don\'t need to worry about synchronizing access to an object\'s instance\nvariables.\nIn addition to that, Celluloid also gives you the ability to call methods\nasynchronously, so the receiver to do things in the background for you\nwithout the caller having to sit around waiting for the result.\nLike Celluloid? Join the Google Group\nSupported Platforms\nCelluloid works on Ruby 1.9.2, JRuby 1.6 (in 1.9 mode), and Rubinius 2.0. JRuby\nor Rubinius are the preferred platforms as they support true concurrent threads.\nTo use JRuby in 1.9 mode, you\'ll need to pass the \"--1.9\" command line option\nto the JRuby executable, or set the \"JRUBY_OPTS=--1.9\" environment variable.\nCelluloid works on Rubinius in either 1.8 or 1.9 mode.\nBasic Usage\nTo use Celluloid, define a normal Ruby class that includes Celluloid:\nclass Sheen\n  include Celluloid\n\n  def initialize(name)\n    @name = name\n  end\n\n  def set_status(status)\n    @status = status\n  end\n\n  def report\n    \"#{@name} is #{@status}\"\n  end\nend\n\nNow when you create new instances of this class, they\'re actually concurrent\nobjects, each running in their own thread:\n>> charlie = Sheen.new \"Charlie Sheen\"\n => #<Celluloid::Actor(Sheen:0x00000100a312d0) @name=\"Charlie Sheen\">\n>> charlie.set_status \"winning!\"\n => \"winning!\"\n>> charlie.report\n => \"Charlie Sheen is winning!\"\n>> charlie.set_status! \"asynchronously winning!\"\n => nil\n>> charlie.report\n => \"Charlie Sheen is asynchronously winning!\"\n\nYou can call methods on this concurrent object just like you would any other\nRuby object. The Sheen#set_status method works exactly like you\'d expect,\nreturning the last expression evaluated.\nHowever, Celluloid\'s secret sauce kicks in when you call banged predicate\nmethods (i.e. methods ending in !). Even though the Sheen class has no\nset_status! method, you can still call it. Why is this? Because bang methods\nhave a special meaning in Celluloid. (Note: this also means you can\'t define\nbang methods on Celluloid classes and expect them to be callable from other\nobjects)\nAdding a bang to the end of a method instructs Celluloid that you would like\nfor the given method to be called asynchronously. This means that rather\nthan the caller waiting for a response, the caller sends a message to the\nconcurrent object that you\'d like the given method invoked, and then the\ncaller proceeds without waiting for a response. The concurrent object\nreceiving the message will then process the method call in the background.\nAdding a bang to a method name is a convention in Ruby used to indicate that\nthe method is in some way \"dangerous\", and in Celluloid this is no exception.\nYou have no guarantees that just because you made an asynchronous call it was\never actually invoked. Asynchronous calls will never raise an exception, even\nif an exception occurs when the receiver is processing it. Worse, unhandled\nexceptions will crash the receiver, and making an asynchronous call to a\ncrashed object will not raise an error.\nHowever, you can still handle errors created by asynchronous calls using\ntwo features of Celluloid called supervisors and linking. See the\ncorresponding sections below for more information.\nFutures\nFutures allow you to request a computation and get the result later. There are\ntwo types of futures supported by Celluloid: method futures and block futures.\nMethod futures work by invoking the future method on an actor. This method\nis analogous to the typical send method in that it takes a method name,\nfollowed by an arbitrary number of arguments, and a block. Let\'s invoke the\nreport method from the charlie object used in the above example using a future:\n>> future = charlie.future :report\n => #<Celluloid::Future:0x000001009759b8>\n>> future.value\n => \"Charlie Sheen is winning!\"\n\nThe call to charlie.future immediately returns a Celluloid::Future object,\nregardless of how long it takes to execute the \"report\" method. To obtain\nthe result of the call to \"report\", we call the value method of the\nfuture object. This call will block until the value returned from the method\ncall is available (i.e. the method has finished executing). If an exception\noccured during the method call, the call to future.value will reraise the\nsame exception.\nFutures also allow you to background the computation of any block:\n>> future = Celluloid::Future.new { 2 + 2 }\n => #<Celluloid::Future:0x000001008425f0>\n>> future.value\n => 4\n\nOne thing to be aware of when using futures: always make sure to obtain the\nvalue of any future you make. Futures create a thread in the background which\nwill continue to run until the future\'s value is obtained. Failing to obtain\nthe value of futures you create will leak threads.\nSupervisors\nYou may be familiar with tools like Monit or God which keep an eye on your\napplications and restart them when they crash. Celluloid supervisors work in\na similar fashion, except instead of monitoring applications, they monitor\nindividual actors and restart them when they crash. Crashes occur whenever\nan unhandled exception is raised anywhere within an actor.\nTo supervise an actor, start it with the supervise method. Using the Sheen\nclass from the example above:\n>> supervisor = Sheen.supervise \"Charlie Sheen\"\n => #<Celluloid::Supervisor(Sheen) \"Charlie Sheen\">\n\nThis created a new Celluloid::Supervisor actor, and also created a new Sheen\nactor, giving its initialize method the argument \"Charlie Sheen\". The\nsupervise method has the same method signature as new. However, rather\nthan returning the newly created actor, supervise returns the supervisor.\nTo retrieve the actor that the supervisor is currently using, use the\nCelluloid::Supervisor#actor method:\n>> supervisor = Sheen.supervise \"Charlie Sheen\"\n => #<Celluloid::Supervisor(Sheen) \"Charlie Sheen\">\n>> charlie = supervisor.actor\n => #<Celluloid::Actor(Sheen:0x00000100a312d0)>\n\nSupervisors can also automatically put actors into the actor registry using\nthe supervise_as method:\n>> Sheen.supervise_as :charlie, \"Charlie Sheen\"\n => #<Celluloid::Supervisor(Sheen) \"Charlie Sheen\">\n>> charlie = Celluloid::Actor[:charlie]\n => #<Celluloid::Actor(Sheen:0x00000100a312d0)>\n\nIn this case, the supervisor will ensure that an actor of the Sheen class,\ncreated using the given arguments, is aways available by calling\nCelluloid::Actor[:charlie]. The first argument to supervise_as is the name\nyou\'d like the newly created actor to be registered under. The remaining\narguments are passed to initialize just like you called new.\nSee the \"Registry\" section below for more information on the actor registry\nLinking\nWhenever any unhandled exceptions occur in any of the methods of an actor,\nthat actor crashes and dies. Let\'s start with an example:\nclass JamesDean\n  include Celluloid\n  class CarInMyLaneError < StandardError; end\n\n  def drive_little_bastard\n    raise CarInMyLaneError, \"that guy\'s gotta stop. he\'ll see us\"\n  end\nend\n\nNow, let\'s have James drive Little Bastard and see what happens:\n>> james = JamesDean.new\n => #<Celluloid::Actor(JamesDean:0x1068)>\n>> james.drive_little_bastard!\n => nil\n>> james\n => #<Celluloid::Actor(JamesDean:0x1068) dead>\n\nWhen we told james asynchronously to drive Little Bastard, it killed him! If\nwe were Elizabeth Taylor, co-star in James\' latest film at the time of his\ndeath, we\'d certainly want to know when he died. So how can we do that?\nActors can link to other actors they\'re interested in and want to receive\ncrash notifications from. In order to receive these events, we need to use the\ntrap_exit method to be notified of them. Let\'s look at how a hypothetical\nElizabeth Taylor object could be notified that James Dean has crashed:\nclass ElizabethTaylor\n  include Celluloid\n  trap_exit :actor_died\n\n  def actor_died(actor, reason)\n    puts \"Oh no! #{actor.inspect} has died because of a #{reason.class}\"\n  end\nend\n\nWe\'ve now used the trap_exit method to configure a callback which is invoked\nwhenever any linked actors crashed. Now we need to link Elizabeth to James so\nJames\' crash notifications get sent to her:\n>> james = JamesDean.new\n => #<Celluloid::Actor(JamesDean:0x11b8)>\n>> elizabeth = ElizabethTaylor.new\n => #<Celluloid::Actor(ElizabethTaylor:0x11f0)>\n>> elizabeth.link james\n => #<Celluloid::Actor(JamesDean:0x11b8)>\n>> james.drive_little_bastard!\n => nil\nOh no! #<Celluloid::Actor(JamesDean:0x11b8) dead> has died because of a JamesDean::CarInMyLaneError\n\nElizabeth called the link method to receive crash events from James. Because\nElizabeth was linked to James, when James crashed, James\' exit message was\nsent to her. Because Elizabeth was trapping the exit messages she received\nusing the trap_exit method, the callback she specified was invoked, allowing\nher to take action (in this case, printing the error). But what would happen\nif she weren\'t trapping exits? Let\'s break James apart into two separate\nobjects, one for James himself and one for Little Bastard, his car:\nclass PorscheSpider\n  include Celluloid\n  class CarInMyLaneError < StandardError; end\n\n  def drive_on_route_466\n    raise CarInMyLaneError, \"head on collision :(\"\n  end\nend\n\nclass JamesDean\n  include Celluloid\n\n  def initialize\n    @little_bastard = PorscheSpider.new_link\n  end\n\n  def drive_little_bastard\n    @little_bastard.drive_on_route_466\n  end\nend\n\nIf you take a look in JamesDean#initialize, you\'ll notice that to create an\ninstance of PorcheSpider, James is calling the new_link method.\nThis method works similarly to new, except it combines new and link\ninto a single call.\nNow what happens if we repeat the same scenario with Elizabeth Taylor watching\nfor James Dean\'s crash?\n>> james = JamesDean.new\n => #<Celluloid::Actor(JamesDean:0x1108) @little_bastard=#<Celluloid::Actor(PorscheSpider:0x10ec)>>\n>> elizabeth = ElizabethTaylor.new\n => #<Celluloid::Actor(ElizabethTaylor:0x1144)>\n>> elizabeth.link james\n => #<Celluloid::Actor(JamesDean:0x1108) @little_bastard=#<Celluloid::Actor(PorscheSpider:0x10ec)>>\n>> james.drive_little_bastard!\n => nil\nOh no! #<Celluloid::Actor(JamesDean:0x1108) dead> has died because of a PorscheSpider::CarInMyLaneError\n\nWhen Little Bastard crashed, it killed James as well. Little Bastard killed\nJames, and because Elizabeth was trapping James\' exit events, she received the\nnotification of James\' death.\nActors that are linked together propagate their error messages to all other\nactors that they\'re linked to. Unless those actors are trapping exit events,\nthose actors too will die, like James did in this case. If you have many,\nmany actors linked together in a large object graph, killing one will kill them\nall unless they are trapping exits.\nThis allows you to factor your problem into several actors. If an error occurs\nin any of them, it will kill off all actors used in a particular system. In\ngeneral, you\'ll probably want to have a supervisor start a single actor which\nis in charge of a particular part of your system, and have that actor\nnew_link to other actors which are part of the same system. If any error\noccurs in any of these actors, all of them will be killed off and the entire\nsubsystem will be restarted by the supervisor in a clean state.\nIf, for any reason, you\'ve linked to an actor and want to sever the link,\nthere\'s a corresponding unlink method to remove links between actors.\nRegistry\nCelluloid lets you register actors so you can refer to them symbolically.\nYou can register Actors using Celluloid::Actor[]:\n>> james = JamesDean.new\n => #<Celluloid::Actor(JamesDean:0x80c27ce0)>\n>> Celluloid::Actor[:james] = james\n => #<Celluloid::Actor(JamesDean:0x80c27ce0)>\n>> Celluloid::Actor[:james]\n => #<Celluloid::Actor(JamesDean:0x80c27ce0)>\n\nThe Celluloid::Actor constant acts as a hash, allowing you to register actors\nunder the name of your choosing, and access actors by name rather than\nreference. This is important because actors may crash. If you\'re attempting to\nreference an actor explicitly by storing it in a variable, you may be holding\nonto a reference to a crashed copy of that actor, rather than talking to a\nworking, freshly-restarted version.\nThe main use of the registry is for interfacing with actors that are\nautomatically restarted by supervisors when they crash.\nApplications\nCelluloid provides a DSL for describing all of the actors in a given\napplication. This lets you start a group of actors in one swoop and\nalso provides an additional level of supervision: applications supervise\nthe supervisors of all the actors in your system, an approach known\nas supervision trees.\nDefine Celluloid::Applications with the following syntax:\nclass MyApplication < Celluloid::Application\n  supervise MyActor, :as => :my_actor\n  supervise AnotherActor, :as => :another_actor\nend\n\nThis will start the MyActor and AnotherActor actors under a supervisor and\nautomatically register them as Celluloid::Actor[:my_actor] and\nCelluloid::Actor[:another_actor].\nTo launch your application, do:\nMyApplication.run\n\nThis launches your application in the foreground. To launch in in the\nbackground, do:\nMyApplication.run!\n\nSignaling\nSignaling is an advanced technique similar to condition variables in typical\nmultithreaded programming. One method within a concurrent object can suspend\nitself waiting for a particular event, allowing other methods to run. Another\nmethod can then signal all methods waiting for a particular event, and even\nsend them a value in the process:\nclass SignalingExample\n  include Celluloid\n  attr_reader :signaled\n\n  def initialize\n    @signaled = false\n  end\n\n  def wait_for_signal\n    value = wait :ponycopter\n    @signaled = true\n    value\n  end\n\n  def send_signal(value)\n    signal :ponycopter, value\n  end\nend\n\nThe #wait_for_signal method in turn calls a method called \"wait\". Wait suspends\nthe running method until another method of the same object calls the \"signal\"\nmethod with the same label.\nThe #send_signal method of this class does just that, signaling \"ponycopter\"\nwith the given value. This value is returned from the original wait call.\nProtocol Interaction\nThe asynchronous message protocol Celluloid uses can be used directly to add\nnew behavior to actors.\nTo send a raw asynchronous message to an actor, use:\nactor.mailbox << MyMessage.new\n\nMethods can wait on incoming MyMessage objects using the #receive method:\nclass MyActor\n  def initialize\n    wait_for_my_messages!\n  end\n\n  def wait_for_my_messages\n    loop do\n      message = receive { |msg| msg.is_a? MyMessage }\n	  puts \"Got a MyMessage: #{message.inspect}\"\n    end\n  end\nend\n\nThe #receive method takes a block, and yields any incoming messages which are\nreceived by the current actor to the block, waiting for the block to return\ntrue. Calls to #receive sleep until a message is received which makes the\nblock return true, at which point the matching message is returned.\nHandling I/O with Celluloid::IO\nCelluloid provides a separate class of actors which run alongside I/O\noperations. These actors are slower and more heavyweight and should only be\nused when writing actors that also handle IO operations. Every IO actor will\nuse 2 file descriptors (it uses a pipe for signaling), so use them sparingly\nand only when directly interacting with IO.\nTo create an IO actor, include Celluloid::IO:\nclass IOActor\n  include Celluloid::IO\n\n  def initialize(sock)\n    @sock = sock\n  end\n\n  def read\n    wait_readable(@sock) do\n      @sock.read_nonblock\n    end\n  end\nend\n\nThe Celluloid::IO#wait_readable and #wait_writeable methods suspend execution\nof the current method until the given IO object is ready to be read from or\nwritten to respectively. In the meantime, the current actor will continue\nprocessing incoming messages, allowing it to respond to method requests even\nwhile a method (or many methods) are waiting on IO objects.\nLogging\nBy default, Celluloid will log any errors and backtraces from any crashing\nactors to STDOUT. However, if you wish you can use any logger which is\nduck typed with the standard Ruby Logger API (i.e. it implements the #error\nmethod). For example, if you\'re using Celluloid within a Rails\napplication, you\'ll probably want to do:\nCelluloid.logger = Rails.logger\n\nThe logger class you specify must be thread-safe, although with a logging\nAPI about the worst you have to worry about with thread safety bugs is\nout-of-order messages in the log.\nImplementation and Gotchas\nCelluloid is fundamentally a messaging system which uses thread-safe proxies\nto manage all inter-object communication in the system. While the goal of\nthese proxies is to make it simple for you to write concurrent programs by\napplying the uniform access principle to thread-safe inter-object messaging,\nyou can\'t simply forget they\'re there.\nThe thread-safety guarantees Celluloid provides around synchronizing access to\ninstance variables only work so long as all access to actors go through the\nproxy objects. If the real objects that Celluloid is wrapping in an actor\nmanage to leak out of the system, all hell will break loose.\nHere are a few rules you can follow to keep this from happening:\n\n\nNEVER RETURN SELF (or pass self as an argument to other actors): in\ncases where you want to pass an actor around to other actors or threads,\nuse Celluloid.current_actor, or if you\'re within an actor itself, you can\njust call the #current_actor method. If you really need to get ahold of\n\"self\" in order to add instance-specific behavior, e.g for metaprogramming\npurposes or adding stubs during tests, call MyActor#wrapped_object to\nobtain the actual object an actor is wrapping.\n\n\nDon\'t mutate the state of objects you\'ve sent in calls to other actors:\nThis means you must think about data in one of two different ways: either\nyou \"fire and forget\" the data, leaving it for other actors to do with\nwhat they will, or you must treat it as immutable if you have any plans\nof sharing it with other actors. If you\'re paranoid (and when you\'re\ndealing with concurrency, there\'s nothing wrong with being paranoid),\nyou can freeze objects so you can detect subsequent mutations (or rather,\nturn attempts at mutation into errors).\n\n\nDon\'t mix Ruby thread primitives and calls to other actors: if you make\na call to another actor with a mutex held, you\'re doing it wrong. It\'s\nperfectly fine and strongly encouraged to call out to thread safe\nlibraries from Celluloid actors. However, if you\'re using libraries that\nacquire mutexes and then execute callbacks (e.g. they take a block while\nthey\'re holding a mutex) the guarantees that Celluloid provides will\nbecome weak and you may encounter deadlocks.\n\n\nUse Fibers at your own risk: Celluloid employs Fibers as an intrinsic part\nof how it implements actors. While it\'s possible for certain uses of Fibers\nto cooperatively work alongside how Celluloid behaves, in most cases you\'ll\nbe writing a check you can\'t afford. So please ask yourself: why are you\nusing Fibers, and why can\'t it be solved by a block? If you\'ve got a really\ngood reason and you\'re feeling lucky, knock yourself out.\n\n\nIf you need to mock the behaviour of an Actor, you should mock its subject\nrather than the proxy itself (#actor_subject). This ensures that any time\nthe subject calls methods on self, they will also be appropriately mocked.\n\n\nOn Thread Safety in Ruby\nRuby actually has a pretty good story when it comes to thread safety. The best\nstrategy for thread safety is to share as little state as possible, and if\nyou do share state, you should never mutate it. The worry of anyone stepping\ninto a thread safe world is that you\'re using a bunch of legacy libraries with\ndubious thread safety. Who knows what those crazy library authors were doing?\nRelax people. You\'re using a language where somebody can change what the \'+\'\noperator does to numbers. So why aren\'t we afraid to add numbers? Who knows\nwhat those crazy library authors may have done! Instead of freaking out, we\ncan learn some telltale signs of things that will cause thread safety problems\nin Ruby programs so we can identify potential problem libraries just from how\ntheir APIs behave.\nThe #1 thread safety issue to look out for in a Ruby library is if it provides\nsome sort of singleton access to a particular object through a class method,\ne.g MyClass.zomgobject, as opposed to asking you do do MyClass.new. If you\naren\'t allocating the object, it isn\'t yours, it\'s somebody else\'s, and you\nbetter damn well make sure you can share nice, or you shouldn\'t play with it\nat all.\nHow do we share nicely? Let\'s find out by first looking at a thread-unsafe\nversion of a singleton method:\nclass Foo\n  def self.current\n    @foo ||= Foo.new\n  end\nend\n\nSeems bad. All threads will share access to the same Foo object, and there\'s\nalso a secondary bug here which means when the object is first being allocated\nand memoized as @foo. The first thread that tries to allocate it may get a\ndifferent version than all the other threads because the memo value it set\ngot clobbered by another thread because it\'s unsynchronized.\nWhat else can we do? It depends on why the library is memoizing. Perhaps the\nFoo object has some kind of setup cost, such as making a network connection,\nand we want to keep it around instead of setting it up and tearing it down\nevery time. If that\'s the case, the simplest thing we can do to make this\ncode thread safe is to create a thread-specific memo of the object:\nclass Foo\n  def self.current\n    Thread.current[:foo] ||= Foo.new\n  end\nend\n\nKeep in mind that this will require N Foo objects for N threads. If each\nobject is wrapping a network connection, this might be a concern. That said,\nif you see this pattern employed in the singleton methods of a library,\nit\'s most likely thread safe, provided that Foo doesn\'t do other wonky things.\nContributing to Celluloid\n\nFork Celluloid on github\nMake your changes and send me a pull request\nIf I like them I\'ll merge them and give you commit access to my repository\n\nCopyright\nCopyright (c) 2011 Tony Arcieri. See LICENSE.txt for further details.\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/chat_gram', '\nChatGram\nChatGram is a barebones Instagram realtime endpoint for posting images\nto a chat service.  With it, you can...\n\n\nSee when friends post pics -- in RealTime (tm)!\n\n\n\nSearch for recent Instagrams in a given lat/long!\n\n(Note: This requires custom location search => lat/long coordinates\nintegration).\n\n\nInstallation\n\nClone the repo from GitHub.\nbundle install --binstubs to load the right dependencies.\nbin/rake chatgram:setup to create the database.\nbin/rackup config.ru to start the server.\n\nSee chat_gram.rb for the env variables.\nIf you don\'t want to use Bundler or Rubygems, you can require\nchat_gram/app manually and start it up like any other Rack\napplication.  Booya.\nDeployment\nChatGram is designed to be deployed on Heroku.  That means, config files\nare replaced with environment variables.  See\n./chat_gram.rb for the expected environment variables.\nCustomizing\nI tried to make the basic pieces as abstract as possible.  You should be\nable to write custom chat service endpoints, or store your data in\nCouchDB...\nChat Services\nThe only service supported currently is Campfire.\nData Store\nThe data store has a simple API and can basically support anything.\nOnly basic DB support is included.\nTODO\n\nDocument startup env vars better\nAdd yaml config file support.\nFinish Admin UI.\nCome up with a clever way to load other chat services or data stores.\nBundle into a gem.\n\n\n');
INSERT INTO `pinfo` (`login`, `avatar`, `followers`, `following`, `blog`, `public_repos`, `repos_url`, `created_at`, `rName`, `readme`) VALUES
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/coffee-resque', '\nCoffee-Resque\nCoffeescript/Node.js port of Resque.\nUSAGE\nFirst, you\'ll want to queue some jobs in your app:\nvar resque = require(\'coffee-resque\').connect({\n  host: redisHost,\n  port: redisPort\n});\nresque.enqueue(\'math\', \'add\', [1,2], function(err, remainingJobs) {\n  console.log(\'New job queued. Remaining jobs in queue: \' + remainingJobs);\n});\nNext, you\'ll want to setup a worker to handle these jobs.\nUpon completion of the job, invoke the passed callback with a result\n(if a result was produced by the job) or an Error (if an error was\nencountered).  If an Error is received, resque fails the\njob. In all other cases resque assumes the job is successful.\nThe callback is important—it notifies resque that the worker\nhas completed the current job and is ready for another. Neglecting to\ninvoke the callback will result in worker starvation.\n// implement your job functions.\nvar myJobs = {\n  add: function(a, b, callback) { callback(a + b); },\n  succeed: function(arg, callback) { callback(); },\n  fail: function(arg, callback) { callback(new Error(\'fail\')); }\n}\n\n// setup a worker\nvar worker = require(\'coffee-resque\').connect({\n  host: redisHost,\n  port: redisPort\n}).worker(\'*\', myJobs)\n\n// some global event listeners\n//\n// Triggered every time the Worker polls.\nworker.on(\'poll\', function(worker, queue) {})\n\n// Triggered before a Job is attempted.\nworker.on(\'job\', function(worker, queue, job) {})\n\n// Triggered every time a Job errors.\nworker.on(\'error\', function(err, worker, queue, job) {})\n\n// Triggered on every successful Job run.\nworker.on(\'success\', function(worker, queue, job, result) {})\n\nworker.start()\nWorker Polling Mechanism\nAs of v0.1.9, workers poll the given queues similar to Ruby Resque:\nstart\nloop do\n  if job = reserve\n    job.process\n  else\n    sleep 5 # Polling frequency = 5\n  end\nend\nshutdown\n\nThis ensures that multiple queues are polled in the priority mentioned.  Eg: If a worker is started on \"queue1,queue2\",\nqueue1 is drained completely before jobs in queue2 are processed.\nPrior to v0.1.9, workers used to poll the queues in a round-robin fashion.\nDevelopment\nAll code is written in Coffee Script and converted to javascript as it\'s\npublished to npm.\nFor normal development, all you need to be concerned about is testing:\n$ make test\nIf you need to generate javascript for production purposes and don\'t want to use npm packages, you can use:\n$ make generate-js\n$ make remove-js\nYou can also have coffeescript watch the src directory and generate Javascript files as they\'re updated.\n$ make dev\nTODO\n\nGeneric failure handling\nBetter polling\n\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/coffee-sprites', '\nMario Sprites\nPlaying around with Coffee Script, and HTML 5.  Heavily inspired by:\nhttp://gamesinhtml5.blogspot.com/2010/07/game-in-progress-sprites-and-animation.html\nSuper Mario 3 sprites:\nhttp://www.yes2web.nl/files/artikelen/sprites/supermario3.gif\nDevelopment\n$ coffee -wc -o javascript src/**/*.coffee\n\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/dangerroom', '\nDanger Room\nThe Danger Room is a training center for web services to hone their HTTP\nrequest skills.  It includes alien Shi\'ar proxy technology, creating the ultimate\nSOA simulator.\nAPI\nThe bin/danger server exposes a JSON API that lets you setup harnesses on\npaths.\nPUT /~danger/foo\nContent-Type: application/vnd.danger-room.limiting-harness+json\n{\n  \"target\": \"https://website.com/to/proxy\",\n  \"harness\": {\n    \"response_size_limit\": 500\n  }\n}\n\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/elixir-rubyports', '\nElixir Ruby Ports\nThis is just some hacks to learn Elixir by porting some familiar Ruby\nAPIs.\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/emoji-css-builder', '\nEmoji CSS Builder\nA simple tool for helping you display emoji in web browsers.\nWhat you get\n\nTiled image of the emoji icons for efficient retrieval.\nGenerated CSS classes for each icon.\nSample HTML5 demo.\n\nWhat you need\n\nAny Ruby\nImageMagick (or more specifically, the montage command)\n\n\nWhat you do\nJust require emoji_css_builder:\n# Build the default iphone emoji set\nEmojiCSSBuilder.build(:iphone, \"/path/to/assets\")\n\n# Build a subset of the iphone emoji:\nEmojiCSSBuilder.build(:iphone, \"/path/to/assets\", \n  %w(e001 e002 e00d))\n\nOr use rake:\nrake emoji DEST=/path/to/assets\nrake emoji DEST=/path/to/assets SET=iphone\nrake emoji DEST=/path/to/assets SET=iphone ICONS=e001,e002,e00d\n\nWhat I need\n\nImages and icon sets for more\n\nWhom you should thank\n\nKyle Barrow (http://pukupi.com/post/1964/)\n\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/etcd', '\netcd\nREADME version 0.2.0\n\nA highly-available key value store for shared configuration and service discovery.\netcd is inspired by zookeeper and doozer, with a focus on:\n\nSimple: curl\'able user facing API (HTTP+JSON)\nSecure: optional SSL client cert authentication\nFast: benchmarked 1000s of writes/s per instance\nReliable: Properly distributed using Raft\n\nEtcd is written in Go and uses the Raft consensus algorithm to manage a highly-available replicated log.\nSee etcdctl for a simple command line client.\nOr feel free to just use curl, as in the examples below.\nContact\n\nMailing list: http://coreos.com/lists/etcd-dev/\nIRC: #coreos on irc.freenode.net\nPlanning/Roadmap: https://trello.com/b/OiEbU547/etcd\nBugs: https://github.com/coreos/etcd/issues\n\nGetting Started\nGetting etcd\nThe latest release is available as a binary at Github.\nBuilding\nYou can build etcd from source:\ngit clone https://github.com/coreos/etcd\ncd etcd\n./build\nThis will generate a binary in the base directory called ./etcd.\nNOTE: you need go 1.1+. Please check your installation with\ngo version\n\nRunning a single machine\nThese examples will use a single machine cluster to show you the basics of the etcd REST API.\nLet\'s start etcd:\n./etcd -data-dir machine0 -name machine0\nThis will bring up etcd listening on port 4001 for client communication and on port 7001 for server-to-server communication.\nThe -data-dir machine0 argument tells etcd to write machine configuration, logs and snapshots to the ./machine0/ directory.\nThe -name machine tells the rest of the cluster that this machine is named machine0.\nUsage\nSetting the value to a key\nLet’s set the first key-value pair to the datastore.\nIn this case the key is /message and the value is Hello world.\ncurl -L http://127.0.0.1:4001/v2/keys/message -X PUT -d value=\"Hello world\"\n{\"action\":\"set\",\"key\":\"/message\",\"value\":\"Hello world\",\"modifiedIndex\":2}\nThis response contains four fields.\nWe will introduce three more fields as we try more commands.\n\n\nThe action of the request; we set the value via a PUT request, thus the action is set.\n\n\nThe key of the request; we set /message to Hello world, so the key field is /message.\nWe use a file system like structure to represent the key-value pairs so each key starts with /.\n\n\nThe current value of the key; we set the value toHello world.\n\n\nModified Index is a unique, monotonically incrementing index created for each change to etcd.\nRequests that change the index include set, delete, update, create and compareAndSwap.\nSince the get and watch commands do not change state in the store, they do not change the index.\nYou may notice that in this example the index is 2 even though it is the first request you sent to the server.\nThis is because there are internal commands that also change the state like adding and syncing servers.\n\n\nGet the value of a key\nWe can get the value that we just set in /message by issuing a GET request:\ncurl -L http://127.0.0.1:4001/v2/keys/message\n{\"action\":\"get\",\"key\":\"/message\",\"value\":\"Hello world\",\"modifiedIndex\":2}\nChanging the value of a key\nYou can change the value of /message from Hello world to Hello etcd with another PUT request to the key:\ncurl -L http://127.0.0.1:4001/v2/keys/message -XPUT -d value=\"Hello etcd\"\n{\"action\":\"set\",\"key\":\"/message\",\"prevValue\":\"Hello world\",\"value\":\"Hello etcd\",\"index\":3}\nNotice that the prevValue is set to the previous value of the key - Hello world.\nIt is useful when you want to atomically set a value to a key and get its old value.\nDeleting a key\nYou can remove the /message key with a DELETE request:\ncurl -L http://127.0.0.1:4001/v2/keys/message -XDELETE\n{\"action\":\"delete\",\"key\":\"/message\",\"prevValue\":\"Hello etcd\",\"modifiedIndex\":4}\nUsing key TTL\nKeys in etcd can be set to expire after a specified number of seconds.\nYou can do this by setting a TTL (time to live) on the key when send a PUT request:\ncurl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -d ttl=5\n{\"action\":\"set\",\"key\":\"/foo\",\"value\":\"bar\",\"expiration\":\"2013-11-12T20:21:22.629352334-05:00\",\"ttl\":5,\"modifiedIndex\":5}\nNote the two new fields in response:\n\n\nThe expiration is the time that this key will expire and be deleted.\n\n\nThe ttl is the time to live for the key, in seconds.\n\n\nNOTE: Keys can only be expired by a cluster leader so if a machine gets disconnected from the cluster, its keys will not expire until it rejoins.\nNow you can try to get the key by sending a GET request:\ncurl -L http://127.0.0.1:4001/v2/keys/foo\nIf the TTL has expired, the key will be deleted, and you will be returned a 100.\n{\"errorCode\":100,\"message\":\"Key Not Found\",\"cause\":\"/foo\",\"index\":6}\nWaiting for a change\nWe can watch for a change on a key and receive a notification by using long polling.\nThis also works for child keys by passing recursive=true in curl.\nIn one terminal, we send a get request with wait=true :\ncurl -L http://127.0.0.1:4001/v2/keys/foo?wait=true\nNow we are waiting for any changes at path /foo.\nIn another terminal, we set a key /foo with value bar:\ncurl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar\nThe first terminal should get the notification and return with the same response as the set request.\n{\"action\":\"set\",\"key\":\"/foo\",\"value\":\"bar\",\"modifiedIndex\":7}\nHowever, the watch command can do more than this.\nUsing the the index we can watch for commands that has happened in the past.\nThis is useful for ensuring you don\'t miss events between watch commands.\nLet\'s try to watch for the set command of index 7 again:\ncurl -L http://127.0.0.1:4001/v2/keys/foo?wait=true\\&waitIndex=7\nThe watch command returns immediately with the same response as previous.\nAtomic Compare-and-Swap (CAS)\nEtcd can be used as a centralized coordination service in a cluster and CompareAndSwap is the most basic operation to build distributed lock service.\nThis command will set the value of a key only if the client-provided conditions are equal to the current conditions.\nThe current comparable conditions are:\n\n\nprevValue - checks the previous value of the key.\n\n\nprevIndex - checks the previous index of the key.\n\n\nprevExist - checks existence of the key: if prevExist is true, it is a  update request; if prevExist is false, it is a create request.\n\n\nHere is a simple example.\nLet\'s create a key-value pair first: foo=one.\ncurl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=one\nLet\'s try an invalid CompareAndSwap command first.\nWe can provide the prevValue parameter to the set command to make it a CompareAndSwap command.\ncurl -L http://127.0.0.1:4001/v2/keys/foo?prevValue=two -XPUT -d value=three\nThis will try to compare the previous value of the key and the previous value we provided. If they are equal, the value of the key will change to three.\n{\"errorCode\":101,\"message\":\"Test Failed\",\"cause\":\"[two != one] [0 != 8]\",\"index\":8}\nwhich means CompareAndSwap failed.\nLet\'s try a valid condition:\ncurl -L http://127.0.0.1:4001/v2/keys/foo?prevValue=one -XPUT -d value=two\nThe response should be\n{\"action\":\"compareAndSwap\",\"key\":\"/foo\",\"prevValue\":\"one\",\"value\":\"two\",\"modifiedIndex\":9}\nWe successfully changed the value from “one” to “two” since we gave the correct previous value.\nListing a directory\nIn etcd we can store two types of things: keys and directories.\nKeys store a single string value.\nDirectories store a set of keys and/or other directories.\nIn this example, let\'s first create some keys:\nWe already have /foo=two so now we\'ll create another one called /foo_dir/foo with the value of bar:\ncurl -L http://127.0.0.1:4001/v2/keys/foo_dir/foo -XPUT -d value=bar\n{\"action\":\"set\",\"key\":\"/foo_dir/foo\",\"value\":\"bar\",\"modifiedIndex\":10}\nNow we can list the keys under root /:\ncurl -L http://127.0.0.1:4001/v2/keys/\nWe should see the response as an array of items:\n{\"action\":\"get\",\"key\":\"/\",\"dir\":true,\"kvs\":[{\"key\":\"/foo\",\"value\":\"two\",\"modifiedIndex\":9},{\"key\":\"/foo_dir\",\"dir\":true,\"modifiedIndex\":10}],\"modifiedIndex\":0}\nHere we can see /foo is a key-value pair under / and /foo_dir is a directory.\nWe can also recursively get all the contents under a directory by adding recursive=true.\ncurl -L http://127.0.0.1:4001/v2/keys/?recursive=true\n{\"action\":\"get\",\"key\":\"/\",\"dir\":true,\"kvs\":[{\"key\":\"/foo\",\"value\":\"two\",\"modifiedIndex\":9},{\"key\":\"/foo_dir\",\"dir\":true,\"kvs\":[{\"key\":\"/foo_dir/foo\",\"value\":\"bar\",\"modifiedIndex\":10}],\"modifiedIndex\":10}],\"modifiedIndex\":0}\nDeleting a directory\nNow let\'s try to delete the directory /foo_dir.\nTo delete a directory, we must add recursive=true.\ncurl -L http://127.0.0.1:4001/v2/keys/foo_dir?recursive=true -XDELETE\n{\"action\":\"delete\",\"key\":\"/foo_dir\",\"dir\":true,\"modifiedIndex\":11}\nCreating a hidden node\nWe can create a hidden key-value pair or directory by add a _ prefix.\nThe hidden item will not be listed when sending a GET request for a directory.\nFirst we\'ll add a hidden key named /_message:\ncurl -L http://127.0.0.1:4001/v2/keys/_message -XPUT -d value=\"Hello hidden world\"\n{\"action\":\"set\",\"key\":\"/_message\",\"value\":\"Hello hidden world\",\"modifiedIndex\":12}\nNext we\'ll add a regular key named /message:\ncurl -L http://127.0.0.1:4001/v2/keys/message -XPUT -d value=\"Hello world\"\n{\"action\":\"set\",\"key\":\"/message\",\"value\":\"Hello world\",\"modifiedIndex\":13}\nNow let\'s try to get a listing of keys under the root directory, /:\ncurl -L http://127.0.0.1:4001/v2/keys/\n{\"action\":\"get\",\"key\":\"/\",\"dir\":true,\"kvs\":[{\"key\":\"/foo\",\"value\":\"two\",\"modifiedIndex\":9},{\"key\":\"/message\",\"value\":\"Hello world\",\"modifiedIndex\":13}],\"modifiedIndex\":0}\nHere we see the /message key but our hidden /_message key is not returned.\nAdvanced Usage\nTransport security with HTTPS\nEtcd supports SSL/TLS and client cert authentication for clients to server, as well as server to server communication.\nFirst, you need to have a CA cert clientCA.crt and signed key pair client.crt, client.key.\nThis site has a good reference for how to generate self-signed key pairs:\nhttp://www.g-loaded.eu/2005/11/10/be-your-own-ca/\nFor testing you can use the certificates in the fixtures/ca directory.\nLet\'s configure etcd to use this keypair:\n./etcd -f -name machine0 -data-dir machine0 -cert-file=./fixtures/ca/server.crt -key-file=./fixtures/ca/server.key.insecure\nThere are a few new options we\'re using:\n\n-f - forces a new machine configuration, even if an existing configuration is found. (WARNING: data loss!)\n-cert-file and -key-file specify the location of the cert and key files to be used for for transport layer security between the client and server.\n\nYou can now test the configuration using HTTPS:\ncurl --cacert ./fixtures/ca/server-chain.pem https://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -v\nYou should be able to see the handshake succeed.\n...\nSSLv3, TLS handshake, Finished (20):\n...\n\nAnd also the response from the etcd server:\n{\"action\":\"set\",\"key\":\"/foo\",\"prevValue\":\"bar\",\"value\":\"bar\",\"modifiedIndex\":3}\nAuthentication with HTTPS client certificates\nWe can also do authentication using CA certs.\nThe clients will provide their cert to the server and the server will check whether the cert is signed by the CA and decide whether to serve the request.\n./etcd -f -name machine0 -data-dir machine0 -ca-file=./fixtures/ca/ca.crt -cert-file=./fixtures/ca/server.crt -key-file=./fixtures/ca/server.key.insecure\n-ca-file is the path to the CA cert.\nTry the same request to this server:\ncurl --cacert ./fixtures/ca/server-chain.pem https://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -v\nThe request should be rejected by the server.\n...\nroutines:SSL3_READ_BYTES:sslv3 alert bad certificate\n...\n\nWe need to give the CA signed cert to the server.\ncurl --key ./fixtures/ca/server2.key.insecure --cert ./fixtures/ca/server2.crt --cacert ./fixtures/ca/server-chain.pem -L https://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -v\nYou should able to see:\n...\nSSLv3, TLS handshake, CERT verify (15):\n...\nTLS handshake, Finished (20)\n\nAnd also the response from the server:\n{\"action\":\"set\",\"key\":\"/foo\",\"prevValue\":\"bar\",\"value\":\"bar\",\"modifiedIndex\":3}\nClustering\nExample cluster of three machines\nLet\'s explore the use of etcd clustering.\nWe use Raft as the underlying distributed protocol which provides consistency and persistence of the data across all of the etcd instances.\nLet start by creating 3 new etcd instances.\nWe use -peer-addr to specify server port and -addr to specify client port and -data-dir to specify the directory to store the log and info of the machine in the cluster:\n./etcd -peer-addr 127.0.0.1:7001 -addr 127.0.0.1:4001 -data-dir machines/machine1 -name machine1\nNote: If you want to run etcd on an external IP address and still have access locally, you\'ll need to add -bind-addr 0.0.0.0 so that it will listen on both external and localhost addresses.\nA similar argument -peer-bind-addr is used to setup the listening address for the server port.\nLet\'s join two more machines to this cluster using the -peers argument:\n./etcd -peer-addr 127.0.0.1:7002 -addr 127.0.0.1:4002 -peers 127.0.0.1:7001 -data-dir machines/machine2 -name machine2\n./etcd -peer-addr 127.0.0.1:7003 -addr 127.0.0.1:4003 -peers 127.0.0.1:7001 -data-dir machines/machine3 -name machine3\nWe can retrieve a list of machines in the cluster using the HTTP API:\ncurl -L http://127.0.0.1:4001/v2/machines\nWe should see there are three machines in the cluster\nhttp://127.0.0.1:4001, http://127.0.0.1:4002, http://127.0.0.1:4003\n\nThe machine list is also available via the main key API:\ncurl -L http://127.0.0.1:4001/v2/keys/_etcd/machines\n[{\"action\":\"get\",\"key\":\"/_etcd/machines/machine1\",\"value\":\"raft=http://127.0.0.1:7001\\u0026etcd=http://127.0.0.1:4001\",\"index\":1},{\"action\":\"get\",\"key\":\"/_etcd/machines/machine2\",\"value\":\"raft=http://127.0.0.1:7002\\u0026etcd=http://127.0.0.1:4002\",\"index\":1},{\"action\":\"get\",\"key\":\"/_etcd/machines/machine3\",\"value\":\"raft=http://127.0.0.1:7003\\u0026etcd=http://127.0.0.1:4003\",\"index\":1}]\nWe can also get the current leader in the cluster:\ncurl -L http://127.0.0.1:4001/v2/leader\n\nThe first server we set up should still be the leader unless it has died during these commands.\nhttp://127.0.0.1:7001\n\nNow we can do normal SET and GET operations on keys as we explored earlier.\ncurl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar\n{\"action\":\"set\",\"key\":\"/foo\",\"value\":\"bar\",\"modifiedIndex\":4}\nKilling Nodes in the Cluster\nNow if we kill the leader of the cluster, we can get the value from one of the other two machines:\ncurl -L http://127.0.0.1:4002/v2/keys/foo\nWe can also see that a new leader has been elected:\ncurl -L http://127.0.0.1:4002/v2/leader\n\nhttp://127.0.0.1:7002\n\nor\nhttp://127.0.0.1:7003\n\nTesting Persistence\nNext we\'ll kill all the machines to test persistence.\nType CTRL-C on each terminal and then rerun the same command you used to start each machine.\nYour request for the foo key will return the correct value:\ncurl -L http://127.0.0.1:4002/v2/keys/foo\n{\"action\":\"get\",\"key\":\"/foo\",\"value\":\"bar\",\"index\":4}\nUsing HTTPS between servers\nIn the previous example we showed how to use SSL client certs for client-to-server communication.\nEtcd can also do internal server-to-server communication using SSL client certs.\nTo do this just change the -*-file flags to -peer-*-file.\nIf you are using SSL for server-to-server communication, you must use it on all instances of etcd.\nContributing\nSee CONTRIBUTING for details on submitting patches and contacting developers via IRC and mailing lists.\nLibraries and Tools\nTools\n\netcdctl - A command line client for etcd\n\nGo libraries\n\ngo-etcd\n\nJava libraries\n\njustinsb/jetcd\ndiwakergupta/jetcd\n\nPython libraries\n\ntransitorykris/etcd-py\njplana/python-etcd\nrussellhaering/txetcd - a Twisted Python library\n\nNode libraries\n\nstianeikeland/node-etcd\n\nRuby libraries\n\niconara/etcd-rb\njpfuentes2/etcd-ruby\nranjib/etcd-ruby\n\nC libraries\n\njdarcy/etcd-api\n\nClojure libraries\n\naterreno/etcd-clojure\nrthomas/clj-etcd\n\nErlang libraries\n\nmarshall-lee/etcd.erl\n\nChef Integration\n\ncoderanger/etcd-chef\n\nChef Cookbook\n\nspheromak/etcd-cookbook\n\nProjects using etcd\n\nbinocarlos/yoda - etcd + ZeroMQ\ncalavera/active-proxy - HTTP Proxy configured with etcd\nderekchiang/etcdplus - A set of distributed synchronization primitives built upon etcd\ngo-discover - service discovery in Go\ngleicon/goreman - Branch of the Go Foreman clone with etcd support\ngarethr/hiera-etcd - Puppet hiera backend using etcd\nmattn/etcd-vim - SET and GET keys from inside vim\nmattn/etcdenv - \"env\" shebang with etcd integration\nkelseyhightower/confd - Manage local app config files using templates and data from etcd\n\nFAQ\nWhat size cluster should I use?\nEvery command the client sends to the master is broadcast to all of the followers.\nThe command is not committed until the majority of the cluster peers receive that command.\nBecause of this majority voting property, the ideal cluster should be kept small to keep speed up and be made up of an odd number of peers.\nOdd numbers are good because if you have 8 peers the majority will be 5 and if you have 9 peers the majority will still be 5.\nThe result is that an 8 peer cluster can tolerate 3 peer failures and a 9 peer cluster can tolerate 4 machine failures.\nAnd in the best case when all 9 peers are responding the cluster will perform at the speed of the fastest 5 machines.\nWhy SSLv3 alert handshake failure when using SSL client auth?\nThe crypto/tls package of golang checks the key usage of the certificate public key before using it.\nTo use the certificate public key to do client auth, we need to add clientAuth to Extended Key Usage when creating the certificate public key.\nHere is how to do it:\nAdd the following section to your openssl.cnf:\n[ ssl_client ]\n...\n  extendedKeyUsage = clientAuth\n...\n\nWhen creating the cert be sure to reference it in the -extensions flag:\nopenssl ca -config openssl.cnf -policy policy_anything -extensions ssl_client -out certs/machine.crt -infiles machine.csr\n\nProject Details\nVersioning\netcd uses semantic versioning.\nNew minor versions may add additional features to the API however.\nYou can get the version of etcd by issuing a request to /version:\ncurl -L http://127.0.0.1:4001/version\nDuring the pre-v1.0.0 series of releases we may break the API as we fix bugs and get feedback.\nLicense\netcd is under the Apache 2.0 license. See the LICENSE file for details.\n\n'),
('technoweenie', 'https://avatars3.githubusercontent.com/u/21?v=4', 2441, 17, 'http://techno-weenie.net', 165, 'https://api.github.com/users/technoweenie/repos', 2008, 'technoweenie/fab', '\n(fab) - a modular async web framework\n(fab) is a lightweight toolkit that makes it easy to build asynchronous web apps. It takes advantage of the flexibility and functional nature of javascript to create a concise \"DSL\", without pre-compilation or magic scope hackery.\nHere\'s an example of a \"hello world\" app:\nfab = require( \"../\" );\n\nrequire( \"http\" ).createServer( fab\n\n  ( /^\\/hello/ )\n  \n    ( fab.tmpl, \"Hello, <%= this[ 0 ] %>!\" )\n\n    ( /^\\/(\\w+)$/ )\n      ( fab.capture )\n      ( [ \"world\" ] )\n  \n  ( 404 )\n\n).listen( 0xFAB );\n\nSee more examples, learn how to make your own apps, or see the apps that (fab) provides for you.\n\n');

-- --------------------------------------------------------

--
-- Tablo için tablo yapısı `users`
--

CREATE TABLE `users` (
  `id` int(11) NOT NULL,
  `name` text NOT NULL,
  `email` text NOT NULL,
  `username` text NOT NULL,
  `password` text NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Tablo döküm verisi `users`
--

INSERT INTO `users` (`id`, `name`, `email`, `username`, `password`) VALUES
(3, 'aaaaa', 'kbez1997@hotmail.com', 'istanbulgs', '$5$rounds=535000$vt6rxPTKQsfJ6EQl$mlug/Ee91hWdKTwSV/fHDltZL6iPTSoGJ4YkN.xE7X2'),
(4, 'aaaaa', 'kbez1997@hotmail.com', 'istanbulgs', '$5$rounds=535000$Q7bV7..188ZOZWL/$4Nb6/LCUKrX54w8lrTi9Mz7E.Ex2oYJltiicwLjIjrB'),
(5, 'barış', 'kbez1997@hotmail.com', 'baris', '$5$rounds=535000$VnRBx9ZOjwk64isa$iRSHiVBoRC9PDJ5zJBq2kQlZLx0/cjH4Wa17CM4O/S0');

--
-- Dökümü yapılmış tablolar için indeksler
--

--
-- Tablo için indeksler `articles`
--
ALTER TABLE `articles`
  ADD PRIMARY KEY (`id`);

--
-- Tablo için indeksler `users`
--
ALTER TABLE `users`
  ADD PRIMARY KEY (`id`);

--
-- Dökümü yapılmış tablolar için AUTO_INCREMENT değeri
--

--
-- Tablo için AUTO_INCREMENT değeri `articles`
--
ALTER TABLE `articles`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=11;

--
-- Tablo için AUTO_INCREMENT değeri `users`
--
ALTER TABLE `users`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=6;
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
